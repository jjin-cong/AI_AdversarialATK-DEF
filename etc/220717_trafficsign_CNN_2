{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20220717_trafficsign_CNN_채현","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9YIy5j67oWri"},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","import csv\n","import pandas as pd\n","import natsort\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB/"],"metadata":{"id":"ZBPA2Z-bofzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metainfo = pd.read_csv(\"Meta.csv\")\n","traininfo = pd.read_csv(\"Train.csv\")\n","testinfo = pd.read_csv(\"Test.csv\")"],"metadata":{"id":"bpKug8YvokTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgs_path = \"Train\"\n","data_list = []\n","labels_list = []\n","result_class = [3,7, 9, 10, 11, 12,13, 17, 18, 25,35,38]\n","\n","for i in result_class:\n","    i_path = os.path.join(imgs_path, str(i)) #3,7, 9, 10, 11, 12,13, 17, 18, 25,35,38\n","    num = 0\n","    for img in os.listdir(i_path):\n","        im = Image.open(i_path +'/'+ img)\n","        im = im.resize((32,32))\n","        im = np.array(im)\n","        data_list.append(im)\n","        labels_list.append(i)\n","        num = num + 1\n","        if num == 1000:\n","            break;\n","data = np.array(data_list)\n","labels = np.array(labels_list)\n","print(\"Done\")"],"metadata":{"id":"mnOqtPQxolQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transpose = data.transpose(0,3,1,2)"],"metadata":{"id":"C-c__eXjouma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_train = []\n","for i in labels:\n","    if i == 3:\n","        labels_train.append(0)\n","    elif i == 7:\n","        labels_train.append(1)\n","    elif i == 9:\n","        labels_train.append(2)\n","    elif i == 10:\n","        labels_train.append(3)\n","    elif i == 11:\n","        labels_train.append(4)\n","    elif i == 12:\n","        labels_train.append(5)\n","    elif i == 13:\n","        labels_train.append(6)\n","    elif i == 17:\n","        labels_train.append(7)\n","    elif i == 18:\n","        labels_train.append(8)\n","    elif i == 25:\n","        labels_train.append(9)\n","    elif i == 35:\n","        labels_train.append(10)\n","    elif i == 38:\n","        labels_train.append(11)\n","\n","labels = np.array(labels_train)"],"metadata":{"id":"zcLNtrEYowpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import natsort\n","\n","imgs_path = \"Test\"\n","data_list = []\n","labels_list = []\n","\n","for img in natsort.natsorted(os.listdir(imgs_path)):\n","    im = Image.open(imgs_path +'/'+ img)\n","    im = im.resize((32,32))\n","    im = np.array(im)\n","    data_list.append(im)\n","data_test = np.array(data_list)\n","\n","for i in range(len(testinfo.ClassId)):\n","    labels_list.append(testinfo.ClassId[i])\n","\n","labels_test = np.array(labels_list)\n","print(\"Done\")"],"metadata":{"id":"eMvJfLkcoz7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_test_index = []\n","for i in range(len(labels_test)):\n","    if (labels_test[i] == 3) | (labels_test[i] == 7) | (labels_test[i] == 9) | (labels_test[i] == 10) | (labels_test[i] == 11) | (labels_test[i] == 12) | (labels_test[i] == 13) | (labels_test[i] == 17) | (labels_test[i] == 18) | (labels_test[i] == 25) | (labels_test[i] == 35) | (labels_test[i] == 38):\n","        labels_test_index.append(i)"],"metadata":{"id":"iMk5cXpqo1YZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = []\n","test_label = []\n","for i in labels_test_index:\n","    test_data.append(data_test[i])\n","    test_label.append(labels_test[i])\n","data_test = np.array(test_data)   \n","labels_test = np.array(test_label)"],"metadata":{"id":"TWpYGhAXo294"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_test_transpose = data_test.transpose(0,3,1,2)"],"metadata":{"id":"8YdRZ8gso9T2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_test_convert = []\n","for i in labels_test:\n","    if i == 3:\n","        labels_test_convert.append(0)\n","    elif i == 7:\n","        labels_test_convert.append(1)\n","    elif i == 9:\n","        labels_test_convert.append(2)\n","    elif i == 10:\n","        labels_test_convert.append(3)\n","    elif i == 11:\n","        labels_test_convert.append(4)\n","    elif i == 12:\n","        labels_test_convert.append(5)\n","    elif i == 13:\n","        labels_test_convert.append(6)\n","    elif i == 17:\n","        labels_test_convert.append(7)\n","    elif i == 18:\n","        labels_test_convert.append(8)\n","    elif i == 25:\n","        labels_test_convert.append(9)\n","    elif i == 35:\n","        labels_test_convert.append(10)\n","    elif i == 38:\n","        labels_test_convert.append(11)"],"metadata":{"id":"YhLHAg-ho4Y8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_test = np.array(labels_test_convert)"],"metadata":{"id":"4ErU3eyYo7P_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN 구현\n","\n","\n","CONV -> RELU -> POOL -> AFF -> RELU -> AFF -> SOFTMAX 구조의 CNN을 구현할 것이다."],"metadata":{"id":"4f4tMzVdpBwz"}},{"cell_type":"code","source":["def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n","    \n","    Parameters\n","    ----------\n","    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","    \n","    Returns\n","    -------\n","    col : 2차원 배열\n","    \"\"\"\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","\n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","    return col\n","\n","\n","def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n","    \n","    Parameters\n","    ----------\n","    col : 2차원 배열(입력 데이터)\n","    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","    \n","    Returns\n","    -------\n","    img : 변환된 이미지들\n","    \"\"\"\n","    N, C, H, W = input_shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n","\n","    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n","\n","    return img[:, :, pad:H + pad, pad:W + pad]"],"metadata":{"id":"9WK2UfdDqTLy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","def identity_function(x):\n","    return x\n","\n","\n","def step_function(x):\n","    return np.array(x > 0, dtype=np.int)\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))    \n","\n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","    \n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","\n","def relu_grad(x):\n","    grad = np.zeros(x)\n","    grad[x>=0] = 1\n","    return grad\n","    \n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","\n","def mean_squared_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)\n","\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","def softmax_loss(X, t):\n","    y = softmax(X)\n","    return cross_entropy_error(y, t)"],"metadata":{"id":"XqWDtJUgsm2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = sigmoid(x)\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        \n","        self.x = None\n","        self.original_x_shape = None\n","        # 가중치와 편향 매개변수의 미분\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # 텐서 대응\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        self.x = x\n","\n","        out = np.dot(self.x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","        \n","        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실함수\n","        self.y = None    # softmax의 출력\n","        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n","        \n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        \n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n","            dx = (self.y - self.t) / batch_size\n","        else:\n","            dx = self.y.copy()\n","            dx[np.arange(batch_size), self.t] -= 1\n","            dx = dx / batch_size\n","        \n","        return dx\n","\n","\n","class Dropout:\n","    \"\"\"\n","    http://arxiv.org/abs/1207.0580\n","    \"\"\"\n","    def __init__(self, dropout_ratio=0.5):\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","\n","    def forward(self, x, train_flg=True):\n","        if train_flg:\n","            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n","            return x * self.mask\n","        else:\n","            return x * (1.0 - self.dropout_ratio)\n","\n","    def backward(self, dout):\n","        return dout * self.mask\n","\n","\n","class BatchNormalization:\n","    \"\"\"\n","    http://arxiv.org/abs/1502.03167\n","    \"\"\"\n","    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n","        self.gamma = gamma\n","        self.beta = beta\n","        self.momentum = momentum\n","        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n","\n","        # 시험할 때 사용할 평균과 분산\n","        self.running_mean = running_mean\n","        self.running_var = running_var  \n","        \n","        # backward 시에 사용할 중간 데이터\n","        self.batch_size = None\n","        self.xc = None\n","        self.std = None\n","        self.dgamma = None\n","        self.dbeta = None\n","\n","    def forward(self, x, train_flg=True):\n","        self.input_shape = x.shape\n","        if x.ndim != 2:\n","            N, C, H, W = x.shape\n","            x = x.reshape(N, -1)\n","\n","        out = self.__forward(x, train_flg)\n","        \n","        return out.reshape(*self.input_shape)\n","            \n","    def __forward(self, x, train_flg):\n","        if self.running_mean is None:\n","            N, D = x.shape\n","            self.running_mean = np.zeros(D)\n","            self.running_var = np.zeros(D)\n","                        \n","        if train_flg:\n","            mu = x.mean(axis=0)\n","            xc = x - mu\n","            var = np.mean(xc**2, axis=0)\n","            std = np.sqrt(var + 10e-7)\n","            xn = xc / std\n","            \n","            self.batch_size = x.shape[0]\n","            self.xc = xc\n","            self.xn = xn\n","            self.std = std\n","            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n","            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n","        else:\n","            xc = x - self.running_mean\n","            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n","            \n","        out = self.gamma * xn + self.beta \n","        return out\n","\n","    def backward(self, dout):\n","        if dout.ndim != 2:\n","            N, C, H, W = dout.shape\n","            dout = dout.reshape(N, -1)\n","\n","        dx = self.__backward(dout)\n","\n","        dx = dx.reshape(*self.input_shape)\n","        return dx\n","\n","    def __backward(self, dout):\n","        dbeta = dout.sum(axis=0)\n","        dgamma = np.sum(self.xn * dout, axis=0)\n","        dxn = self.gamma * dout\n","        dxc = dxn / self.std\n","        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n","        dvar = 0.5 * dstd / self.std\n","        dxc += (2.0 / self.batch_size) * self.xc * dvar\n","        dmu = np.sum(dxc, axis=0)\n","        dx = dxc - dmu / self.batch_size\n","        \n","        self.dgamma = dgamma\n","        self.dbeta = dbeta\n","        \n","        return dx\n","\n","\n","class Convolution:\n","    def __init__(self, W, b, stride=1, pad=0):\n","        self.W = W\n","        self.b = b\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        # 중간 데이터（backward 시 사용）\n","        self.x = None   \n","        self.col = None\n","        self.col_W = None\n","        \n","        # 가중치와 편향 매개변수의 기울기\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        FN, C, FH, FW = self.W.shape\n","        N, C, H, W = x.shape\n","        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n","        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n","\n","        col = im2col(x, FH, FW, self.stride, self.pad)\n","        col_W = self.W.reshape(FN, -1).T\n","\n","        out = np.dot(col, col_W) + self.b\n","        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.col = col\n","        self.col_W = col_W\n","\n","        return out\n","\n","    def backward(self, dout):\n","        FN, C, FH, FW = self.W.shape\n","        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n","\n","        self.db = np.sum(dout, axis=0)\n","        self.dW = np.dot(self.col.T, dout)\n","        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","        dcol = np.dot(dout, self.col_W.T)\n","        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","\n","        return dx\n","\n","\n","class Pooling:\n","    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n","        self.pool_h = pool_h\n","        self.pool_w = pool_w\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        self.x = None\n","        self.arg_max = None\n","\n","    def forward(self, x):\n","        N, C, H, W = x.shape\n","        out_h = int(1 + (H - self.pool_h) / self.stride)\n","        out_w = int(1 + (W - self.pool_w) / self.stride)\n","\n","        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n","        col = col.reshape(-1, self.pool_h*self.pool_w)\n","\n","        arg_max = np.argmax(col, axis=1)\n","        out = np.max(col, axis=1)\n","        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.arg_max = arg_max\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout = dout.transpose(0, 2, 3, 1)\n","        \n","        pool_size = self.pool_h * self.pool_w\n","        dmax = np.zeros((dout.size, pool_size))\n","        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","        dmax = dmax.reshape(dout.shape + (pool_size,)) \n","        \n","        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n","        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","        \n","        return dx"],"metadata":{"id":"0LmgEf0PsEFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def _numerical_gradient_1d(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        \n","    return grad\n","\n","\n","def numerical_gradient_2d(f, X):\n","    if X.ndim == 1:\n","        return _numerical_gradient_1d(f, X)\n","    else:\n","        grad = np.zeros_like(X)\n","        \n","        for idx, x in enumerate(X):\n","            grad[idx] = _numerical_gradient_1d(f, x)\n","        \n","        return grad\n","\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        it.iternext()   \n","        \n","    return grad"],"metadata":{"id":"Tp4pTmolsbRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SimpleConvNet:\n","    \"\"\"단순한 합성곱 신경망\n","    \n","    conv - relu - pool - affine - relu - affine - softmax\n","    \n","    Parameters\n","    ----------\n","    input_size : 입력 크기（MNIST의 경우엔 784）\n","    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n","    output_size : 출력 크기（MNIST의 경우엔 10）\n","    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n","    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n","        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n","        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n","    \"\"\"\n","    def __init__(self, input_dim=(1, 28, 28), \n","                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n","                 hidden_size=100, output_size=10, weight_init_std=0.01):\n","        filter_num = conv_param['filter_num']\n","        filter_size = conv_param['filter_size']\n","        filter_pad = conv_param['pad']\n","        filter_stride = conv_param['stride']\n","        input_size = input_dim[1]\n","        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n","        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n","\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * \\\n","                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n","        self.params['b1'] = np.zeros(filter_num)\n","        self.params['W2'] = weight_init_std * \\\n","                            np.random.randn(pool_output_size, hidden_size)\n","        self.params['b2'] = np.zeros(hidden_size)\n","        self.params['W3'] = weight_init_std * \\\n","                            np.random.randn(hidden_size, output_size)\n","        self.params['b3'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n","                                           conv_param['stride'], conv_param['pad'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n","\n","        self.last_layer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    def loss(self, x, t):\n","        \"\"\"손실 함수를 구한다.\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        \"\"\"\n","        y = self.predict(x)\n","        return self.last_layer.forward(y, t)\n","\n","    def accuracy(self, x, t, batch_size=100):\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","        \n","        acc = 0.0\n","        \n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            tt = t[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(tx)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == tt) \n","        \n","        return acc / x.shape[0]\n","\n","    def numerical_gradient(self, x, t):\n","        \"\"\"기울기를 구한다（수치미분）.\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        Returns\n","        -------\n","        각 층의 기울기를 담은 사전(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        loss_w = lambda w: self.loss(x, t)\n","\n","        grads = {}\n","        for idx in (1, 2, 3):\n","            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n","            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        \"\"\"기울기를 구한다(오차역전파법).\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        Returns\n","        -------\n","        각 층의 기울기를 담은 사전(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n","        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grads\n","        \n","    def save_params(self, file_name=\"params.pkl\"):\n","        params = {}\n","        for key, val in self.params.items():\n","            params[key] = val\n","        with open(file_name, 'wb') as f:\n","            pickle.dump(params, f)\n","\n","    def load_params(self, file_name=\"params.pkl\"):\n","        with open(file_name, 'rb') as f:\n","            params = pickle.load(f)\n","        for key, val in params.items():\n","            self.params[key] = val\n","\n","        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n","            self.layers[key].W = self.params['W' + str(i+1)]\n","            self.layers[key].b = self.params['b' + str(i+1)]"],"metadata":{"id":"ywfpu4DKo-7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    \"\"\"신경망 훈련을 대신 해주는 클래스\n","    \"\"\"\n","    def __init__(self, network, x_train, t_train, x_test, t_test,\n","                 epochs=20, mini_batch_size=100,\n","                 optimizer='SGD', optimizer_param={'lr':0.01}, \n","                 evaluate_sample_num_per_epoch=None, verbose=True):\n","        self.network = network\n","        self.verbose = verbose\n","        self.x_train = x_train\n","        self.t_train = t_train\n","        self.x_test = x_test\n","        self.t_test = t_test\n","        self.epochs = epochs\n","        self.batch_size = mini_batch_size\n","        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n","\n","        # optimzer\n","        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n","                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n","        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n","        \n","        self.train_size = x_train.shape[0]\n","        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n","        self.max_iter = int(epochs * self.iter_per_epoch)\n","        self.current_iter = 0\n","        self.current_epoch = 0\n","        \n","        self.train_loss_list = []\n","        self.train_acc_list = []\n","        self.test_acc_list = []\n","\n","    def train_step(self):\n","        batch_mask = np.random.choice(self.train_size, self.batch_size)\n","        x_batch = self.x_train[batch_mask]\n","        t_batch = self.t_train[batch_mask]\n","        \n","        grads = self.network.gradient(x_batch, t_batch)\n","        self.optimizer.update(self.network.params, grads)\n","        \n","        loss = self.network.loss(x_batch, t_batch)\n","        self.train_loss_list.append(loss)\n","        if self.verbose: print(\"train loss:\" + str(loss))\n","        \n","        if self.current_iter % self.iter_per_epoch == 0:\n","            self.current_epoch += 1\n","            \n","            x_train_sample, t_train_sample = self.x_train, self.t_train\n","            x_test_sample, t_test_sample = self.x_test, self.t_test\n","            if not self.evaluate_sample_num_per_epoch is None:\n","                t = self.evaluate_sample_num_per_epoch\n","                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n","                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n","                \n","            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n","            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n","            self.train_acc_list.append(train_acc)\n","            self.test_acc_list.append(test_acc)\n","\n","            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n","        self.current_iter += 1\n","\n","    def train(self):\n","        for i in range(self.max_iter):\n","            self.train_step()\n","\n","        test_acc = self.network.accuracy(self.x_test, self.t_test)\n","\n","        if self.verbose:\n","            print(\"=============== Final Test Accuracy ===============\")\n","            print(\"test acc:\" + str(test_acc))"],"metadata":{"id":"faUg968wtj3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding: utf-8\n","import numpy as np\n","\n","class SGD:\n","\n","    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n","\n","    def __init__(self, lr=0.01):\n","        self.lr = lr\n","        \n","    def update(self, params, grads):\n","        for key in params.keys():\n","            params[key] -= self.lr * grads[key] \n","\n","\n","class Momentum:\n","\n","    \"\"\"모멘텀 SGD\"\"\"\n","\n","    def __init__(self, lr=0.01, momentum=0.9):\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.v is None:\n","            self.v = {}\n","            for key, val in params.items():                                \n","                self.v[key] = np.zeros_like(val)\n","                \n","        for key in params.keys():\n","            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n","            params[key] += self.v[key]\n","\n","\n","class Nesterov:\n","\n","    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n","    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n","    \n","    def __init__(self, lr=0.01, momentum=0.9):\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.v is None:\n","            self.v = {}\n","            for key, val in params.items():\n","                self.v[key] = np.zeros_like(val)\n","            \n","        for key in params.keys():\n","            self.v[key] *= self.momentum\n","            self.v[key] -= self.lr * grads[key]\n","            params[key] += self.momentum * self.momentum * self.v[key]\n","            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n","\n","\n","class AdaGrad:\n","\n","    \"\"\"AdaGrad\"\"\"\n","\n","    def __init__(self, lr=0.01):\n","        self.lr = lr\n","        self.h = None\n","        \n","    def update(self, params, grads):\n","        if self.h is None:\n","            self.h = {}\n","            for key, val in params.items():\n","                self.h[key] = np.zeros_like(val)\n","            \n","        for key in params.keys():\n","            self.h[key] += grads[key] * grads[key]\n","            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n","\n","\n","class RMSprop:\n","\n","    \"\"\"RMSprop\"\"\"\n","\n","    def __init__(self, lr=0.01, decay_rate = 0.99):\n","        self.lr = lr\n","        self.decay_rate = decay_rate\n","        self.h = None\n","        \n","    def update(self, params, grads):\n","        if self.h is None:\n","            self.h = {}\n","            for key, val in params.items():\n","                self.h[key] = np.zeros_like(val)\n","            \n","        for key in params.keys():\n","            self.h[key] *= self.decay_rate\n","            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n","            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n","\n","\n","class Adam:\n","\n","    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n","\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.iter = 0\n","        self.m = None\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.m is None:\n","            self.m, self.v = {}, {}\n","            for key, val in params.items():\n","                self.m[key] = np.zeros_like(val)\n","                self.v[key] = np.zeros_like(val)\n","        \n","        self.iter += 1\n","        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n","        \n","        for key in params.keys():\n","            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n","            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n","            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n","            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n","            \n","            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n","            \n","            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n","            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n","            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"],"metadata":{"id":"_JI7lFm9tptm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","\n","# print(tf.__version__)   # Tensorflow의 버전을 출력\n","\n","# mnist = keras.datasets.mnist\n","\n","# # MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n","# (x_train, t_train), (x_test, t_test) = mnist.load_data()   \n","# x_train = x_train.reshape(-1,1,28,28)\n","# x_test = x_test.reshape(-1,1,28,28)"],"metadata":{"id":"B1JQuSbGtBPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 10개 클래쓰만 사용 (1000장 이상인 클래스 중 골라서 1000장씩 사용하였고, test는 해당 클래스면 다 사용)\n","# Train & Test & X & y load\n","clas = [3, 7, 9, 10, 11, 12 ,13, 17, 18, 25, 35, 38]\n","\n","# Train\n","trainpath = \"Train\"\n","x_train = []\n","t_train = []\n","for cla in clas:\n","  c = [cla]\n","  cc = c * 1000\n","  t_train += cc\n","  imgs_all = natsort.natsorted(os.listdir(trainpath+\"/\"+str(cla)))\n","  imgs = imgs_all[0:1000]\n","  for img in imgs:\n","    image = Image.open(trainpath+\"/\"+str(cla)+\"/\"+img)\n","    image = image.resize((32,32))\n","    image = np.array(image)\n","    image = image.transpose(2,0,1)\n","    x_train.append(image)\n","  print(\"Train\"+str(cla)+\":done\")\n","x_train = np.array(x_train)\n","t_train = np.array(t_train)\n","print(\"Train:done\")\n","\n","\n","# Test\n","testpath = \"Test\"\n","x_test = []\n","t_test = []\n","df = pd.read_csv(\"./Test.csv\")\n","df = np.array(df)\n","y_test_allc = df[:,6]\n","for cla in clas:\n","  claposs = np.where(y_test_allc == cla)[0]\n","  testdataset = natsort.natsorted(os.listdir(testpath))\n","  c = [cla]\n","  cc = c * len(claposs)\n","  t_test += cc\n","  for clapos in claposs:\n","    image = Image.open(testpath+\"/\"+testdataset[clapos])\n","    image = image.resize((32,32))\n","    image = np.array(image)\n","    image = image.transpose(2,0,1)\n","    x_test.append(image)\n","  print(\"Test\"+str(cla)+\":done\")\n","x_test = np.array(x_test)\n","t_test = np.array(y_test)\n","print(\"Test:done\")"],"metadata":{"id":"iCbQw-2kuyQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 0~11 class일 때 y 코드\n","\n","# array에서 string으로 바꾸고 replace하고 다시 integer로\n","y_train12 = y_train\n","y_test12 = y_test\n","clas = [3, 7, 9, 10, 11, 12 ,13, 17, 18, 25, 35, 38]\n","\n","rep = 0\n","for cla in clas:\n","  print(rep)\n","  poss_train = np.where(y_train == cla)[0]\n","  poss_test = np.where(y_test == cla)[0]\n","  for pos_train in poss_train:\n","    y_train12[pos_train] = rep\n","  for pos_test in poss_test:\n","    y_test12[pos_test] = rep\n","  rep += 1\n","\n","print(min(y_train12))\n","print(max(y_train12))\n","print(min(y_test12))\n","print(max(y_test12))\n","print(y_train12)"],"metadata":{"id":"h70Pwbxqu5h5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_train.shape"],"metadata":{"id":"wNCH2GHYuZz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 읽기\n","#(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n","\n","# 시간이 오래 걸릴 경우 데이터를 줄인다.\n","#x_train, t_train = x_train[:5000], t_train[:5000]\n","#x_test, t_test = x_test[:1000], t_test[:1000]\n","\n","max_epochs = 20\n","\n","network = SimpleConvNet(input_dim=(1,28,28), \n","                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n","                        hidden_size=100, output_size=10, weight_init_std=0.01)\n","                        \n","trainer = Trainer(network, x_train, t_train, x_test, t_test,\n","                  epochs=max_epochs, mini_batch_size=100,\n","                  optimizer='Adam', optimizer_param={'lr': 0.001},\n","                  evaluate_sample_num_per_epoch=1000)\n","trainer.train()\n","\n","# 매개변수 보존\n","network.save_params(\"params.pkl\")\n","print(\"Saved Network Parameters!\")\n","\n","# 그래프 그리기\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(max_epochs)\n","plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n","plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"8tCa5Ehesysk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PieHupzotR-g"},"execution_count":null,"outputs":[]}]}