{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP1F6ft+xegLXyij/nc0xk5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["참조 코드 링크 : https://github.com/Harry24k/PGD-pytorch/blob/master/PGD.ipynb"],"metadata":{"id":"kaQKjrkV0HfR"}},{"cell_type":"markdown","source":["# 0.Google Drive"],"metadata":{"id":"ZCxIuq2s1a8K"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31DXK8GG1P7f","executionInfo":{"status":"ok","timestamp":1662105759152,"user_tz":-540,"elapsed":18715,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"eae3839d-9847-474c-bec6-314200c9b012"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/'[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구'/3. 소스코드"],"metadata":{"id":"K7m7io2d1aHD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662105759154,"user_tz":-540,"elapsed":15,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"33533741-752d-4301-edb2-a5344c9e6498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드\n"]}]},{"cell_type":"markdown","source":["# 1. Requirements"],"metadata":{"id":"aNeShz3V0Eo-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uBZBg05z53H"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as Data\n","\n","import torchvision.utils\n","from torchvision import models\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"LCfLf7Cb0M3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set Args\n","use_cuda = True\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"],"metadata":{"id":"ngoUqCCR0QYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Prepare Data\n","* 12개 클래스만 사용? : # 3, 7, 9, 10, 11, 12, 13, 17, 18, 25, 35, 38"],"metadata":{"id":"VXQscfr10SzE"}},{"cell_type":"code","source":["# 이 중에서 14(stop), 35(ahead only), 38(keep right)만 사용할 것이다.\n","class_idx = { 0:'Speed limit (20km/h)',\n","            1:'Speed limit (30km/h)', \n","            2:'Speed limit (50km/h)', \n","            3:'Speed limit (60km/h)', \n","            4:'Speed limit (70km/h)', \n","            5:'Speed limit (80km/h)', \n","            6:'End of speed limit (80km/h)', \n","            7:'Speed limit (100km/h)', \n","            8:'Speed limit (120km/h)', \n","            9:'No passing', \n","            10:'No passing veh over 3.5 tons', \n","            11:'Right-of-way at intersection', \n","            12:'Priority road', \n","            13:'Yield', \n","            14:'Stop', \n","            15:'No vehicles', \n","            16:'Veh > 3.5 tons prohibited', \n","            17:'No entry', \n","            18:'General caution', \n","            19:'Dangerous curve left', \n","            20:'Dangerous curve right', \n","            21:'Double curve', \n","            22:'Bumpy road', \n","            23:'Slippery road', \n","            24:'Road narrows on the right', \n","            25:'Road work', \n","            26:'Traffic signals', \n","            27:'Pedestrians', \n","            28:'Children crossing', \n","            29:'Bicycles crossing', \n","            30:'Beware of ice/snow',\n","            31:'Wild animals crossing', \n","            32:'End speed + passing limits', \n","            33:'Turn right ahead', \n","            34:'Turn left ahead', \n","            35:'Ahead only', \n","            36:'Go straight or right', \n","            37:'Go straight or left', \n","            38:'Keep right', \n","            39:'Keep left', \n","            40:'Roundabout mandatory', \n","            41:'End of no passing', \n","            42:'End no passing veh > 3.5 tons' }"],"metadata":{"id":"8Xx2elcVQKiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx2label = [class_idx[k] for k in range(len(class_idx))]"],"metadata":{"id":"3R1OvjtrQilL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metainfo = np.array(pd.read_csv(\"./GTSRB/Meta.csv\"))\n","traininfo = np.array(pd.read_csv(\"./GTSRB/Train.csv\"))\n","testinfo = np.array(pd.read_csv(\"./GTSRB/Test.csv\"))\n","meta_idx = metainfo[:,1]\n","train_idx = traininfo[:,6]"],"metadata":{"id":"Q9GARhnHBQc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((32, 32)), # 이미지 크기 32 by 32로 맞춤\n","    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n","])\n","# transforms.Compose : 데이터를 여러 단계로 변환해야 하는 경우, Compose를 통해 여러 단계를 묶을 수 있음"],"metadata":{"id":"wnW5kfEH0WVH","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def image_folder_custom_label(root, transform, custom_label) :\n","    # custom_label\n","    # type : List\n","    # index -> label (ex) ['tench', 'goldfish', 'great_white_shark', 'tiger_shark']\n","    \n","    old_data = dsets.ImageFolder(root = root, transform = transform)\n","    old_classes = old_data.classes\n","    \n","    label2idx = {}\n","    for i, item in enumerate(idx2label) :\n","        label2idx[item] = i\n","    \n","    new_data = dsets.ImageFolder(root = root, transform = transform, \n","                                 target_transform = lambda x : custom_label.index(old_classes[x]))\n","    new_data.classes = idx2label\n","    new_data.class_to_idx = label2idx\n","\n","    return new_data"],"metadata":{"id":"kYf_4QDZ0YPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normal_data = image_folder_custom_label(root = './GTSRB/Train', transform = transform, custom_label = idx2label)\n","normal_loader = Data.DataLoader(normal_data, batch_size=1, shuffle=False)"],"metadata":{"id":"hI0xnikK0Y5D","executionInfo":{"status":"ok","timestamp":1662106638478,"user_tz":-540,"elapsed":1313,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a16d7fc-85d0-4492-da8b-81b2d75a205a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset ImageFolder\n","    Number of datapoints: 39209\n","    Root location: ./GTSRB/Train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n","               ToTensor()\n","           )\n","\t\n","['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '5', '6', '7', '8', '9']\n"]}]},{"cell_type":"code","source":["imgs_path = \"Train\"\n","data_list = []\n","labels_list = []\n","result_class = [3,10,18]\n","\n","for i in result_class:\n","    i_path = os.path.join(imgs_path, str(i)) #3,7, 9, 10, 11, 12,13, 17, 18, 25,35,38\n","    num = 0\n","    for img in os.listdir(i_path):\n","        im = Image.open(i_path +'/'+ img)\n","        im = im.resize((32,32))\n","        im = np.array(im)\n","        data_list.append(im)\n","        labels_list.append(i)\n","        num = num + 1\n","        if num == 1000:\n","            break;\n","data = np.array(data_list)\n","labels = np.array(labels_list)\n","print(\"Done\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"_1KFBVROBt3J","executionInfo":{"status":"error","timestamp":1662102530556,"user_tz":-540,"elapsed":9,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"6ae53439-4c9e-43d3-c2bc-398f9049c0e1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-548cd815d642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mi_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#3,7, 9, 10, 11, 12,13, 17, 18, 25,35,38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"code","source":["def imshow(img, title):\n","    npimg = img.numpy()\n","    fig = plt.figure(figsize = (5, 15))\n","    plt.imshow(np.transpose(npimg,(1,2,0)))\n","    plt.title(title)\n","    plt.show()"],"metadata":{"id":"ux54FpPz0bgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normal_iter = iter(normal_loader)\n","images, labels = normal_iter.next()\n","\n","print(\"True Image & True Label\")\n","imshow(torchvision.utils.make_grid(images, normalize=True), [normal_data.classes[i] for i in labels])"],"metadata":{"id":"bwvQU5vC0dSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Download the Inception v3"],"metadata":{"id":"BGSPbUZT0fn6"}},{"cell_type":"code","source":["model = models.inception_v3(pretrained=True).to(device)"],"metadata":{"id":"xzgN5GBO0hRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"True Image & Predicted Label\")\n","\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","for images, labels in normal_loader:\n","    \n","    images = images.to(device)\n","    labels = labels.to(device)\n","    outputs = model(images)\n","    \n","    _, pre = torch.max(outputs.data, 1)\n","    \n","    total += 1\n","    correct += (pre == labels).sum()\n","    \n","    imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [normal_data.classes[i] for i in pre])\n","        \n","print('Accuracy of test text: %f %%' % (100 * float(correct) / total))"],"metadata":{"id":"U1Psdby60i2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Adversarial Attack"],"metadata":{"id":"uGH0whRF0iog"}},{"cell_type":"code","source":["# PGD Attack\n","# MNIST init\n","def pgd_attack(model, images, labels, eps=0.3, alpha=2/255, iters=40) :\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    loss = nn.CrossEntropyLoss()\n","        \n","    ori_images = images.data\n","        \n","    for i in range(iters) :    \n","        images.requires_grad = True\n","        outputs = model(images)\n","\n","        model.zero_grad()\n","        cost = loss(outputs, labels).to(device)\n","        cost.backward()\n","\n","        adv_images = images + alpha*images.grad.sign()\n","        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n","        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n","            \n","    return images"],"metadata":{"id":"74SJAYG60nBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Attack Image & Predicted Label\")\n","\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","for images, labels in normal_loader:\n","    \n","    images = pgd_attack(model, images, labels)\n","    labels = labels.to(device)\n","    outputs = model(images)\n","    \n","    _, pre = torch.max(outputs.data, 1)\n","\n","    total += 1\n","    correct += (pre == labels).sum()\n","    \n","    imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [normal_data.classes[i] for i in pre])\n","    \n","print('Accuracy of test text: %f %%' % (100 * float(correct) / total))"],"metadata":{"id":"sO5oRFBL0r5e"},"execution_count":null,"outputs":[]}]}