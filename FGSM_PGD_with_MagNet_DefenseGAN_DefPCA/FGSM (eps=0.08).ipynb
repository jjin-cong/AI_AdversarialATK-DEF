{"cells":[{"cell_type":"markdown","source":["데이터셋 설정 관련\n","* for문으로 돌리려다가, 방어기법에서까지 하나하나 돌리는게 너무 오래 걸려서 코드로 나누고 계정별로 돌릴 수 있게 코드 변경했습니다\n","* 공격 종류 (FGSM, PGD), 공격 입실론 (0.02 등)을 목차 [Attack 수행 : 공격 데이터셋 만드는 코드] 부분에서 수정하고 코드 돌리면 됩니다\n","\n","1. 모든 train valid 데이터는 0~1사이로 정규화 되어있음.\n","2. 모든 test 데이터는 0~255로 정규화 안되어있음\n","3. target 값은 정규화하면 안됨."],"metadata":{"id":"05NggkxjGxlN"}},{"cell_type":"markdown","source":["# 기본 import"],"metadata":{"id":"H5r08xIXerXB"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"WgKss5hJBb5B","executionInfo":{"status":"ok","timestamp":1664034784499,"user_tz":-540,"elapsed":2814,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import os\n","import pathlib\n","\n","import cv2 #영상처리에 사용하는 오픈소스 라이브러리, 컴퓨터가 사람 눈처럼 인식할 수 있게 처리\n","from PIL import Image # 파이썬 이미지 처리 pillow 라이브러리\n","from tensorflow.keras.preprocessing import image\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator #imagedatagenerater는 이미지를 학습시킬 때 학습 데이터의 양이 적을 경우 학습데이터를 조금씩 변형 시켜서 학습데이터의 양을 늘리는 방식중 하나\n","from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from tqdm.auto import tqdm\n","\n","#난수 랜덤성 고정\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25888,"status":"ok","timestamp":1664034810383,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"},"user_tz":-540},"id":"Qn7HC1qiBnNE","outputId":"9045bd67-91fb-4cce-aa64-a7c8751dbdac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3113,"status":"ok","timestamp":1664034813493,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"},"user_tz":-540},"id":"VwTVXhm7BoaE","outputId":"a328dc5c-660a-49c3-eaf1-d1a62c5cbe95"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1WKEjdIyqtzI-NV5o0O_ixsHslngaSiQX/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB\n"]}],"source":["cd drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB"]},{"cell_type":"markdown","source":["# Train & Test 데이터 불러오기"],"metadata":{"id":"FhnfS-0MH1sa"}},{"cell_type":"markdown","metadata":{"id":"JjBcm524Fams"},"source":["Train Data 불러오기"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kZnmTWHIBo_E","executionInfo":{"status":"ok","timestamp":1664034813494,"user_tz":-540,"elapsed":7,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["import numpy as np\n","import os\n","import gzip\n","import urllib.request\n","\n","from keras.models import load_model\n","\n","def ordered_onehotencoding(labels):\n","    labels_ordered = []\n","    for i in range(len(labels)):\n","        if labels[i] == 3:\n","            labels_ordered.append(0)\n","        elif labels[i] == 7:\n","            labels_ordered.append(1)\n","        elif labels[i] == 9:\n","            labels_ordered.append(2)\n","        elif labels[i] == 10:\n","            labels_ordered.append(3)\n","        elif labels[i] == 11:\n","            labels_ordered.append(4)\n","        elif labels[i] == 12:\n","            labels_ordered.append(5)\n","        elif labels[i] == 13:\n","            labels_ordered.append(6)\n","        elif labels[i] == 17:\n","            labels_ordered.append(7)\n","        elif labels[i] == 18:\n","            labels_ordered.append(8)\n","        elif labels[i] == 25:\n","            labels_ordered.append(9)\n","        elif labels[i] == 35:\n","            labels_ordered.append(10)\n","        elif labels[i] == 38:\n","            labels_ordered.append(11)\n","    \n","    return np.array(labels_ordered)\n","\n","class GTSRB:\n","    def __init__(self):\n","        imgs_path = \"Train\"\n","        data_list = []\n","        labels_list = []\n","\n","        result_class = [3,7, 9, 10, 11, 12, 13, 17, 18, 25, 35, 38]\n","\n","        for i in result_class:\n","            i_path = os.path.join(imgs_path, str(i)) # 3, 7, 9, 10, 11, 12,13, 17, 18, 25, 35, 38\n","            num = 0\n","            for img in os.listdir(i_path):\n","          \n","                im = Image.open(i_path +'/'+ img)\n","                im = im.resize((32,32))\n","                im = np.array(im)\n","\n","                data_list.append(im)\n","                labels_list.append(i)\n","                num = num + 1\n","                if num == 1000:\n","                    break;\n","\n","        data = np.array(data_list)\n","        labels = ordered_onehotencoding(labels_list)\n","\n","        labels = to_categorical(labels)\n","\n","        VALIDATION_SIZE = 5000\n","        \n","        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(np.array(data), labels, test_size=0.4)    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lWo9BzndEm67","executionInfo":{"status":"ok","timestamp":1664035207767,"user_tz":-540,"elapsed":394279,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["data = GTSRB()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Zc5Q_EXiEppb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664035207767,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"933356d6-83de-4ae7-8904-d2d14de278d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["(7200, 32, 32, 3)\n","(4800, 32, 32, 3)\n","(7200, 12)\n","(4800, 12)\n"]}],"source":["print(data.x_train.shape) # 0~255\n","print(data.x_test.shape) # 0~255\n","print(data.y_train.shape) # 0~11 원핫 인코딩\n","print(data.y_test.shape) # 0~11 원핫 인코딩"]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtM8GsuYfDFc","executionInfo":{"status":"ok","timestamp":1664035207767,"user_tz":-540,"elapsed":10,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"84870e5f-a020-40ca-8c14-09623930ed62"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","255\n","0\n","255\n"]}]},{"cell_type":"code","source":["data.x_train, data.y_train, data.x_test, data.y_test =data.x_train/255, data.y_train, data.x_test/255, data.y_test #0~1로 찐 train만 정규화"],"metadata":{"id":"L88pCixpf4wC","executionInfo":{"status":"ok","timestamp":1664035207768,"user_tz":-540,"elapsed":8,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoKSRN2ef-n0","executionInfo":{"status":"ok","timestamp":1664035207768,"user_tz":-540,"elapsed":8,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"079c133a-68dd-4b0a-ef29-d2718757ef06"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","0.0\n","1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"P2uSEM66FdVa"},"source":["Test Data 불러오기"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"_shX58HHFisj","executionInfo":{"status":"ok","timestamp":1664035210450,"user_tz":-540,"elapsed":2687,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["metainfo = pd.read_csv(\"Meta.csv\")\n","traininfo = pd.read_csv(\"Train.csv\")\n","testinfo = pd.read_csv(\"Test.csv\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"z9HPAPg2FZ9L","executionInfo":{"status":"ok","timestamp":1664035210450,"user_tz":-540,"elapsed":4,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["import natsort\n","\n","class GTSRB_test:\n","    def __init__(self):\n","        imgs_path = \"Test\"\n","        data_list = []\n","        labels_list = []\n","        \n","        for img in natsort.natsorted(os.listdir(imgs_path)):\n","            im = Image.open(imgs_path +'/'+ img)\n","            im = im.resize((32,32))\n","            im = np.array(im)\n","            data_list.append(im)\n","        data_test = np.array(data_list)\n","        \n","        for i in range(len(testinfo.ClassId)):\n","            labels_list.append(testinfo.ClassId[i])\n","        \n","        labels_test = np.array(labels_list)\n","\n","        labels_test_index = []\n","        for i in range(len(labels_test)):\n","            if (labels_test[i] == 3) | (labels_test[i] == 7) | (labels_test[i] == 9) | (labels_test[i] == 10) | (labels_test[i] == 11) | (labels_test[i] == 12) | (labels_test[i] == 13) | (labels_test[i] == 17) | (labels_test[i] == 18) | (labels_test[i] == 25) | (labels_test[i] == 35) | (labels_test[i] == 38):\n","                labels_test_index.append(i)\n","\n","        test_data = []\n","        test_label = []\n","        for i in labels_test_index:\n","            test_data.append(data_test[i])\n","            test_label.append(labels_test[i])\n","\n","        data_test = np.array(test_data)\n","\n","        labels_test =ordered_onehotencoding(test_label)\n","\n","        labels_test = to_categorical(labels_test)\n","        \n","        self.x_test = data_test\n","        self.y_test = labels_test    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB_test\""]},{"cell_type":"code","execution_count":12,"metadata":{"id":"CcWfJaa6Ft7z","executionInfo":{"status":"ok","timestamp":1664035431476,"user_tz":-540,"elapsed":221029,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["data_test = GTSRB_test()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"VE9NTZrAFv7e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664035431477,"user_tz":-540,"elapsed":28,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"27bedfa9-0fbb-467d-b578-a661698fe451"},"outputs":[{"output_type":"stream","name":"stdout","text":["(6180, 32, 32, 3)\n","(6180, 12)\n"]}],"source":["print(data_test.x_test.shape) # 0~255\n","print(data_test.y_test.shape) # 12개 원핫 인코딩"]},{"cell_type":"code","source":["print(np.min(data_test.x_test))\n","print(np.max(data_test.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phupmxLufTsN","executionInfo":{"status":"ok","timestamp":1664035431477,"user_tz":-540,"elapsed":24,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"c0ed0e25-ce8e-4a0e-e96a-acc3557bbe34"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","255\n"]}]},{"cell_type":"markdown","metadata":{"id":"XWfrLWt3EtSN"},"source":["# 분류기 : CNN"]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ll78wYPewyM","executionInfo":{"status":"ok","timestamp":1664035431477,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"27fe223a-61f7-4808-a748-3524359578c2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","0.0\n","1.0\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"gI5nIfEQE0Dt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664035484098,"user_tz":-540,"elapsed":52639,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"06887d5a-8261-4b92-b220-1fab7a67aed0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 30, 30, 96)        2688      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 15, 15, 96)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 15, 15, 96)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 13, 13, 192)       166080    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 6, 6, 192)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 6, 6, 192)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 192)         331968    \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                196672    \n","                                                                 \n"," dense_1 (Dense)             (None, 12)                780       \n","                                                                 \n","=================================================================\n","Total params: 698,188\n","Trainable params: 698,188\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/30\n","60/60 [==============================] - 14s 27ms/step - loss: 0.3272 - accuracy: 0.1235 - val_loss: 0.2785 - val_accuracy: 0.3521\n","Epoch 2/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.2043 - accuracy: 0.4944 - val_loss: 0.1296 - val_accuracy: 0.7960\n","Epoch 3/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0787 - accuracy: 0.8703 - val_loss: 0.0477 - val_accuracy: 0.9329\n","Epoch 4/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0352 - accuracy: 0.9567 - val_loss: 0.0266 - val_accuracy: 0.9725\n","Epoch 5/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0217 - accuracy: 0.9739 - val_loss: 0.0177 - val_accuracy: 0.9806\n","Epoch 6/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0150 - accuracy: 0.9847 - val_loss: 0.0130 - val_accuracy: 0.9856\n","Epoch 7/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0111 - accuracy: 0.9885 - val_loss: 0.0100 - val_accuracy: 0.9912\n","Epoch 8/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0086 - accuracy: 0.9924 - val_loss: 0.0091 - val_accuracy: 0.9910\n","Epoch 9/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0068 - accuracy: 0.9946 - val_loss: 0.0076 - val_accuracy: 0.9937\n","Epoch 10/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0059 - accuracy: 0.9949 - val_loss: 0.0069 - val_accuracy: 0.9940\n","Epoch 11/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0040 - accuracy: 0.9975 - val_loss: 0.0059 - val_accuracy: 0.9948\n","Epoch 12/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0033 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9952\n","Epoch 13/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0028 - accuracy: 0.9978 - val_loss: 0.0056 - val_accuracy: 0.9948\n","Epoch 14/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0024 - accuracy: 0.9985 - val_loss: 0.0052 - val_accuracy: 0.9956\n","Epoch 15/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0022 - accuracy: 0.9982 - val_loss: 0.0055 - val_accuracy: 0.9952\n","Epoch 16/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9952\n","Epoch 17/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0019 - accuracy: 0.9985 - val_loss: 0.0045 - val_accuracy: 0.9956\n","Epoch 18/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9956\n","Epoch 19/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9960\n","Epoch 20/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9956\n","Epoch 21/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0046 - val_accuracy: 0.9960\n","Epoch 22/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0015 - accuracy: 0.9989 - val_loss: 0.0059 - val_accuracy: 0.9942\n","Epoch 23/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9958\n","Epoch 24/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0040 - val_accuracy: 0.9967\n","Epoch 25/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0038 - val_accuracy: 0.9971\n","Epoch 26/30\n","60/60 [==============================] - 1s 19ms/step - loss: 6.0403e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9965\n","Epoch 27/30\n","60/60 [==============================] - 1s 19ms/step - loss: 5.9919e-04 - accuracy: 0.9997 - val_loss: 0.0039 - val_accuracy: 0.9965\n","Epoch 28/30\n","60/60 [==============================] - 1s 18ms/step - loss: 3.7970e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9971\n","Epoch 29/30\n","60/60 [==============================] - 1s 24ms/step - loss: 4.4506e-04 - accuracy: 0.9999 - val_loss: 0.0052 - val_accuracy: 0.9956\n","Epoch 30/30\n","60/60 [==============================] - 1s 19ms/step - loss: 6.1414e-04 - accuracy: 0.9997 - val_loss: 0.0045 - val_accuracy: 0.9969\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","import tensorflow as tf\n","import os\n","\n","\n","def train(data, file_name, num_epochs=50, batch_size=128):\n","    \"\"\"\n","    Standard neural network training procedure.\n","    \"\"\"\n","    model = Sequential()\n","\n","    IMG_HEIGHT = 32\n","    IMG_WIDTH = 32\n","\n","    # 첫번째 Convolutional Layer : 입력 데이터로부터 특징을 추출\n","    model.add(Conv2D(filters=96, kernel_size=3, activation='relu', input_shape=data.x_train.shape[1:]))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25))\n","\n","    # 두번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25)) # 인풋데이터의 25%를 무작위로 0으로 만듦\n","\n","    # 세번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu')) # 특징을 추출하는 기능을 하는 필터, 비선형 값으로 바꿔주는 activation 함수->relu\n","    # model.add(GlobalAveragePooling2D())\n","    model.add(Flatten())\n","\n","    model.add(Dense(units=64, activation='relu'))\n","    model.add(Dense(12, activation='softmax'))\n","\n","\n","    # 모델 컴파일 하기\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","\n","    # 모델 핏하기\n","    EPOCHS = num_epochs\n","    model.fit(data.x_train, data.y_train,\n","              validation_data = (data.x_test, data.y_test), \n","              epochs=EPOCHS, steps_per_epoch=60\n","              )\n","\n","    if file_name != None:\n","        model.save(file_name)\n","\n","    return model\n","\n","\n","if not os.path.isdir('models'):\n","    os.makedirs('models')\n","\n","model = train(data, None, num_epochs=30)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Ez-W2phKFLws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664035485387,"user_tz":-540,"elapsed":1304,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"b1a011d4-332a-4581-8660-647faf1161fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["225/225 [==============================] - 1s 3ms/step - loss: 1.5156e-04 - accuracy: 1.0000\n","train set accuracy:  100.0\n","150/150 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 0.9969\n","valid set accuracy:  99.6874988079071\n"]}],"source":["loss, accuracy = model.evaluate(data.x_train, data.y_train)\n","\n","print('train set accuracy: ', accuracy * 100) #train 성능\n","\n","loss, accuracy = model.evaluate(data.x_test, data.y_test)\n","\n","print('valid set accuracy: ', accuracy * 100) #val 성능"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xbYmfSHYHOIa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664035487302,"user_tz":-540,"elapsed":1917,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"581b9f74-20ea-4ca9-e5a9-650b25505bb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["194/194 [==============================] - 1s 4ms/step - loss: 0.0308 - accuracy: 0.9748\n","test set accuracy with nomalization:  97.47572541236877\n"]}],"source":["loss, accuracy = model.evaluate(data_test.x_test/255, data_test.y_test)\n","\n","print('test set accuracy with nomalization: ', accuracy * 100) #찐 test"]},{"cell_type":"markdown","source":["# 공격 데이터셋 : FGSM & PGD"],"metadata":{"id":"QNRCIvO3-8lf"}},{"cell_type":"code","source":["def tf_preprocess(image):\n","  image = tf.cast(image, tf.float32)\n","  image = image/255\n","  image = tf.image.resize(image, (32, 32))\n","  image = image[None, ...]\n","  return image\n","\n","# 확률 벡터에서 레이블을 추출해주는 헬퍼 메서드\n","def get_tf_label(labels):\n","    label = tf.cast(labels, tf.int32)\n","    label = tf.reshape(label,[1,12])\n","    return label\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","\n","def create_adversarial_pattern(input_image, input_label):\n","  with tf.GradientTape() as tape:\n","    tape.watch(input_image)\n","    input_img = tf.reshape(input_image,[1,32,32,3])\n","    prediction = model(input_img)\n","    loss = loss_object(input_label, prediction)\n","\n","  # 입력 이미지에 대한 손실 함수의 기울기를 구합니다.\n","  gradient = tape.gradient(loss, input_image)\n","  # 왜곡을 생성하기 위해 그래디언트의 부호를 구합니다.\n","  signed_grad = tf.sign(gradient)\n","  return signed_grad"],"metadata":{"id":"Dn9x64obA9UB","executionInfo":{"status":"ok","timestamp":1664035487303,"user_tz":-540,"elapsed":9,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SUb6-MoJtay"},"source":["### FGSM & PGD define"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"DGBdi7tWJT1K","executionInfo":{"status":"ok","timestamp":1664035487303,"user_tz":-540,"elapsed":9,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["def fgsm_attack(model,test_x,test_y,eps):\n","    \n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # FGSM 공격 수행\n","        perturbations = create_adversarial_pattern(img, label)\n","        adv_x = img + eps * perturbations\n","        adv_x = tf.clip_by_value(adv_x, 0, 1) # 공격받은 이미지\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(target_label)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"]},{"cell_type":"code","source":["def pgd_attack(model,test_x,test_y,eps,step_size=2,num_steps=7): \n","    \"\"\"\n","    FGSM 코드와 차이점\n","    - step_size, num_steps 파라미터 추가됨\n","    - unifrom distribution 코드 추가\n","    - FGSM 공격 수행 -> PGD 공격 수행\n","    ** 모든 return 형식은 동일함\n","    \n","    default 값\n","    - step_size = 2 (alpha 값)\n","    - num_steps = 7 (iterations 값)\n","\n","    \"\"\"\n","\n","    prog = 0 # 진행상황 확인용 변수\n","\n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # PGD uniform distribution 코드\n","        data = data + np.random.uniform(-eps,eps,data.shape)\n","        data = np.clip(data,0,255)\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리 -> 0~1사이로 정규화 함.\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # PGD 공격 수행\n","        adv_x = img # 공격받은 이미지 (for문으로 업데이트)\n","        for num_step in range(num_steps):\n","          perturbations = create_adversarial_pattern(adv_x,label) # signed_grad를 리턴한 값\n","          adv_x += step_size * perturbations\n","          adv_x = tf.clip_by_value(adv_x,img-eps,img+eps)\n","          adv_x = tf.clip_by_value(adv_x,0,1)\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(target_label) #공격 받은 이미지에 원래 타겟 값을 저장합니다 ! \n","\n","        prog += 1\n","\n","        if prog%500 == 0:\n","          print(\"prog :\", prog)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"],"metadata":{"id":"GmiLgFduVhR0","executionInfo":{"status":"ok","timestamp":1664035487303,"user_tz":-540,"elapsed":8,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Attack 수행 : 공격 데이터셋 만드는 코드\n","\n","**여기에서 [attack_type]과 [eps] 설정하면 됩니다!!!**\n","\n","* attack_type : FGSM, PGD\n","* eps = 0.02, 0.03, 8/255, 0.05, 0.08, 0.10\n","* 정상 이미지에 대한 분류기 정확도 -> 위에 있음 (분류기:CNN; 약 97%)"],"metadata":{"id":"AVs5AnagBdTK"}},{"cell_type":"code","source":["################################################################################\n","# 공격 데이터 설정 #################################################################\n","attack_type = \"FGSM\" # FGSM, PGD\n","eps = 0.08 # 0.02, 0.03, 0.04, 0.05, 0.08, 0.10\n","################################################################################\n","################################################################################\n","\n","if attack_type == \"FGSM\":\n","  acc, ex, ad_examples, orig_labels = fgsm_attack(model, data_test.x_test, data_test.y_test, eps) #x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\n","elif attack_type == \"PGD\":\n","  acc, ex, ad_examples, orig_labels = pgd_attack(model, data_test.x_test, data_test.y_test, eps) #x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\n","\n","print(\"Attack Data 생성 완료\")"],"metadata":{"id":"ZE2wZtfAEOBh","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"error","timestamp":1664035528610,"user_tz":-540,"elapsed":41313,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}},"outputId":"0e1528f9-d8ca-40eb-a303-f8636e113691"},"execution_count":22,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5224\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5226\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    512\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-d47537f99534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattack_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"FGSM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mad_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mattack_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PGD\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mad_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-bd0e95358264>\u001b[0m in \u001b[0;36mfgsm_attack\u001b[0;34m(model, test_x, test_y, eps)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mresult_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# one-hot 형태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresult_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1959\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 361\u001b[0;31m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2022\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m           \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5237\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5238\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m     \"\"\"\n\u001b[1;32m   3070\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3071\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3072\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m       captured = object_identity.ObjectIdentitySet(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    246\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mgrab_batch\u001b[0;34m(i, data)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     dataset = dataset.map(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     dataset = dataset.map(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   5242\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m       \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5244\u001b[0;31m       batch_dims=batch_dims)\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                 instructions)\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5225\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3952\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m   3953\u001b[0m         \u001b[0;34m\"GatherV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3954\u001b[0;31m                     batch_dims=batch_dims, name=name)\n\u001b[0m\u001b[1;32m   3955\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m    267\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 268\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    288\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m--> 290\u001b[0;31m       \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_invoke_op_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    693\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    694\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3782\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3784\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3785\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2173\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack_for_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_stack.py\u001b[0m in \u001b[0;36mextract_stack_for_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    183\u001b[0m   return _tf_stack.extract_stack_for_node(\n\u001b[1;32m    184\u001b[0m       \u001b[0m_source_mapper_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m       _source_filter_stacks[thread_key][-1].internal_set, node)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(orig_labels[0])\n","plt.figure()\n","plt.imshow(ad_examples[0])\n","plt.show();"],"metadata":{"id":"Fjghmlv4kRKa","executionInfo":{"status":"aborted","timestamp":1664035528611,"user_tz":-540,"elapsed":6,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 방어모델 : MagNet, Defense-GAN, PCA"],"metadata":{"id":"6DfDZcE5_GGB"}},{"cell_type":"markdown","source":["## MagNet"],"metadata":{"id":"Lb-doZmmIFqj"}},{"cell_type":"markdown","source":["### MagNet - def (utils, worker, Defensive Model)"],"metadata":{"id":"-Yc-yrW7ktMW"}},{"cell_type":"markdown","source":["#### MagNet - utils"],"metadata":{"id":"bt4nIJ5xg73T"}},{"cell_type":"code","source":["## utils.py -- utility functions\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","def prepare_data(dataset, idx):\n","    \"\"\"\n","    Extract data from index.\n","\n","    dataset: Full, working dataset. Such as MNIST().\n","    idx: Index of test examples that we care about.\n","    return: X, targets, Y\n","    \"\"\"\n","    return dataset.x_test[idx], dataset.y_test[idx], np.argmax(dataset.y_test[idx], axis=1)\n","\n","\n","def save_obj(obj, name, directory='./attack_data/'):\n","    with open(os.path.join(directory, name + '.pkl'), 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","\n","def load_obj(name, directory='./attack_data/'):\n","    if name.endswith(\".pkl\"): name = name[:-4]\n","    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n","        return pickle.load(f)"],"metadata":{"id":"hAvlahKmMFdO","executionInfo":{"status":"aborted","timestamp":1664035528612,"user_tz":-540,"elapsed":6,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - worker"],"metadata":{"id":"EhncucofhEvv"}},{"cell_type":"code","source":["## setup_mnist.py -- mnist data and model loading code\n","##\n","## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","## Modified for MagNet's use.\n","\n","## worker.py -- evaluation code\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","from scipy.stats import entropy\n","from numpy.linalg import norm\n","from matplotlib.ticker import FuncFormatter\n","from keras.models import Sequential, load_model\n","from keras.activations import softmax\n","from keras.layers import Lambda\n","import numpy as np\n","import pylab\n","import os\n","import matplotlib.pyplot as plt\n","\n","\n","class AEDetector:\n","    def __init__(self, path, p=1):\n","        \"\"\"\n","        Error based detector.\n","        Marks examples for filtering decisions.\n","\n","        path: Path to the autoencoder used.\n","        p: Distance measure to use.\n","        \"\"\"\n","\n","        self.model = load_model(path)\n","        self.path = path\n","        self.p = p\n","\n","    def mark(self, X):\n","        diff = np.abs(X - self.model.predict(X)) # input X와 예측값 X'(autoencoder를 통해 노이즈가 더해진 값) 의 오차 값\n","        marks = np.mean(np.power(diff, self.p), axis=(1,2,3)) # 오차값의 분산\n","        return marks\n","\n","    def print(self):\n","        return \"AEDetector:\" + self.path.split(\"/\")[-1]\n","\n","\n","class IdReformer:\n","    def __init__(self, path=\"IdentityFunction\"):\n","        \"\"\"\n","        Identity reformer.\n","        Reforms an example to itself.\n","        \"\"\"\n","        self.path = path\n","        self.heal = lambda X: X\n","\n","    def print(self):\n","        return \"IdReformer:\" + self.path\n","\n","\n","class SimpleReformer:\n","    def __init__(self, path):\n","        \"\"\"\n","        Reformer.\n","        Reforms examples with autoencoder. Action of reforming is called heal.\n","\n","        path: Path to the autoencoder used.\n","        \"\"\"\n","        self.model = load_model(path)\n","        self.path = path\n","\n","    def heal(self, X):\n","        X = self.model.predict(X) # autoencoder로 X값 재구성\n","        return np.clip(X, 0.0, 1.0)\n","\n","    def print(self):\n","        return \"SimpleReformer:\" + self.path.split(\"/\")[-1]\n","\n","\n","def JSD(P, Q):\n","    _P = P / norm(P, ord=1)\n","    _Q = Q / norm(Q, ord=1)\n","    _M = 0.5 * (_P + _Q)\n","    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M)) # Xp와 Xr의 분포의 entropy \n","    # KL divergence: Q(one autoencoder)를 기반으로 했을 때의 cross entropy와 P(magnet)를 기반으로 했을 때의 entropy의 차이\n","\n","\n","\n","class DBDetector:\n","    def __init__(self, reconstructor, prober, classifier, option=\"jsd\", T=1):\n","        \"\"\"\n","        Divergence-Based Detector.\n","\n","        reconstructor: One autoencoder.\n","        prober: Another autoencoder.\n","        classifier: Classifier object.\n","        option: Measure of distance, jsd as default.\n","        T: Temperature to soften the classification decision.\n","        \"\"\"\n","        self.prober = prober\n","        self.reconstructor = reconstructor\n","        self.classifier = classifier\n","        self.option = option\n","        self.T = T\n","\n","    def mark(self, X):\n","        return self.mark_jsd(X)\n","\n","    def mark_jsd(self, X):\n","        Xp = self.prober.heal(X) # 1번 autoencoder로 생성한 이미지 \n","        Xr = self.reconstructor.heal(X) #2번 autoencoder로 생성한 이미지 \n","        Pp = self.classifier.classify(Xp, option=\"prob\", T=self.T) # Xp의 확률\n","        Pr = self.classifier.classify(Xr, option=\"prob\", T=self.T) # Xr의 확률\n","\n","        marks = [(JSD(Pp[i], Pr[i])) for i in range(len(Pr))]\n","        return np.array(marks)\n","\n","    def print(self):\n","        return \"Divergence-Based Detector\"\n","\n","\n","class Classifier:\n","    def __init__(self, classifier_path):\n","        \"\"\"\n","        Keras classifier wrapper.\n","        Note that the wrapped classifier should spit logits as output.\n","\n","        classifier_path: Path to Keras classifier file.\n","        \"\"\"\n","        self.path = classifier_path\n","        self.model = load_model(classifier_path)\n","        self.softmax = Sequential()\n","        self.softmax.add(Lambda(lambda X: softmax(X, axis=1)))\n","\n","    def classify(self, X, option=\"logit\", T=1):\n","        if option == \"logit\":\n","            return self.model.predict(X)\n","        if option == \"prob\":\n","            logits = self.model.predict(X)/T\n","            return self.softmax.predict(logits)\n","\n","    def print(self):\n","        return \"Classifier:\"+self.path.split(\"/\")[-1]\n","\n","\n","class Operator:\n","    def __init__(self, data, classifier, det_dict, reformer):\n","        \"\"\"\n","        Operator.\n","        Describes the classification problem and defense.\n","\n","        data: Standard problem dataset. Including train, test, and validation.\n","        classifier: Target classifier.\n","        reformer: Reformer of defense.\n","        det_dict: Detector(s) of defense.\n","        \"\"\"\n","\n","        self.data = data\n","        self.classifier = classifier\n","        self.det_dict = det_dict \n","        self.reformer = reformer\n","        self.normal = self.operate(AttackData(data.x_train, np.argmax(data.y_train, axis=1), \"Normal\"))\n","        \n","\n","    def get_thrs(self, drop_rate):\n","        \"\"\"\n","        Get filtering threshold by marking validation set.\n","        \"\"\"\n","        thrs = dict()\n","        for name, detector in self.det_dict.items():\n","            num = int(len(data.x_test) * drop_rate[name])\n","            marks = detector.mark(data.x_test)\n","            marks = np.sort(marks)\n","            thrs[name] = marks[-num]\n","        return thrs\n","\n","    def operate(self, untrusted_obj):\n","        \"\"\"\n","        For untrusted input(normal or adversarial), classify original input and\n","        reformed input. Classifier is unaware of the source of input.\n","\n","        untrusted_obj: Input data.\n","        \"\"\"\n","\n","        X = untrusted_obj.data\n","        Y_true = untrusted_obj.labels\n","\n","\n","        X_prime = self.reformer.heal(X) # autoencoder 값으로 재구성\n","        Y = np.argmax(self.classifier.classify(X), axis=1) # 원본 input X 분류\n","        Y_judgement = (Y == Y_true[:len(X_prime)]) # 실제 label과 X 분류 label 비교\n","        Y_prime = np.argmax(self.classifier.classify(X_prime), axis=1)  # autoencoder로 재구성한 X' 분류\n","        Y_prime_judgement = (Y_prime == Y_true[:len(X_prime)])  # 실제 label과 X' 분류 label 비교\n","        return np.array(list(zip(Y_judgement, Y_prime_judgement)))\n","\n","    def filter(self, X, thrs):\n","        \"\"\"\n","        untrusted_obj: Untrusted input to test against.\n","        thrs: Thresholds.\n","\n","        return:\n","        all_pass: Index of examples that passed all detectors.\n","        collector: Number of examples that escaped each detector.\n","        \"\"\"\n","        collector = dict()\n","        all_pass = np.array(range(10000)) #Index\n","        for name, detector in self.det_dict.items():\n","            marks = detector.mark(X) #  KL divergnece: Xp와 Xr의 분포의 entropy \n","            idx_pass = np.argwhere(marks < thrs[name]) # KL divergnece가 thershold보다 작을 경우 pass, 클 경우 reject\n","            collector[name] = len(idx_pass) # pass가 된 수\n","            all_pass = np.intersect1d(all_pass, idx_pass) # 전체 index array와 pass된 array의 교집합\n","        return all_pass, collector\n","\n","    def print(self):\n","        components = [self.reformer, self.classifier]\n","        return \" \".join(map(lambda obj: getattr(obj, \"print\")(), components))\n","\n","\n","class AttackData:\n","    def __init__(self, examples, labels, name=\"\"):\n","        \"\"\"\n","        Input data wrapper. May be normal or adversarial.\n","\n","        examples: Path or object of input examples.\n","        labels: Ground truth labels.\n","        \"\"\"\n","        # if isinstance(examples, str): \n","        #   self.data = load_obj(examples)\n","        # else: \n","\n","        self.data = examples\n","        self.labels = labels\n","        self.name = name\n","\n","    def print(self):\n","        return \"Attack:\"+self.name\n","\n","\n","class Evaluator:\n","    def __init__(self, operator, untrusted_data, graph_dir=\"./graph\"):\n","        \"\"\"\n","        Evaluator.\n","        For strategy described by operator, conducts tests on untrusted input.\n","        Mainly stats and plotting code. Most methods omitted for clarity.\n","\n","        operator: Operator object.\n","        untrusted_data: Input data to test against.\n","        graph_dir: Where to spit the graphs.\n","        \"\"\"\n","        self.operator = operator\n","        self.untrusted_data = untrusted_data # attacked data\n","        self.graph_dir = graph_dir\n","        self.data_package = operator.operate(untrusted_data)\n","\n","    def bind_operator(self, operator):\n","        self.operator = operator\n","        self.data_package = operator.operate(self.untrusted_data)\n","\n","    def load_data(self, data):\n","        self.untrusted_data = data\n","        self.data_package = self.operator.operate(self.untrusted_data)\n","\n","    def get_normal_acc(self, normal_all_pass):\n","        \"\"\"\n","        traning data에 대한 정확도\n","\n","        Break down of who does what in defense. Accuracy of defense on normal\n","        input.\n","\n","        both: Both detectors and reformer take effect\n","        det_only: detector(s) take effect\n","        ref_only: Only reformer takes effect\n","        none: Attack effect with no defense\n","        \"\"\"\n","        normal_tups = self.operator.normal\n","        num_normal = len(normal_tups)\n","        filtered_normal_tups = normal_tups[normal_all_pass]\n","\n","        both_acc = sum(1 for _, XpC in filtered_normal_tups if XpC)/num_normal # detector and refomer\n","        det_only_acc = sum(1 for XC, XpC in filtered_normal_tups if XC)/num_normal # only detector\n","        ref_only_acc = sum([1 for _, XpC in normal_tups if XpC])/num_normal # only reformer\n","        none_acc = sum([1 for XC, _ in normal_tups if XC])/num_normal # no defense\n","\n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def get_attack_acc(self, attack_pass):\n","        \"\"\"\n","        attacked data에 대한 정확도 \n","        \"\"\"\n","        attack_tups = self.data_package\n","        num_untrusted = len(attack_tups)\n","        filtered_attack_tups = attack_tups[attack_pass]\n","\n","\n","        both_acc = 1 - sum(1 for _, XpC in filtered_attack_tups if not XpC)/num_untrusted # detector and refomer\n","        det_only_acc = 1 - sum(1 for XC, XpC in filtered_attack_tups if not XC)/num_untrusted # only detector\n","        ref_only_acc = sum([1 for _, XpC in attack_tups if XpC])/num_untrusted # only reformer\n","        none_acc = sum([1 for XC, _ in attack_tups if XC])/num_untrusted # no defense\n","        \n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def plot_various_confidences(self, graph_name, drop_rate,\n","                                 idx_file=\"example_idx\",\n","                                 confs=(0.0, 10.0),\n","                                 get_attack_data_name=lambda c: \"example_carlini_\"+str(c)):\n","        \"\"\"\n","        Test defense performance against Carlini L2 attack of various confidences.\n","\n","        graph_name: Name of graph file.\n","        drop_rate: How many normal examples should each detector drops?\n","        idx_file: Index of adversarial examples in standard test set.\n","        confs: A series of confidence to test against.\n","        get_attack_data_name: Function mapping confidence to corresponding file.\n","        \"\"\"\n","        pylab.rcParams['figure.figsize'] = 6, 4\n","        fig = plt.figure(1, (6, 4))\n","        ax = fig.add_subplot(1, 1, 1)\n","\n","        idx = orig_labels\n","        # idx = original_labels_list\n","        X, _, Y = prepare_data(self.operator.data, idx)\n","\n","\n","        det_only = []\n","        ref_only = []\n","        both = []\n","        none = []\n","\n","        print(\"\\n==========================================================\")\n","        print(\"Drop Rate:\", drop_rate)\n","        thrs = self.operator.get_thrs(drop_rate)\n","\n","        all_pass, _ = self.operator.filter(self.operator.data.x_train, thrs)\n","        all_on_acc, _, _, _ = self.get_normal_acc(all_pass)\n","\n","        print(\"Classification accuracy with all defense on:\", all_on_acc)\n","\n","        for confidence in confs:\n","            # f = get_attack_data_name(confidence)\n","            self.load_data(AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\"))\n","\n","            print(\"----------------------------------------------------------\")\n","            print(\"Confidence:\", confidence)\n","            all_pass, detector_breakdown = self.operator.filter(self.untrusted_data.data, thrs)\n","            both_acc, det_only_acc, ref_only_acc, none_acc = self.get_attack_acc(all_pass)\n","            print(detector_breakdown)\n","            both.append(both_acc)\n","            det_only.append(det_only_acc)\n","            ref_only.append(ref_only_acc)\n","            none.append(none_acc)\n","\n","        size = 2.5\n","\n","        print(\"With detector & reformer: \", both_acc)\n","        print(\"With detector: \",det_only_acc)\n","        print(\"With reformer: \", ref_only_acc)\n","        print(\"No Defense: \",none_acc)\n","\n","        # print(\"With detector & reformer: \", both)\n","        # print(\"With detector: \",det_only)\n","        # print(\"With reformer: \", ref_only)\n","        # print(\"No Defense: \",none)\n","\n","        plt.plot(confs, none, c=\"green\", label=\"No Defense\", marker=\"x\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, det_only, c=\"orange\", label=\"With detector\", marker=\"o\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, ref_only, c=\"blue\", label=\"With reformer\", marker=\"^\", markersize=size,alpha=0.5)\n","        plt.plot(confs, both, c=\"red\", label=\"With detector & reformer\", marker=\"s\", markersize=size,alpha=0.5)\n","\n","        pylab.legend(loc='lower left', bbox_to_anchor=(0.02, 0.1), prop={'size':8})\n","        plt.grid(linestyle='dotted')\n","        plt.xlabel(r\"Confidence in Carlini $L^2$ attack\")\n","        plt.ylabel(\"Classification accuracy\")\n","        plt.xlim(min(confs)-1.0, max(confs)+1.0)\n","        plt.ylim(-0.05, 1.05)\n","        ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n","\n","        save_path = os.path.join(self.graph_dir, graph_name+\".pdf\")\n","        plt.savefig(save_path)\n","        plt.clf()\n","\n","    def print(self):\n","        return \" \".join([self.operator.print(), self.untrusted_data.print()])"],"metadata":{"id":"ZRv7EXuShHMK","executionInfo":{"status":"aborted","timestamp":1664035528612,"user_tz":-540,"elapsed":6,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - Defensive Model"],"metadata":{"id":"4e-GBUowhNUQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from keras.layers.core import Lambda\n","from keras.layers.merge import Average, add\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D\n","from keras.models import Model\n","import keras.regularizers as regs\n","\n","\n","class DenoisingAutoEncoder:\n","    def __init__(self, image_shape,\n","                 structure,\n","                 v_noise=0.0,\n","                 activation=\"relu\",\n","                 model_dir=\"./defensive_models/\",\n","                 reg_strength=0.0):\n","        \"\"\"\n","        Denoising Autoencoder(DAE)\n","        training data에 nosie를 추가하여 인코더에 넣어서 학습된 결과가 \n","        noise를 붙이기 전 데이터와의 error을 최소화하는 목적을 가진 Autoencoder\n","\n","        image_shape: Shape of input image. e.g. 28, 28, 1.\n","        structure: Structure of autoencoder.\n","        v_noise: Volume of noise while training.\n","        activation: What activation function to use.\n","        model_dir: Where to save / load model from.\n","        reg_strength: Strength of L2 regularization.\n","        \"\"\"\n","        h, w, c = image_shape\n","        self.image_shape = image_shape # shape of input image (32,32,3)\n","        self.model_dir = model_dir \n","        self.v_noise = v_noise\n","\n","        input_img = Input(shape=self.image_shape)\n","        x = input_img\n","\n","        # encoder 정의 \n","        for layer in structure: \n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\":\n","                x = MaxPooling2D((2, 2), padding=\"same\")(x)\n","            elif layer == \"average\":\n","                x = AveragePooling2D((2, 2), padding=\"same\")(x)\n","            else:\n","                print(layer, \"is not recognized!\")\n","                exit(0)\n","        \n","        for layer in reversed(structure):\n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\" or layer == \"average\":\n","                x = UpSampling2D((2, 2))(x)\n","\n","        # decoder 정의 \n","        decoded = Conv2D(c, (3, 3), activation='sigmoid', padding='same',\n","                         activity_regularizer=regs.l2(reg_strength))(x)\n","\n","        self.model = Model(input_img, decoded) # autoencoder 모델\n","\n","    def train(self, data, archive_name, num_epochs=100, batch_size=32):\n","        self.model.compile(loss='mean_squared_error',\n","                           metrics=['mean_squared_error'],\n","                           optimizer='adam')\n","        \n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train)) # 랜덤 노이즈 \n","        noisy_train_data = data.x_train + noise # Input Data에 랜덤 노이즈 추가 \n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0) # [0,1] 범위로 재구성\n","\n","        self.model.fit(noisy_train_data, data.x_train,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test, data.x_test),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        print(os.path.join(self.model_dir, archive_name))        \n","        self.model.save(os.path.join(self.model_dir, archive_name))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))\n","\n","\n","class PackedAutoEncoder:\n","    def __init__(self, image_shape, structure, data,\n","                 v_noise=0.1, n_pack=2, pre_epochs=3, activation=\"relu\",\n","                 model_dir=\"./defensive_models/\"):\n","        \"\"\"\n","        Train different autoencoders.\n","        Demo code for graybox scenario.\n","\n","        pre_epochs: How many epochs do we train before fine-tuning.\n","        n_pack: Number of autoencoders we want to train at once.\n","        \"\"\"\n","        self.v_noise = v_noise\n","        self.n_pack = n_pack\n","        self.model_dir = model_dir\n","        pack = []\n","\n","\n","\n","        for i in range(n_pack):\n","            dae = DenoisingAutoEncoder(image_shape, structure, v_noise=v_noise,\n","                                       activation=activation, model_dir=model_dir)\n","            dae.train(data, \"\", num_epochs=pre_epochs)\n","            pack.append(dae.model)\n","\n","\n","        shared_input = Input(shape=image_shape, name=\"shared_input\")\n","        outputs = [dae(shared_input) for dae in pack]\n","        avg_output = Average()(outputs)\n","        delta_outputs = [add([avg_output, Lambda(lambda x: -x)(output)])\n","                         for output in outputs]\n","\n","        self.model = Model(inputs=shared_input, outputs=outputs+delta_outputs)\n","\n","    def train(self, data, archive_name, alpha, num_epochs=10, batch_size=32):\n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train))\n","        noisy_train_data = data.x_train + noise\n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0)\n","\n","        train_zeros = [np.zeros_like(data.x_train)] * self.n_pack\n","        val_zeros = [np.zeros_like(data.x_test)] * self.n_pack\n","\n","        self.model.compile(loss=\"mean_squared_error\", optimizer=\"adam\",\n","                           loss_weights=[1.0]*self.n_pack + [-alpha]*self.n_pack)\n","\n","        self.model.fit(noisy_train_data,\n","                       [data.x_train]*self.n_pack + train_zeros,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test,\n","                            [data.x_test]*self.n_pack+val_zeros),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        for i in range(self.n_pack):\n","            model = Model(self.model.input, self.model.outputs[i])\n","            self.model.save(\"\")\n","            print(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","            self.model.save(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))"],"metadata":{"id":"HyIrYmBIhPRc","executionInfo":{"status":"aborted","timestamp":1664035528613,"user_tz":-540,"elapsed":7,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - Train Defensive Model"],"metadata":{"id":"ukxjkHWahV8B"}},{"cell_type":"code","source":["DAE = DenoisingAutoEncoder\n","PAE = PackedAutoEncoder\n","\n","shape = [32, 32, 3]\n","combination_I = [3, \"average\", 3]\n","combination_II = [3]\n","activation = \"sigmoid\"\n","reg_strength = 1e-9\n","epochs = 350\n","\n","# data = GTSRB()\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=8)\n","# AE_II.train(data, \"_8_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation)\n","# AE_II.train(data, \"_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)    \n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=32)\n","# AE_II.train(data, \"O32_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)  \n","\n","AE_I = DAE(shape, combination_I, v_noise=0.1, activation=activation, reg_strength=reg_strength)\n","#AE_I.train(data, \"350_0918_DAE_GTSRB_I\", num_epochs=epochs)\n","\n","AE_II = DAE(shape, combination_II, v_noise=0.1, activation=activation,  reg_strength=reg_strength)\n","#AE_II.train(data, \"350_0918_DAE_GTSRB_II\", num_epochs=epochs)"],"metadata":{"id":"C5S8s9wxhanJ","executionInfo":{"status":"aborted","timestamp":1664035528613,"user_tz":-540,"elapsed":7,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - 1"],"metadata":{"id":"mzltzwx8he_N"}},{"cell_type":"code","source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"],"metadata":{"id":"BZXZc5yXGbHR","executionInfo":{"status":"aborted","timestamp":1664035528613,"user_tz":-540,"elapsed":749752,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ad_examples1.shape"],"metadata":{"id":"jNeNPwhcwXgR","executionInfo":{"status":"aborted","timestamp":1664035528614,"user_tz":-540,"elapsed":749744,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.min(ad_examples1[0]))\n","print(np.max(ad_examples1[0]))"],"metadata":{"id":"n88gOua2r0cF","executionInfo":{"status":"aborted","timestamp":1664035528614,"user_tz":-540,"elapsed":749736,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["orig_labels1[0]"],"metadata":{"id":"S6TVgUZer8GA","executionInfo":{"status":"aborted","timestamp":1664035528615,"user_tz":-540,"elapsed":749729,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" data_test.y_test"],"metadata":{"id":"W0UMav8hwSJ0","executionInfo":{"status":"aborted","timestamp":1664035529209,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원본 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(data_test.x_test/255, data_test.y_test)\n","\n","print('test set accuracy (original) : ', accuracy * 100)"],"metadata":{"id":"0vMxKPzjhhAy","executionInfo":{"status":"aborted","timestamp":1664035529209,"user_tz":-540,"elapsed":20,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 공격받은 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(ad_examples1, orig_labels1)\n","\n","print('test set accuracy (attacked) : ', accuracy * 100)"],"metadata":{"id":"ca5TN0fGhrgk","executionInfo":{"status":"aborted","timestamp":1664035529210,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_detector_I = AEDetector(\"./defensive_models/350_0918_DAE_GTSRB_I\", p=2)\n","DAE_detector_II = AEDetector(\"./defensive_models/350_0918_DAE_GTSRB_II\", p=1)\n","DAE_reformer = SimpleReformer(\"./defensive_models/350_0918_DAE_GTSRB_I\")\n","\n","\n","DAE_id_reformer = IdReformer()\n","DAE_classifier = Classifier(\"./models/gtsrb_classifier\")"],"metadata":{"id":"L6gv49XOhtCl","executionInfo":{"status":"aborted","timestamp":1664035529210,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detector_JSD1 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=10)\n","detector_JSD2 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=40)\n","\n","\n","DAE_detector_dict = dict()\n","DAE_detector_dict[\"I\"] = DAE_detector_I\n","DAE_detector_dict[\"II\"] = DAE_detector_II\n","DAE_detector_dict[\"JSD1\"] = detector_JSD1\n","DAE_detector_dict[\"JSD2\"] = detector_JSD2"],"metadata":{"id":"qRcUtasdhuRN","executionInfo":{"status":"aborted","timestamp":1664035529211,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_operator = Operator(data, DAE_classifier, DAE_detector_dict, DAE_reformer)\n","DAE_testAttack = AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\")"],"metadata":{"id":"36Yx1eqahvso","executionInfo":{"status":"aborted","timestamp":1664035529211,"user_tz":-540,"elapsed":21,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_evaluator = Evaluator(DAE_operator, DAE_testAttack)\n","\n","DAE_evaluator.plot_various_confidences(\"defense_performance\", drop_rate={\"I\": 0.001, \"II\": 0.001,\"JSD1\": 0.001,\"JSD2\": 0.001})"],"metadata":{"id":"C3MbM1HDhwv0","executionInfo":{"status":"aborted","timestamp":1664035529211,"user_tz":-540,"elapsed":20,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('with reformer')\n","start = time.time()\n","predict = np.array((DAE_reformer.model.predict(ad_examples1))).reshape(-1,32,32,3)\n","activ_time = time.time() - start\n","meantime = activ_time / len(ad_examples1)\n","predicted1 = np.argmax(classifier.model.predict(predict),axis=1)\n","\n","print(np.mean(predicted1[:len(orig_labels)] == orig_labels[:len(orig_labels)]))\n","# print(.sum(predicted==orig_labels))#,predicted)\n","print(meantime)"],"metadata":{"id":"gBi_uO4Dhx6D","executionInfo":{"status":"aborted","timestamp":1664035529212,"user_tz":-540,"elapsed":20,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(data.x_train))\n","print(type(ad_examples))"],"metadata":{"id":"AmEhY0bFhzJd","executionInfo":{"status":"aborted","timestamp":1664035529212,"user_tz":-540,"elapsed":19,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defense-GAN"],"metadata":{"id":"vIK9wb4vIIlv"}},{"cell_type":"markdown","metadata":{"id":"P9v5DhrcQTi_"},"source":["###  Defense Model 1 : DefenseGAN"]},{"cell_type":"markdown","source":["### DefenseGAN 구현\n"],"metadata":{"id":"2TtBNBJxn2Db"}},{"cell_type":"code","source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"],"metadata":{"id":"oDv8n7tn6Hep","executionInfo":{"status":"aborted","timestamp":1664035529212,"user_tz":-540,"elapsed":19,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ad_examples1.shape)\n","print(orig_labels1.shape)"],"metadata":{"id":"fhZ-PyFGn3lz","executionInfo":{"status":"aborted","timestamp":1664035529213,"user_tz":-540,"elapsed":20,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GAN 모델 저장\n","from keras.models import load_model\n","\n","generator_base = load_model('baseGAN_Generator_attacked0.02.h5')"],"metadata":{"id":"lZngn6VduEUE","executionInfo":{"status":"aborted","timestamp":1664035529213,"user_tz":-540,"elapsed":19,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(generator_base.predict(np.random.normal(0, 1, (1, 100))).reshape(32,32,3))"],"metadata":{"id":"P4m8Jituudng","executionInfo":{"status":"aborted","timestamp":1664035529213,"user_tz":-540,"elapsed":18,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt"],"metadata":{"id":"g5GA13HB7okG","executionInfo":{"status":"aborted","timestamp":1664035529214,"user_tz":-540,"elapsed":19,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def DefenseGAN(img_at,L,R):\n","    z_list = []\n","    img = img_at.reshape(32,32,3)\n","    img_st = (img - np.mean(img)) / np.std(img) \n","    img_var = tf.Variable(img_st,dtype = float)\n","    opt = tf.keras.optimizers.SGD(learning_rate=0.1,momentum = 0.7)\n","\n","    def compute():\n","        z_hats_recs = generator_base(z_var)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","\n","    for r in range(R):\n","        z = np.random.normal(0, 1, (1, 100))\n","        z_var = tf.Variable(z,dtype = float)\n","    \n","        for l in range(L):\n","            opt.minimize(compute,[z_var])\n","        z_list.append(z_var)\n","\n","    def compute_10(z):\n","        #generator_base.trainable = False #아직 더해야하는지 뺴야하는지 판단 x\n","        z_hats_recs = generator_base(z)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","    \n","\n","    loss_list = []\n","    \n","    for i in range(len(z_list)):\n","        loss = compute_10(z_list[i])\n","        loss_list.append(loss)\n","    \n","    index_min = np.argmin(loss_list)\n","\n","    z_min = np.array(z_list[index_min])\n","\n","    generated_images = 0.5 * generator_base.predict(z_min)+ 0.5\n","\n","    generated_images = generated_images.reshape(32,32,3)\n","\n","    return generated_images"],"metadata":{"id":"2Zppe2W6n3n9","executionInfo":{"status":"aborted","timestamp":1664035529214,"user_tz":-540,"elapsed":18,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_example_data, orig_label_data)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","\n","        generated_img = DefenseGAN(data.reshape(32,32,3),200,10).reshape(-1,32,32,3)\n","\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"],"metadata":{"id":"HsGI-NY4n3qe","executionInfo":{"status":"aborted","timestamp":1664035529214,"user_tz":-540,"elapsed":18,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df, meantime = test(model, ad_examples1[:100], orig_labels1[:100])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"],"metadata":{"id":"qnYmHP5Bn3tC","executionInfo":{"status":"aborted","timestamp":1664035529214,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meantime"],"metadata":{"id":"PBdm3k8iA2kP","executionInfo":{"status":"aborted","timestamp":1664035529215,"user_tz":-540,"elapsed":18,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PCA"],"metadata":{"id":"63SBpYd-IL2u"}},{"cell_type":"markdown","metadata":{"id":"1ywsHS3sfPxD"},"source":["### Defense 2 : PCA (Components = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuoOeVKxsyvD","executionInfo":{"status":"aborted","timestamp":1664035529215,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAB3avuSbB3q","executionInfo":{"status":"aborted","timestamp":1664035529215,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2uWTxjIf-a-","executionInfo":{"status":"aborted","timestamp":1664035529215,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","        generated_img = defense_PCA(data,5)\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGm6ObXDhR2V","executionInfo":{"status":"aborted","timestamp":1664035529216,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df,meantime = test(model, ad_examples1[:2000], orig_labels1[:2000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)\n","meantime"]},{"cell_type":"markdown","metadata":{"id":"9Y_-9xiJiAeC"},"source":["### Defense 2 : PCA (Components = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOiE-bIgsz6w","executionInfo":{"status":"aborted","timestamp":1664035529216,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyLQ-kGQwqEZ","executionInfo":{"status":"aborted","timestamp":1664035529216,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWXuP7V4iPO0","executionInfo":{"status":"aborted","timestamp":1664035529217,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","        generated_img = defense_PCA(data,10)\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP7_KoXqiWhf","executionInfo":{"status":"aborted","timestamp":1664035529217,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df,meantime = test(model, ad_examples1[:2000], orig_labels1[:2000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)\n","meantime"]},{"cell_type":"code","source":[],"metadata":{"id":"j8yRgtGAaO1w","executionInfo":{"status":"aborted","timestamp":1664035529217,"user_tz":-540,"elapsed":17,"user":{"displayName":"진규코랩2","userId":"03861400779398970195"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}