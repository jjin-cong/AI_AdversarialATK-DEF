{"cells":[{"cell_type":"markdown","source":["# 기본 import"],"metadata":{"id":"H5r08xIXerXB"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"WgKss5hJBb5B","executionInfo":{"status":"ok","timestamp":1663409810236,"user_tz":-540,"elapsed":2749,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import os\n","import pathlib\n","\n","import cv2 #영상처리에 사용하는 오픈소스 라이브러리, 컴퓨터가 사람 눈처럼 인식할 수 있게 처리\n","from PIL import Image # 파이썬 이미지 처리 pillow 라이브러리\n","from tensorflow.keras.preprocessing import image\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator #imagedatagenerater는 이미지를 학습시킬 때 학습 데이터의 양이 적을 경우 학습데이터를 조금씩 변형 시켜서 학습데이터의 양을 늘리는 방식중 하나\n","from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import matplotlib.pyplot as plt\n","\n","from tqdm.auto import tqdm\n","\n","#난수 랜덤성 고정\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3319,"status":"ok","timestamp":1663409814917,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"Qn7HC1qiBnNE","outputId":"367bbd63-f34e-48bb-f889-868368ac3e10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1663409814917,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"VwTVXhm7BoaE","outputId":"e4bf8f2f-2067-409d-af4c-982624d4ee2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1WKEjdIyqtzI-NV5o0O_ixsHslngaSiQX/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB\n"]}],"source":["cd drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB"]},{"cell_type":"markdown","source":["# Train & Test 데이터 불러오기"],"metadata":{"id":"FhnfS-0MH1sa"}},{"cell_type":"markdown","metadata":{"id":"JjBcm524Fams"},"source":["Train Data 불러오기"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kZnmTWHIBo_E","executionInfo":{"status":"ok","timestamp":1663409818040,"user_tz":-540,"elapsed":470,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["import numpy as np\n","import os\n","import gzip\n","import urllib.request\n","\n","from keras.models import load_model\n","\n","def ordered_onehotencoding(labels):\n","    labels_ordered = []\n","    for i in range(len(labels)):\n","        if labels[i] == 3:\n","            labels_ordered.append(0)\n","        elif labels[i] == 7:\n","            labels_ordered.append(1)\n","        elif labels[i] == 9:\n","            labels_ordered.append(2)\n","        elif labels[i] == 10:\n","            labels_ordered.append(3)\n","        elif labels[i] == 11:\n","            labels_ordered.append(4)\n","        elif labels[i] == 12:\n","            labels_ordered.append(5)\n","        elif labels[i] == 13:\n","            labels_ordered.append(6)\n","        elif labels[i] == 17:\n","            labels_ordered.append(7)\n","        elif labels[i] == 18:\n","            labels_ordered.append(8)\n","        elif labels[i] == 25:\n","            labels_ordered.append(9)\n","        elif labels[i] == 35:\n","            labels_ordered.append(10)\n","        elif labels[i] == 38:\n","            labels_ordered.append(11)\n","    \n","    return np.array(labels_ordered)\n","\n","class GTSRB:\n","    def __init__(self):\n","        imgs_path = \"Train\"\n","        data_list = []\n","        labels_list = []\n","\n","        result_class = [3,7, 9, 10, 11, 12, 13, 17, 18, 25, 35, 38]\n","\n","        for i in result_class:\n","            i_path = os.path.join(imgs_path, str(i)) # 3, 7, 9, 10, 11, 12,13, 17, 18, 25, 35, 38\n","            num = 0\n","            for img in os.listdir(i_path):\n","          \n","                im = Image.open(i_path +'/'+ img)\n","                im = im.resize((32,32))\n","                im = np.array(im)\n","\n","                data_list.append(im)\n","                labels_list.append(i)\n","                num = num + 1\n","                if num == 1000:\n","                    break;\n","\n","        data = np.array(data_list)\n","        labels = ordered_onehotencoding(labels_list)\n","\n","        labels = to_categorical(labels)\n","\n","        VALIDATION_SIZE = 5000\n","        \n","        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(np.array(data), labels, test_size=0.4)    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lWo9BzndEm67","executionInfo":{"status":"ok","timestamp":1663410161402,"user_tz":-540,"elapsed":341097,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["data = GTSRB()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663410161403,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"Zc5Q_EXiEppb","outputId":"8ab4f2e1-28b3-41f8-9f9d-462b33571ad6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(7200, 32, 32, 3)\n","(4800, 32, 32, 3)\n","(7200, 12)\n","(4800, 12)\n"]}],"source":["print(data.x_train.shape)\n","print(data.x_test.shape)\n","print(data.y_train.shape)\n","print(data.y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"P2uSEM66FdVa"},"source":["Test Data 불러오기"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_shX58HHFisj","executionInfo":{"status":"ok","timestamp":1663410164178,"user_tz":-540,"elapsed":2779,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["metainfo = pd.read_csv(\"Meta.csv\")\n","traininfo = pd.read_csv(\"Train.csv\")\n","testinfo = pd.read_csv(\"Test.csv\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z9HPAPg2FZ9L","executionInfo":{"status":"ok","timestamp":1663410164179,"user_tz":-540,"elapsed":4,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["import natsort\n","\n","class GTSRB_test:\n","    def __init__(self):\n","        imgs_path = \"Test\"\n","        data_list = []\n","        labels_list = []\n","        \n","        for img in natsort.natsorted(os.listdir(imgs_path)):\n","            im = Image.open(imgs_path +'/'+ img)\n","            im = im.resize((32,32))\n","            im = np.array(im)\n","            data_list.append(im)\n","        data_test = np.array(data_list)\n","        \n","        for i in range(len(testinfo.ClassId)):\n","            labels_list.append(testinfo.ClassId[i])\n","        \n","        labels_test = np.array(labels_list)\n","\n","        labels_test_index = []\n","        for i in range(len(labels_test)):\n","            if (labels_test[i] == 3) | (labels_test[i] == 7) | (labels_test[i] == 9) | (labels_test[i] == 10) | (labels_test[i] == 11) | (labels_test[i] == 12) | (labels_test[i] == 13) | (labels_test[i] == 17) | (labels_test[i] == 18) | (labels_test[i] == 25) | (labels_test[i] == 35) | (labels_test[i] == 38):\n","                labels_test_index.append(i)\n","\n","        test_data = []\n","        test_label = []\n","        for i in labels_test_index:\n","            test_data.append(data_test[i])\n","            test_label.append(labels_test[i])\n","\n","        data_test = np.array(test_data)\n","\n","        labels_test =ordered_onehotencoding(test_label)\n","\n","        labels_test = to_categorical(labels_test)\n","        \n","        self.x_test = data_test\n","        self.y_test = labels_test    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB_test\""]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CcWfJaa6Ft7z","executionInfo":{"status":"ok","timestamp":1663410450622,"user_tz":-540,"elapsed":286446,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["data_test = GTSRB_test()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663410450622,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"VE9NTZrAFv7e","outputId":"61ba2862-16aa-4a3e-cc9b-a01f49c6aff2"},"outputs":[{"output_type":"stream","name":"stdout","text":["(6180, 32, 32, 3)\n","(6180, 12)\n"]}],"source":["print(data_test.x_test.shape)\n","print(data_test.y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"XWfrLWt3EtSN"},"source":["# 분류기 : CNN"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TqQDIy-WEwQz","executionInfo":{"status":"ok","timestamp":1663410450622,"user_tz":-540,"elapsed":4,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["data.x_train, data.y_train, data.x_test, data.y_test =data.x_train/255, data.y_train/255, data.x_test/255, data.y_test/255"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69888,"status":"ok","timestamp":1663410520507,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"gI5nIfEQE0Dt","outputId":"ee3c917d-4583-4ced-c655-c15999b168a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 30, 30, 96)        2688      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 15, 15, 96)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 15, 15, 96)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 13, 13, 192)       166080    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 6, 6, 192)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 6, 6, 192)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 192)         331968    \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                196672    \n","                                                                 \n"," dense_1 (Dense)             (None, 12)                780       \n","                                                                 \n","=================================================================\n","Total params: 698,188\n","Trainable params: 698,188\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/30\n","60/60 [==============================] - 18s 41ms/step - loss: 0.0380 - accuracy: 0.0835 - val_loss: 0.0057 - val_accuracy: 0.0835\n","Epoch 2/30\n","60/60 [==============================] - 2s 29ms/step - loss: 0.0036 - accuracy: 0.0878 - val_loss: 0.0051 - val_accuracy: 0.0729\n","Epoch 3/30\n","60/60 [==============================] - 3s 43ms/step - loss: 0.0031 - accuracy: 0.1160 - val_loss: 0.0039 - val_accuracy: 0.1610\n","Epoch 4/30\n","60/60 [==============================] - 2s 26ms/step - loss: 0.0030 - accuracy: 0.1942 - val_loss: 0.0039 - val_accuracy: 0.2179\n","Epoch 5/30\n","60/60 [==============================] - 2s 28ms/step - loss: 0.0030 - accuracy: 0.2590 - val_loss: 0.0039 - val_accuracy: 0.2854\n","Epoch 6/30\n","60/60 [==============================] - 2s 41ms/step - loss: 0.0028 - accuracy: 0.3753 - val_loss: 0.0041 - val_accuracy: 0.4546\n","Epoch 7/30\n","60/60 [==============================] - 2s 25ms/step - loss: 0.0027 - accuracy: 0.4883 - val_loss: 0.0031 - val_accuracy: 0.5760\n","Epoch 8/30\n","60/60 [==============================] - 2s 28ms/step - loss: 0.0027 - accuracy: 0.5821 - val_loss: 0.0030 - val_accuracy: 0.6648\n","Epoch 9/30\n","60/60 [==============================] - 2s 28ms/step - loss: 0.0026 - accuracy: 0.6388 - val_loss: 0.0032 - val_accuracy: 0.7000\n","Epoch 10/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0026 - accuracy: 0.6906 - val_loss: 0.0028 - val_accuracy: 0.7569\n","Epoch 11/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0025 - accuracy: 0.7367 - val_loss: 0.0029 - val_accuracy: 0.7865\n","Epoch 12/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0025 - accuracy: 0.7628 - val_loss: 0.0028 - val_accuracy: 0.8012\n","Epoch 13/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0025 - accuracy: 0.7921 - val_loss: 0.0029 - val_accuracy: 0.8358\n","Epoch 14/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0025 - accuracy: 0.8101 - val_loss: 0.0026 - val_accuracy: 0.8512\n","Epoch 15/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0024 - accuracy: 0.8240 - val_loss: 0.0028 - val_accuracy: 0.8775\n","Epoch 16/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0024 - accuracy: 0.8492 - val_loss: 0.0027 - val_accuracy: 0.8910\n","Epoch 17/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0024 - accuracy: 0.8657 - val_loss: 0.0026 - val_accuracy: 0.9015\n","Epoch 18/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0024 - accuracy: 0.8781 - val_loss: 0.0027 - val_accuracy: 0.9071\n","Epoch 19/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0024 - accuracy: 0.8919 - val_loss: 0.0026 - val_accuracy: 0.9217\n","Epoch 20/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0024 - accuracy: 0.8994 - val_loss: 0.0026 - val_accuracy: 0.9254\n","Epoch 21/30\n","60/60 [==============================] - 2s 41ms/step - loss: 0.0024 - accuracy: 0.9086 - val_loss: 0.0026 - val_accuracy: 0.9315\n","Epoch 22/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0023 - accuracy: 0.9142 - val_loss: 0.0026 - val_accuracy: 0.9396\n","Epoch 23/30\n","60/60 [==============================] - 1s 25ms/step - loss: 0.0023 - accuracy: 0.9208 - val_loss: 0.0025 - val_accuracy: 0.9448\n","Epoch 24/30\n","60/60 [==============================] - 2s 26ms/step - loss: 0.0023 - accuracy: 0.9294 - val_loss: 0.0025 - val_accuracy: 0.9508\n","Epoch 25/30\n","60/60 [==============================] - 2s 26ms/step - loss: 0.0023 - accuracy: 0.9344 - val_loss: 0.0025 - val_accuracy: 0.9456\n","Epoch 26/30\n","60/60 [==============================] - 2s 30ms/step - loss: 0.0023 - accuracy: 0.9374 - val_loss: 0.0025 - val_accuracy: 0.9558\n","Epoch 27/30\n","60/60 [==============================] - 1s 23ms/step - loss: 0.0023 - accuracy: 0.9447 - val_loss: 0.0025 - val_accuracy: 0.9619\n","Epoch 28/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0023 - accuracy: 0.9443 - val_loss: 0.0024 - val_accuracy: 0.9648\n","Epoch 29/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0023 - accuracy: 0.9507 - val_loss: 0.0024 - val_accuracy: 0.9646\n","Epoch 30/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.0023 - accuracy: 0.9557 - val_loss: 0.0024 - val_accuracy: 0.9698\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","import tensorflow as tf\n","import os\n","\n","\n","def train(data, file_name, num_epochs=50, batch_size=128):\n","    \"\"\"\n","    Standard neural network training procedure.\n","    \"\"\"\n","    model = Sequential()\n","\n","    IMG_HEIGHT = 32\n","    IMG_WIDTH = 32\n","\n","    # 첫번째 Convolutional Layer : 입력 데이터로부터 특징을 추출\n","    model.add(Conv2D(filters=96, kernel_size=3, activation='relu', input_shape=data.x_train.shape[1:]))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25))\n","\n","    # 두번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25)) # 인풋데이터의 25%를 무작위로 0으로 만듦\n","\n","    # 세번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu')) # 특징을 추출하는 기능을 하는 필터, 비선형 값으로 바꿔주는 activation 함수->relu\n","    # model.add(GlobalAveragePooling2D())\n","    model.add(Flatten())\n","\n","    model.add(Dense(units=64, activation='relu'))\n","    model.add(Dense(12, activation='softmax'))\n","\n","\n","    # 모델 컴파일 하기\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","\n","    # 모델 핏하기\n","    EPOCHS = num_epochs\n","    model.fit(data.x_train, data.y_train,\n","              validation_data = (data.x_test, data.y_test), \n","              epochs=EPOCHS, steps_per_epoch=60\n","              )\n","\n","    if file_name != None:\n","        model.save(file_name)\n","\n","    return model\n","\n","\n","if not os.path.isdir('models'):\n","    os.makedirs('models')\n","\n","model = train(data, \"models/gtsrb_classifier\", num_epochs=30)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1779,"status":"ok","timestamp":1663410522278,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"Ez-W2phKFLws","outputId":"d3453056-b584-474c-8406-73b9472e8221"},"outputs":[{"output_type":"stream","name":"stdout","text":["225/225 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9729\n","test set accuracy:  97.29166626930237\n","150/150 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.9698\n","test set accuracy:  96.97916507720947\n"]}],"source":["loss, accuracy = model.evaluate(data.x_train, data.y_train)\n","\n","print('test set accuracy: ', accuracy * 100)\n","\n","loss, accuracy = model.evaluate(data.x_test, data.y_test)\n","\n","print('test set accuracy: ', accuracy * 100)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3809,"status":"ok","timestamp":1663410526085,"user":{"displayName":"김채현","userId":"14052029844236242388"},"user_tz":-540},"id":"xbYmfSHYHOIa","outputId":"7a0bfe80-c12f-410f-fedb-2e8c399e2f9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["194/194 [==============================] - 1s 4ms/step - loss: 23.2989 - accuracy: 0.8375\n","test set accuracy:  83.75404477119446\n","194/194 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.9102\n","test set accuracy with nomalization:  91.01941585540771\n"]}],"source":["# test data set\n","loss, accuracy = model.evaluate(data_test.x_test, data_test.y_test)\n","\n","print('test set accuracy: ', accuracy * 100)\n","\n","# --> 애초에 오버피팅 되어있음을 확인할 수 있다.\n","\n","loss, accuracy = model.evaluate(data_test.x_test/255, data_test.y_test/255)\n","\n","print('test set accuracy with nomalization: ', accuracy * 100)\n","\n","# --> /255로 정규화 시켜준다면 어느정도 성능 회복 "]},{"cell_type":"markdown","source":["# 공격 데이터셋 : FGSM & PGD"],"metadata":{"id":"QNRCIvO3-8lf"}},{"cell_type":"code","source":["def tf_preprocess(image):\n","  image = tf.cast(image, tf.float32)\n","  image = image/255\n","  image = tf.image.resize(image, (32, 32))\n","  image = image[None, ...]\n","  return image\n","\n","# 확률 벡터에서 레이블을 추출해주는 헬퍼 메서드\n","def get_tf_label(labels):\n","    label = tf.cast(labels, tf.int32)\n","    label = tf.reshape(label,[1,12])\n","    return label\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","\n","def create_adversarial_pattern(input_image, input_label):\n","  with tf.GradientTape() as tape:\n","    tape.watch(input_image)\n","    input_img = tf.reshape(input_image,[1,32,32,3])\n","    prediction = model(input_img)\n","    loss = loss_object(input_label, prediction)\n","\n","  # 입력 이미지에 대한 손실 함수의 기울기를 구합니다.\n","  gradient = tape.gradient(loss, input_image)\n","  # 왜곡을 생성하기 위해 그래디언트의 부호를 구합니다.\n","  signed_grad = tf.sign(gradient)\n","  return signed_grad"],"metadata":{"id":"Dn9x64obA9UB","executionInfo":{"status":"ok","timestamp":1663410526086,"user_tz":-540,"elapsed":7,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SUb6-MoJtay"},"source":["### FGSM define"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"DGBdi7tWJT1K","executionInfo":{"status":"ok","timestamp":1663410526086,"user_tz":-540,"elapsed":6,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"outputs":[],"source":["def fgsm_attack(model,test_x,test_y,eps):\n","    \n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # FGSM 공격 수행\n","        perturbations = create_adversarial_pattern(img, label)\n","        adv_x = img + eps * perturbations\n","        adv_x = tf.clip_by_value(adv_x, 0, 1) # 공격받은 이미지\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(atkresult_label)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"]},{"cell_type":"markdown","source":["### PGD define"],"metadata":{"id":"QT3uHjE_Ahxy"}},{"cell_type":"code","source":["def pgd_attack(model,test_x,test_y,eps,step_size=2,num_steps=7): \n","    \"\"\"\n","    FGSM 코드와 차이점\n","    - step_size, num_steps 파라미터 추가됨\n","    - unifrom distribution 코드 추가\n","    - FGSM 공격 수행 -> PGD 공격 수행\n","    ** 모든 return 형식은 동일함\n","    \n","    default 값\n","    - step_size = 2 (alpha 값)\n","    - num_steps = 7 (iterations 값)\n","\n","    \"\"\"\n","\n","    prog = 0 # 진행상황 확인용 변수\n","\n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # PGD uniform distribution 코드\n","        data = data + np.random.uniform(-eps,eps,data.shape)\n","        data = np.clip(data,0,255)\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # PGD 공격 수행\n","        adv_x = img # 공격받은 이미지 (for문으로 업데이트)\n","        for num_step in range(num_steps):\n","          perturbations = create_adversarial_pattern(adv_x,label) # signed_grad를 리턴한 값\n","          adv_x += step_size * perturbations\n","          adv_x = tf.clip_by_value(adv_x,img-eps,img+eps)\n","          adv_x = tf.clip_by_value(adv_x,0,1)\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(atkresult_label)\n","\n","        prog += 1\n","        if prog%3000 == 0:\n","          print(prog)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"],"metadata":{"id":"GmiLgFduVhR0","executionInfo":{"status":"ok","timestamp":1663411135823,"user_tz":-540,"elapsed":4,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Attack 수행 : 공격 데이터셋 만들기\n","* 정상 이미지에 대한 분류기 정확도 -> 위에 있음 (분류기:CNN; 약 91%)\n","* eps 별"],"metadata":{"id":"AVs5AnagBdTK"}},{"cell_type":"code","source":["# epss = [0.02, 0.03, 8/255, 0.05, 0.08, 0.10]\n","epss = []"],"metadata":{"id":"zaKC_AyFDXg7","executionInfo":{"status":"ok","timestamp":1663411141454,"user_tz":-540,"elapsed":589,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["#### FGSM"],"metadata":{"id":"z-Hyw8gzj6WE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbqAcNpoKBu-"},"outputs":[],"source":["FGSM_varying_epss = {} # eps : [acc, ex, ad_examples, orig_lables]\n","\n","for eps in epss:\n","  acc, ex, ad_examples, orig_labels = fgsm_attack(model, data_test.x_test, data_test.y_test, eps)\n","  FGSM_varying_epss[eps] = [acc, ex, ad_examples, orig_labels]\n","  print(\"FGSM - eps\", eps, \"complete\\n\")"]},{"cell_type":"markdown","source":["#### PGD"],"metadata":{"id":"djrm3R_Lj9ow"}},{"cell_type":"code","source":["PGD_varying_epss = {} # eps : [acc, ex, ad_examples, orig_lables]\n","\n","for eps in epss:\n","  acc, ex, ad_examples, orig_labels = pgd_attack(model, data_test.x_test, data_test.y_test, eps)\n","  PGD_varying_epss[eps] = [acc, ex, ad_examples, orig_labels]\n","  print(\"PGD - eps\", eps, \"complete\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ViPjYOWPcudW","executionInfo":{"status":"error","timestamp":1663413082744,"user_tz":-540,"elapsed":1938398,"user":{"displayName":"김채현","userId":"14052029844236242388"}},"outputId":"4436c62c-b6bd-47e6-ac0a-b1384dd95ccf"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","4300\n","4400\n","4500\n","4600\n","4700\n","4800\n","4900\n","5000\n","5100\n","5200\n","5300\n","5400\n","5500\n","5600\n","Epsilon: 0.02\tTest Accuracy = 3050 / 6180 = 0.4935275080906149\n","PGD - eps 0.02 complete\n","\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","4300\n","4400\n","4500\n","4600\n","4700\n","4800\n","4900\n","5000\n","5100\n","5200\n","5300\n","5400\n","5500\n","5600\n","Epsilon: 0.03\tTest Accuracy = 2267 / 6180 = 0.3668284789644013\n","PGD - eps 0.03 complete\n","\n","100\n","200\n","300\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-474f463b4438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mad_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mPGD_varying_epss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mad_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PGD - eps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"complete\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-62b2fc74f3a6>\u001b[0m in \u001b[0;36mpgd_attack\u001b[0;34m(model, test_x, test_y, eps, step_size, num_steps)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0matkresult_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# one-hot 형태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0matkresult_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matkresult_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# label 형태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1983\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#### (필요 X) 그냥 이미지 뽑아본 것"],"metadata":{"id":"lDK9o_--lA_g"}},{"cell_type":"code","source":["aa = FGSM_varying_epss[0.02]\n","imgs = aa[2]\n","print(len(imgs))\n","img = imgs[0]\n","img *= 255\n","img = np.array(img)\n","img = img.astype(int)\n","print(np.min(img), np.max(img))\n","plt.imshow(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"xW2vStQ2aqXf","executionInfo":{"status":"ok","timestamp":1663401195960,"user_tz":-540,"elapsed":903,"user":{"displayName":"김채현","userId":"16338580172282075794"}},"outputId":"583804a3-2079-4808-d40b-1668d072f811"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5632\n","9 239\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fe289d9a7d0>"]},"metadata":{},"execution_count":39},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["# 방어모델 : MagNet, Defense-GAN, PCA"],"metadata":{"id":"6DfDZcE5_GGB"}},{"cell_type":"markdown","source":["## MagNet"],"metadata":{"id":"Lb-doZmmIFqj"}},{"cell_type":"markdown","source":["### MagNet - def (utils, worker, Defensive Model)"],"metadata":{"id":"-Yc-yrW7ktMW"}},{"cell_type":"markdown","source":["#### MagNet - utils"],"metadata":{"id":"bt4nIJ5xg73T"}},{"cell_type":"code","source":["## utils.py -- utility functions\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","def prepare_data(dataset, idx):\n","    \"\"\"\n","    Extract data from index.\n","\n","    dataset: Full, working dataset. Such as MNIST().\n","    idx: Index of test examples that we care about.\n","    return: X, targets, Y\n","    \"\"\"\n","    return dataset.x_test[idx], dataset.y_test[idx], np.argmax(dataset.y_test[idx], axis=1)\n","\n","\n","def save_obj(obj, name, directory='./attack_data/'):\n","    with open(os.path.join(directory, name + '.pkl'), 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","\n","def load_obj(name, directory='./attack_data/'):\n","    if name.endswith(\".pkl\"): name = name[:-4]\n","    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n","        return pickle.load(f)"],"metadata":{"id":"hAvlahKmMFdO","executionInfo":{"status":"aborted","timestamp":1663410888021,"user_tz":-540,"elapsed":9,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - worker"],"metadata":{"id":"EhncucofhEvv"}},{"cell_type":"code","source":["## setup_mnist.py -- mnist data and model loading code\n","##\n","## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","## Modified for MagNet's use.\n","\n","## worker.py -- evaluation code\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","from scipy.stats import entropy\n","from numpy.linalg import norm\n","from matplotlib.ticker import FuncFormatter\n","from keras.models import Sequential, load_model\n","from keras.activations import softmax\n","from keras.layers import Lambda\n","import numpy as np\n","import pylab\n","import os\n","import matplotlib.pyplot as plt\n","\n","\n","class AEDetector:\n","    def __init__(self, path, p=1):\n","        \"\"\"\n","        Error based detector.\n","        Marks examples for filtering decisions.\n","\n","        path: Path to the autoencoder used.\n","        p: Distance measure to use.\n","        \"\"\"\n","\n","        self.model = load_model(path)\n","        self.path = path\n","        self.p = p\n","\n","    def mark(self, X):\n","        diff = np.abs(X - self.model.predict(X)) # input X와 예측값 X'(autoencoder를 통해 노이즈가 더해진 값) 의 오차 값\n","        marks = np.mean(np.power(diff, self.p), axis=(1,2,3)) # 오차값의 분산\n","        return marks\n","\n","    def print(self):\n","        return \"AEDetector:\" + self.path.split(\"/\")[-1]\n","\n","\n","class IdReformer:\n","    def __init__(self, path=\"IdentityFunction\"):\n","        \"\"\"\n","        Identity reformer.\n","        Reforms an example to itself.\n","        \"\"\"\n","        self.path = path\n","        self.heal = lambda X: X\n","\n","    def print(self):\n","        return \"IdReformer:\" + self.path\n","\n","\n","class SimpleReformer:\n","    def __init__(self, path):\n","        \"\"\"\n","        Reformer.\n","        Reforms examples with autoencoder. Action of reforming is called heal.\n","\n","        path: Path to the autoencoder used.\n","        \"\"\"\n","        self.model = load_model(path)\n","        self.path = path\n","\n","    def heal(self, X):\n","        X = self.model.predict(X) # autoencoder로 X값 재구성\n","        return np.clip(X, 0.0, 1.0)\n","\n","    def print(self):\n","        return \"SimpleReformer:\" + self.path.split(\"/\")[-1]\n","\n","\n","def JSD(P, Q):\n","    _P = P / norm(P, ord=1)\n","    _Q = Q / norm(Q, ord=1)\n","    _M = 0.5 * (_P + _Q)\n","    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M)) # Xp와 Xr의 분포의 entropy \n","    # KL divergence: Q(one autoencoder)를 기반으로 했을 때의 cross entropy와 P(magnet)를 기반으로 했을 때의 entropy의 차이\n","\n","\n","\n","class DBDetector:\n","    def __init__(self, reconstructor, prober, classifier, option=\"jsd\", T=1):\n","        \"\"\"\n","        Divergence-Based Detector.\n","\n","        reconstructor: One autoencoder.\n","        prober: Another autoencoder.\n","        classifier: Classifier object.\n","        option: Measure of distance, jsd as default.\n","        T: Temperature to soften the classification decision.\n","        \"\"\"\n","        self.prober = prober\n","        self.reconstructor = reconstructor\n","        self.classifier = classifier\n","        self.option = option\n","        self.T = T\n","\n","    def mark(self, X):\n","        return self.mark_jsd(X)\n","\n","    def mark_jsd(self, X):\n","        Xp = self.prober.heal(X) # 1번 autoencoder로 생성한 이미지 \n","        Xr = self.reconstructor.heal(X) #2번 autoencoder로 생성한 이미지 \n","        Pp = self.classifier.classify(Xp, option=\"prob\", T=self.T) # Xp의 확률\n","        Pr = self.classifier.classify(Xr, option=\"prob\", T=self.T) # Xr의 확률\n","\n","        marks = [(JSD(Pp[i], Pr[i])) for i in range(len(Pr))]\n","        return np.array(marks)\n","\n","    def print(self):\n","        return \"Divergence-Based Detector\"\n","\n","\n","class Classifier:\n","    def __init__(self, classifier_path):\n","        \"\"\"\n","        Keras classifier wrapper.\n","        Note that the wrapped classifier should spit logits as output.\n","\n","        classifier_path: Path to Keras classifier file.\n","        \"\"\"\n","        self.path = classifier_path\n","        self.model = load_model(classifier_path)\n","        self.softmax = Sequential()\n","        self.softmax.add(Lambda(lambda X: softmax(X, axis=1)))\n","\n","    def classify(self, X, option=\"logit\", T=1):\n","        if option == \"logit\":\n","            return self.model.predict(X)\n","        if option == \"prob\":\n","            logits = self.model.predict(X)/T\n","            return self.softmax.predict(logits)\n","\n","    def print(self):\n","        return \"Classifier:\"+self.path.split(\"/\")[-1]\n","\n","\n","class Operator:\n","    def __init__(self, data, classifier, det_dict, reformer):\n","        \"\"\"\n","        Operator.\n","        Describes the classification problem and defense.\n","\n","        data: Standard problem dataset. Including train, test, and validation.\n","        classifier: Target classifier.\n","        reformer: Reformer of defense.\n","        det_dict: Detector(s) of defense.\n","        \"\"\"\n","\n","        self.data = data\n","        self.classifier = classifier\n","        self.det_dict = det_dict \n","        self.reformer = reformer\n","        self.normal = self.operate(AttackData(data.x_train, np.argmax(data.y_train, axis=1), \"Normal\"))\n","        \n","\n","    def get_thrs(self, drop_rate):\n","        \"\"\"\n","        Get filtering threshold by marking validation set.\n","        \"\"\"\n","        thrs = dict()\n","        for name, detector in self.det_dict.items():\n","            num = int(len(data.x_test) * drop_rate[name])\n","            marks = detector.mark(data.x_test)\n","            marks = np.sort(marks)\n","            thrs[name] = marks[-num]\n","        return thrs\n","\n","    def operate(self, untrusted_obj):\n","        \"\"\"\n","        For untrusted input(normal or adversarial), classify original input and\n","        reformed input. Classifier is unaware of the source of input.\n","\n","        untrusted_obj: Input data.\n","        \"\"\"\n","\n","        X = untrusted_obj.data\n","        Y_true = untrusted_obj.labels\n","\n","\n","        X_prime = self.reformer.heal(X) # autoencoder 값으로 재구성\n","        Y = np.argmax(self.classifier.classify(X), axis=1) # 원본 input X 분류\n","        Y_judgement = (Y == Y_true[:len(X_prime)]) # 실제 label과 X 분류 label 비교\n","        Y_prime = np.argmax(self.classifier.classify(X_prime), axis=1)  # autoencoder로 재구성한 X' 분류\n","        Y_prime_judgement = (Y_prime == Y_true[:len(X_prime)])  # 실제 label과 X' 분류 label 비교\n","        return np.array(list(zip(Y_judgement, Y_prime_judgement)))\n","\n","    def filter(self, X, thrs):\n","        \"\"\"\n","        untrusted_obj: Untrusted input to test against.\n","        thrs: Thresholds.\n","\n","        return:\n","        all_pass: Index of examples that passed all detectors.\n","        collector: Number of examples that escaped each detector.\n","        \"\"\"\n","        collector = dict()\n","        all_pass = np.array(range(10000)) #Index\n","        for name, detector in self.det_dict.items():\n","            marks = detector.mark(X) #  KL divergnece: Xp와 Xr의 분포의 entropy \n","            idx_pass = np.argwhere(marks < thrs[name]) # KL divergnece가 thershold보다 작을 경우 pass, 클 경우 reject\n","            collector[name] = len(idx_pass) # pass가 된 수\n","            all_pass = np.intersect1d(all_pass, idx_pass) # 전체 index array와 pass된 array의 교집합\n","        return all_pass, collector\n","\n","    def print(self):\n","        components = [self.reformer, self.classifier]\n","        return \" \".join(map(lambda obj: getattr(obj, \"print\")(), components))\n","\n","\n","class AttackData:\n","    def __init__(self, examples, labels, name=\"\"):\n","        \"\"\"\n","        Input data wrapper. May be normal or adversarial.\n","\n","        examples: Path or object of input examples.\n","        labels: Ground truth labels.\n","        \"\"\"\n","        # if isinstance(examples, str): \n","        #   self.data = load_obj(examples)\n","        # else: \n","\n","        self.data = examples\n","        self.labels = labels\n","        self.name = name\n","\n","    def print(self):\n","        return \"Attack:\"+self.name\n","\n","\n","class Evaluator:\n","    def __init__(self, operator, untrusted_data, graph_dir=\"./graph\"):\n","        \"\"\"\n","        Evaluator.\n","        For strategy described by operator, conducts tests on untrusted input.\n","        Mainly stats and plotting code. Most methods omitted for clarity.\n","\n","        operator: Operator object.\n","        untrusted_data: Input data to test against.\n","        graph_dir: Where to spit the graphs.\n","        \"\"\"\n","        self.operator = operator\n","        self.untrusted_data = untrusted_data # attacked data\n","        self.graph_dir = graph_dir\n","        self.data_package = operator.operate(untrusted_data)\n","\n","    def bind_operator(self, operator):\n","        self.operator = operator\n","        self.data_package = operator.operate(self.untrusted_data)\n","\n","    def load_data(self, data):\n","        self.untrusted_data = data\n","        self.data_package = self.operator.operate(self.untrusted_data)\n","\n","    def get_normal_acc(self, normal_all_pass):\n","        \"\"\"\n","        traning data에 대한 정확도\n","\n","        Break down of who does what in defense. Accuracy of defense on normal\n","        input.\n","\n","        both: Both detectors and reformer take effect\n","        det_only: detector(s) take effect\n","        ref_only: Only reformer takes effect\n","        none: Attack effect with no defense\n","        \"\"\"\n","        normal_tups = self.operator.normal\n","        num_normal = len(normal_tups)\n","        filtered_normal_tups = normal_tups[normal_all_pass]\n","\n","        both_acc = sum(1 for _, XpC in filtered_normal_tups if XpC)/num_normal # detector and refomer\n","        det_only_acc = sum(1 for XC, XpC in filtered_normal_tups if XC)/num_normal # only detector\n","        ref_only_acc = sum([1 for _, XpC in normal_tups if XpC])/num_normal # only reformer\n","        none_acc = sum([1 for XC, _ in normal_tups if XC])/num_normal # no defense\n","\n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def get_attack_acc(self, attack_pass):\n","        \"\"\"\n","        attacked data에 대한 정확도 \n","        \"\"\"\n","        attack_tups = self.data_package\n","        num_untrusted = len(attack_tups)\n","        filtered_attack_tups = attack_tups[attack_pass]\n","\n","\n","        both_acc = 1 - sum(1 for _, XpC in filtered_attack_tups if not XpC)/num_untrusted # detector and refomer\n","        det_only_acc = 1 - sum(1 for XC, XpC in filtered_attack_tups if not XC)/num_untrusted # only detector\n","        ref_only_acc = sum([1 for _, XpC in attack_tups if XpC])/num_untrusted # only reformer\n","        none_acc = sum([1 for XC, _ in attack_tups if XC])/num_untrusted # no defense\n","        \n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def plot_various_confidences(self, graph_name, drop_rate,\n","                                 idx_file=\"example_idx\",\n","                                 confs=(0.0, 10.0),\n","                                 get_attack_data_name=lambda c: \"example_carlini_\"+str(c)):\n","        \"\"\"\n","        Test defense performance against Carlini L2 attack of various confidences.\n","\n","        graph_name: Name of graph file.\n","        drop_rate: How many normal examples should each detector drops?\n","        idx_file: Index of adversarial examples in standard test set.\n","        confs: A series of confidence to test against.\n","        get_attack_data_name: Function mapping confidence to corresponding file.\n","        \"\"\"\n","        pylab.rcParams['figure.figsize'] = 6, 4\n","        fig = plt.figure(1, (6, 4))\n","        ax = fig.add_subplot(1, 1, 1)\n","\n","        idx = orig_labels\n","        # idx = original_labels_list\n","        X, _, Y = prepare_data(self.operator.data, idx)\n","\n","\n","        det_only = []\n","        ref_only = []\n","        both = []\n","        none = []\n","\n","        print(\"\\n==========================================================\")\n","        print(\"Drop Rate:\", drop_rate)\n","        thrs = self.operator.get_thrs(drop_rate)\n","\n","        all_pass, _ = self.operator.filter(self.operator.data.x_train, thrs)\n","        all_on_acc, _, _, _ = self.get_normal_acc(all_pass)\n","\n","        print(\"Classification accuracy with all defense on:\", all_on_acc)\n","\n","        for confidence in confs:\n","            # f = get_attack_data_name(confidence)\n","            self.load_data(AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\"))\n","\n","            print(\"----------------------------------------------------------\")\n","            print(\"Confidence:\", confidence)\n","            all_pass, detector_breakdown = self.operator.filter(self.untrusted_data.data, thrs)\n","            both_acc, det_only_acc, ref_only_acc, none_acc = self.get_attack_acc(all_pass)\n","            print(detector_breakdown)\n","            both.append(both_acc)\n","            det_only.append(det_only_acc)\n","            ref_only.append(ref_only_acc)\n","            none.append(none_acc)\n","\n","        size = 2.5\n","\n","        print(\"With detector & reformer: \", both_acc)\n","        print(\"With detector: \",det_only_acc)\n","        print(\"With reformer: \", ref_only_acc)\n","        print(\"No Defense: \",none_acc)\n","\n","        # print(\"With detector & reformer: \", both)\n","        # print(\"With detector: \",det_only)\n","        # print(\"With reformer: \", ref_only)\n","        # print(\"No Defense: \",none)\n","\n","        plt.plot(confs, none, c=\"green\", label=\"No Defense\", marker=\"x\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, det_only, c=\"orange\", label=\"With detector\", marker=\"o\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, ref_only, c=\"blue\", label=\"With reformer\", marker=\"^\", markersize=size,alpha=0.5)\n","        plt.plot(confs, both, c=\"red\", label=\"With detector & reformer\", marker=\"s\", markersize=size,alpha=0.5)\n","\n","        pylab.legend(loc='lower left', bbox_to_anchor=(0.02, 0.1), prop={'size':8})\n","        plt.grid(linestyle='dotted')\n","        plt.xlabel(r\"Confidence in Carlini $L^2$ attack\")\n","        plt.ylabel(\"Classification accuracy\")\n","        plt.xlim(min(confs)-1.0, max(confs)+1.0)\n","        plt.ylim(-0.05, 1.05)\n","        ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n","\n","        save_path = os.path.join(self.graph_dir, graph_name+\".pdf\")\n","        plt.savefig(save_path)\n","        plt.clf()\n","\n","    def print(self):\n","        return \" \".join([self.operator.print(), self.untrusted_data.print()])"],"metadata":{"id":"ZRv7EXuShHMK","executionInfo":{"status":"aborted","timestamp":1663410888021,"user_tz":-540,"elapsed":9,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - Defensive Model"],"metadata":{"id":"4e-GBUowhNUQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from keras.layers.core import Lambda\n","from keras.layers.merge import Average, add\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D\n","from keras.models import Model\n","import keras.regularizers as regs\n","\n","\n","class DenoisingAutoEncoder:\n","    def __init__(self, image_shape,\n","                 structure,\n","                 v_noise=0.0,\n","                 activation=\"relu\",\n","                 model_dir=\"./defensive_models/\",\n","                 reg_strength=0.0):\n","        \"\"\"\n","        Denoising Autoencoder(DAE)\n","        training data에 nosie를 추가하여 인코더에 넣어서 학습된 결과가 \n","        noise를 붙이기 전 데이터와의 error을 최소화하는 목적을 가진 Autoencoder\n","\n","        image_shape: Shape of input image. e.g. 28, 28, 1.\n","        structure: Structure of autoencoder.\n","        v_noise: Volume of noise while training.\n","        activation: What activation function to use.\n","        model_dir: Where to save / load model from.\n","        reg_strength: Strength of L2 regularization.\n","        \"\"\"\n","        h, w, c = image_shape\n","        self.image_shape = image_shape # shape of input image (32,32,3)\n","        self.model_dir = model_dir \n","        self.v_noise = v_noise\n","\n","        input_img = Input(shape=self.image_shape)\n","        x = input_img\n","\n","        # encoder 정의 \n","        for layer in structure: \n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\":\n","                x = MaxPooling2D((2, 2), padding=\"same\")(x)\n","            elif layer == \"average\":\n","                x = AveragePooling2D((2, 2), padding=\"same\")(x)\n","            else:\n","                print(layer, \"is not recognized!\")\n","                exit(0)\n","        \n","        for layer in reversed(structure):\n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\" or layer == \"average\":\n","                x = UpSampling2D((2, 2))(x)\n","\n","        # decoder 정의 \n","        decoded = Conv2D(c, (3, 3), activation='sigmoid', padding='same',\n","                         activity_regularizer=regs.l2(reg_strength))(x)\n","\n","        self.model = Model(input_img, decoded) # autoencoder 모델\n","\n","    def train(self, data, archive_name, num_epochs=100, batch_size=32):\n","        self.model.compile(loss='mean_squared_error',\n","                           metrics=['mean_squared_error'],\n","                           optimizer='adam')\n","        \n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train)) # 랜덤 노이즈 \n","        noisy_train_data = data.x_train + noise # Input Data에 랜덤 노이즈 추가 \n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0) # [0,1] 범위로 재구성\n","\n","        self.model.fit(noisy_train_data, data.x_train,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test, data.x_test),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        print(os.path.join(self.model_dir, archive_name))        \n","        self.model.save(os.path.join(self.model_dir, archive_name))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))\n","\n","\n","class PackedAutoEncoder:\n","    def __init__(self, image_shape, structure, data,\n","                 v_noise=0.1, n_pack=2, pre_epochs=3, activation=\"relu\",\n","                 model_dir=\"./defensive_models/\"):\n","        \"\"\"\n","        Train different autoencoders.\n","        Demo code for graybox scenario.\n","\n","        pre_epochs: How many epochs do we train before fine-tuning.\n","        n_pack: Number of autoencoders we want to train at once.\n","        \"\"\"\n","        self.v_noise = v_noise\n","        self.n_pack = n_pack\n","        self.model_dir = model_dir\n","        pack = []\n","\n","\n","\n","        for i in range(n_pack):\n","            dae = DenoisingAutoEncoder(image_shape, structure, v_noise=v_noise,\n","                                       activation=activation, model_dir=model_dir)\n","            dae.train(data, \"\", num_epochs=pre_epochs)\n","            pack.append(dae.model)\n","\n","\n","        shared_input = Input(shape=image_shape, name=\"shared_input\")\n","        outputs = [dae(shared_input) for dae in pack]\n","        avg_output = Average()(outputs)\n","        delta_outputs = [add([avg_output, Lambda(lambda x: -x)(output)])\n","                         for output in outputs]\n","\n","        self.model = Model(inputs=shared_input, outputs=outputs+delta_outputs)\n","\n","    def train(self, data, archive_name, alpha, num_epochs=10, batch_size=32):\n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train))\n","        noisy_train_data = data.x_train + noise\n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0)\n","\n","        train_zeros = [np.zeros_like(data.x_train)] * self.n_pack\n","        val_zeros = [np.zeros_like(data.x_test)] * self.n_pack\n","\n","        self.model.compile(loss=\"mean_squared_error\", optimizer=\"adam\",\n","                           loss_weights=[1.0]*self.n_pack + [-alpha]*self.n_pack)\n","\n","        self.model.fit(noisy_train_data,\n","                       [data.x_train]*self.n_pack + train_zeros,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test,\n","                            [data.x_test]*self.n_pack+val_zeros),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        for i in range(self.n_pack):\n","            model = Model(self.model.input, self.model.outputs[i])\n","            self.model.save(\"\")\n","            print(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","            self.model.save(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))"],"metadata":{"id":"HyIrYmBIhPRc","executionInfo":{"status":"aborted","timestamp":1663410888021,"user_tz":-540,"elapsed":8,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - Train Defensive Model"],"metadata":{"id":"ukxjkHWahV8B"}},{"cell_type":"code","source":["data.x_train, data.y_train, data.x_test, data.y_test =data.x_train/255, data.y_train/255, data.x_test/255, data.y_test/255"],"metadata":{"id":"Y-CppiQYhYQp","executionInfo":{"status":"aborted","timestamp":1663410888022,"user_tz":-540,"elapsed":8,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE = DenoisingAutoEncoder\n","PAE = PackedAutoEncoder\n","\n","shape = [32, 32, 3]\n","combination_I = [3, \"average\", 3]\n","combination_II = [3]\n","activation = \"sigmoid\"\n","reg_strength = 1e-9\n","epochs = 350\n","\n","# data = GTSRB()\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=8)\n","# AE_II.train(data, \"_8_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation)\n","# AE_II.train(data, \"_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)    \n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=32)\n","# AE_II.train(data, \"O32_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)  \n","\n","AE_I = DAE(shape, combination_I, v_noise=0.1, activation=activation, reg_strength=reg_strength)\n","AE_I.train(data, \"350_0903_DAE_GTSRB_I\", num_epochs=epochs)\n","\n","AE_II = DAE(shape, combination_II, v_noise=0.1, activation=activation,  reg_strength=reg_strength)\n","AE_II.train(data, \"350_0903_DAE_GTSRB_II\", num_epochs=epochs)"],"metadata":{"id":"C5S8s9wxhanJ","executionInfo":{"status":"aborted","timestamp":1663410888022,"user_tz":-540,"elapsed":8,"user":{"displayName":"김채현","userId":"14052029844236242388"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - 1"],"metadata":{"id":"mzltzwx8he_N"}},{"cell_type":"code","source":["# Attack 이미지 종류 선택 : (1) 공격 종류 (2) epsilon\n","\n","attack_type = \"FGSM\" # option : FGSM, PGD\n","epsilon_type = 0.03 # option : epss = [0.02, 0.03, 8/255, 0.05, 0.08, 0.10]\n","\n","if attack_type == \"FGSM\":\n","  dict_elem = FGSM_varying_epss[epsilon_type]\n","elif attack_type == \"PGD\":\n","  dict_elem = FGSM_varying_epss[epsilon_type]\n","\n","attacked_accuracy = dict_elem[0]\n","ad_examples1 = np.array(dict_elem[2])\n","orig_labels1 = to_categorial(dict_elem[3])"],"metadata":{"id":"Aa-02b6d8oHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원본 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(data.x_test, data.y_test)\n","\n","print('test set accuracy (original) : ', accuracy * 100)"],"metadata":{"id":"0vMxKPzjhhAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 공격받은 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(ad_examples1, orig_labels1)\n","\n","print('test set accuracy (attacked) : ', accuracy * 100)"],"metadata":{"id":"ca5TN0fGhrgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_detector_I = AEDetector(\"./defensive_models/350_0903_DAE_GTSRB_I\", p=2)\n","DAE_detector_II = AEDetector(\"./defensive_models/350_0903_DAE_GTSRB_II\", p=1)\n","DAE_reformer = SimpleReformer(\"./defensive_models/350_0903_DAE_GTSRB_I\")\n","\n","\n","DAE_id_reformer = IdReformer()\n","DAE_classifier = Classifier(\"./models/gtsrb_classifier\")"],"metadata":{"id":"L6gv49XOhtCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detector_JSD1 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=10)\n","detector_JSD2 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=40)\n","\n","\n","DAE_detector_dict = dict()\n","DAE_detector_dict[\"I\"] = DAE_detector_I\n","DAE_detector_dict[\"II\"] = DAE_detector_II\n","DAE_detector_dict[\"JSD1\"] = detector_JSD1\n","DAE_detector_dict[\"JSD2\"] = detector_JSD2"],"metadata":{"id":"qRcUtasdhuRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_operator = Operator(data, DAE_classifier, DAE_detector_dict, DAE_reformer)\n","DAE_testAttack = AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\")"],"metadata":{"id":"36Yx1eqahvso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_evaluator = Evaluator(DAE_operator, DAE_testAttack)\n","DAE_evaluator.plot_various_confidences(\"defense_performance\", drop_rate={\"I\": 0.001, \"II\": 0.001,\"JSD1\": 0.001,\"JSD2\": 0.001})"],"metadata":{"id":"C3MbM1HDhwv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('with reformer')\n","\n","predict = np.array((DAE_reformer.model.predict(ad_examples1))).reshape(-1,32,32,3)\n","predicted1 = np.argmax(classifier.model.predict(predict),axis=1)\n","\n","print(np.mean(predicted1[:len(orig_labels)] == orig_labels[:len(orig_labels)]))\n","# print(.sum(predicted==orig_labels))#,predicted)"],"metadata":{"id":"gBi_uO4Dhx6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(data.x_train))\n","print(type(ad_examples))"],"metadata":{"id":"AmEhY0bFhzJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defense-GAN"],"metadata":{"id":"vIK9wb4vIIlv"}},{"cell_type":"markdown","metadata":{"id":"P9v5DhrcQTi_"},"source":["###  Defense Model 1 : DefenseGAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1w18KhjvQaN-"},"outputs":[],"source":["import numpy as np\n","import os\n","import gzip\n","import urllib.request\n","\n","from keras.models import load_model\n","\n","def ordered_onehotencoding(labels):\n","    labels_ordered = []\n","    for i in range(len(labels)):\n","        if labels[i] == 3:\n","            labels_ordered.append(0)\n","        elif labels[i] == 7:\n","            labels_ordered.append(1)\n","        elif labels[i] == 9:\n","            labels_ordered.append(2)\n","        elif labels[i] == 10:\n","            labels_ordered.append(3)\n","        elif labels[i] == 11:\n","            labels_ordered.append(4)\n","        elif labels[i] == 12:\n","            labels_ordered.append(5)\n","        elif labels[i] == 13:\n","            labels_ordered.append(6)\n","        elif labels[i] == 17:\n","            labels_ordered.append(7)\n","        elif labels[i] == 18:\n","            labels_ordered.append(8)\n","        elif labels[i] == 25:\n","            labels_ordered.append(9)\n","        elif labels[i] == 35:\n","            labels_ordered.append(10)\n","        elif labels[i] == 38:\n","            labels_ordered.append(11)\n","    \n","    return np.array(labels_ordered)\n","\n","class GTSRB_defenseGAN:\n","    def __init__(self):\n","        imgs_path = \"Train\"\n","        data_list = []\n","        labels_list = []\n","\n","        result_class = [3,7, 9, 10, 11, 12, 13, 17, 18, 25, 35, 38]\n","\n","        for i in result_class:\n","            i_path = os.path.join(imgs_path, str(i)) # 3, 7, 9, 10, 11, 12,13, 17, 18, 25, 35, 38\n","            num = 0\n","            for img in os.listdir(i_path):\n","          \n","                im = Image.open(i_path +'/'+ img)\n","                im = im.resize((32,32))\n","                im = np.array(im)\n","\n","                data_list.append(im)\n","                labels_list.append(i)\n","                num = num + 1\n","                if num == 1000:\n","                    break;\n","\n","        data = np.array(data_list)\n","        labels = ordered_onehotencoding(labels_list)\n","\n","        labels = to_categorical(labels)\n","\n","        VALIDATION_SIZE = 5000\n","        \n","        data = (data.astype(np.float32) - 127.5) / 127.5 #모든 데이터 픽셀 값을 -1~1로 피팅 시킨다 (GAN 학습을 위함)\n","        \n","        self.x_train = np.array(data)\n","        self.y_train = labels\n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB_defenseGAN\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFZPnqlWRPA7"},"outputs":[],"source":["data_train_GAN = GTSRB_defenseGAN()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_zbYD_uRrNQ"},"outputs":[],"source":["print(data_train_GAN.x_train.shape)\n","print(data_train_GAN.y_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"nZvOoqJqTtgW"},"source":["GAN 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOz8KOvWRqFq"},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","\n","from keras.datasets import mnist\n","\n","from keras.models import Sequential, Model\n","\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","\n","from tensorflow.keras.optimizers import Adam\n","\n","noise_data = np.random.normal(0, 1, (32, 100))\n","#generated_images = 0.5 * generator.predict(np.random.normal(0, 1, (32, 100))) + 0.5\n","\n","def show_images(generated_images, n=4, m=8, figsize=(9, 5)):\n","    f, axes = plt.subplots(n, m, figsize=figsize)\n","    #plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)\n","    for i in range(0, n):\n","        for j in range(0, m):\n","            ax = axes[i][j]\n","            ax.imshow(generated_images[i * m + j])\n","            ax.grid(False)\n","            ax.xaxis.set_ticks([])\n","            ax.yaxis.set_ticks([])\n","    plt.tight_layout()\n","    plt.savefig('20220729_basicgan.svg')\n","    plt.show()   \n","#show_images(0.5 * generator.predict(np.random.normal(0, 1, (32, 100))) + 0.5)\n","\n","\n","## create generator         \n","generator_ = Sequential([\n","    Dense(128 * 8 * 8, activation=\"relu\", input_shape=(100,)), \n","    Reshape((8, 8, 128)), \n","    \n","    BatchNormalization(momentum=0.8), # what is batch normalization?? \n","    UpSampling2D(), # what is upsampling?? \n","    Conv2D(128, kernel_size=3, padding=\"same\"),\n","    Activation(\"relu\"), \n","    \n","    BatchNormalization(momentum=0.8), \n","    UpSampling2D(), \n","    Conv2D(64, kernel_size=3, padding=\"same\"), \n","    Activation(\"relu\"), \n","    \n","    BatchNormalization(momentum=0.8), \n","    Conv2D(3, kernel_size=3, padding=\"same\"), \n","    Activation(\"tanh\"), \n","])\n","\n","noise_input = Input(shape=(100,), name=\"noise_input\")\n","generator_base = Model(noise_input, generator_(noise_input), name=\"generator\")\n","\n","generator_.summary()# summary가 매우 유용하군요. \n","\n","optimizer = Adam(0.0002, 0.5)\n","generator_base.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","### create discriminator\n","discriminator_ = Sequential([\n","    Conv2D(32, kernel_size=3, strides=2, input_shape=(32, 32, 3), padding=\"same\"), \n","    LeakyReLU(alpha=0.2), \n","    Dropout(0.25), \n","    \n","    Conv2D(64, kernel_size=3, strides=2, padding=\"same\"), \n","    ZeroPadding2D(padding=((0,1),(0,1))), \n","    LeakyReLU(alpha=0.2), \n","    Dropout(0.25), \n","    BatchNormalization(momentum=0.8), \n","    \n","    Conv2D(128, kernel_size=3, strides=2, padding=\"same\"), \n","    LeakyReLU(alpha=0.2), \n","    Dropout(0.25), \n","    BatchNormalization(momentum=0.8), \n","    \n","    Conv2D(256, kernel_size=3, strides=1, padding=\"same\"), \n","    LeakyReLU(alpha=0.2), \n","    Dropout(0.25), \n","    Flatten(), \n","    Dense(1, activation='sigmoid'), \n","])\n","image_input = Input(shape=(32, 32, 3), name=\"image_input\")\n","\n","discriminator = Model(image_input, discriminator_(image_input), name=\"discriminator\")\n","discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","discriminator_.summary()\n","\n","### Combined Model\n","noise_input2 = Input(shape=(100,), name=\"noise_input2\")\n","\"\"\"\n","model과 sequential의 차이는?? \n","가설1: 레이어를 쌓는 것이 sequential 이라면, sequential을 쌓는 것이 model인가???\n","\n","1) 다음 모델의 경우는 랜덤으로 만든 이미지로부터 학습해서 새로운 이미지를 만들어내는 generator의 데이터를 \n","2) discriminator가 분류하는 형식으로 진행된다. \n","\"\"\"\n","combined = Model(noise_input2, discriminator(generator_base(noise_input2)))\n","combined.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NWG3wxrSF4p"},"outputs":[],"source":["## training\n","\"\"\"\n","- 이 코드에서는 fit을 사용한 것이 아니라, train_on_batch를 사용했음. \n","- train_on_batch와의 차이점?을 구글에 검색해보니, 큰 차이가 없다고 하긴 하는데\n","    - train_on_batch의 경우, 넘겨 받은 데이터에 대해서 gradient vector를 계산해서 적용하고 끝내는 것이고(1epoch)\n","    - fit의 경우는 epoch과 batch_size를 한번에 모두 넘겨준다는 것 정도가 차이가 된다. \n","- GAN의 경우, discriminator의 학습시 마다 generator가 생성하는 데이터가 변화하게 된다. \n","    - 즉 처음부터 모든 데이터가 존재하고 이를 한번에 학습시키는 fit과는 다르게, 한번씩 업데이트를 할때마다 모델이 변화하므로, \n","    - train_on_batch를 사용하는 것이 매우 합당함.\n","\"\"\"\n","batch_size = 256\n","half_batch = batch_size // 2\n","\n","def train(epochs, print_step=10):\n","    history = []\n","    for epoch in range(epochs):\n","        # discriminator 트레이닝 단계\n","        #######################################################################3\n","        # 데이터 절반은 실제 이미지, 절반은 generator가 생성한 가짜 이미지\n","        # discriminator가 실제 이미지와 가짜 이미지를 구별하도록 discriminator를 트레이닝\n","        discriminator.trainable = True\n","        d_loss_real = discriminator.train_on_batch(data_train_GAN.x_train[np.random.randint(0, data_train_GAN.x_train.shape[0], half_batch)], \n","                                                   np.ones((half_batch, 1)))\n","        d_loss_fake = discriminator.train_on_batch(generator_base.predict(np.random.normal(0, 1, (half_batch, 100))), \n","                                                   np.zeros((half_batch, 1)))\n","        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","        # generator 트레이닝 단계\n","        #######################################################################3\n","        # 전부 generator가 생성한 가짜 이미지를 사용. \n","        # discriminator가 구별하지 못하도록 generator를 트레이닝\n","        \n","        \"\"\"\n","        generator를 트레이닝할 때는, 반드시 discriminator가 필요함. \n","        generator가 만든 image를 평가해야 하고, 그래야 feedback이 생겨서 generator가 학습됨. \n","        따라서, generator는 combined model을 통해 학습시키는데, 이때, discriminator도 함께 학습되면 안되기 때문에\n","        discriminator.trainable 을 False로 변경시켜 둔다. \n","        \"\"\"\n","        noise = np.random.normal(0, 1, (batch_size, 100))\n","        discriminator.trainable = False \n","        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))  #여기서는 왜 만들어준 fake img의 y 값을 1로 두는 걸까 ..\n","        # 기록\n","        record = (epoch, d_loss[0], 100 * d_loss[1], g_loss[0], 100 * g_loss[1])\n","        history.append(record)\n","        if epoch % print_step == 0:\n","            print(\"%5d [D loss: %.3f, acc.: %.2f%%] [G loss: %.3f, acc.: %.2f%%]\" % record)\n","            show_images(0.5 * generator_base.predict(noise_data) + 0.5)\n","    return history\n","#%%time, 은\n","history100 = train(20000, 500)\n","show_images(0.5 * generator_base.predict(noise_data) + 0.5)"]},{"cell_type":"code","source":["# GAN 모델 저장\n","from keras.models import load_model\n","\n","generator_base.save('baseGAN_Generator_attacked0.02.h5')"],"metadata":{"id":"dXXJ-Wgyu_bT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BPL_wcJTvXO"},"source":["DefenseGAN 구현 - FGSM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5JFZsEPTqeY"},"outputs":[],"source":["ad_example_data = ad_examples1 #/255로 이미 정규화가 된 이미지이다\n","orig_label_data = orig_labels1\n","\n","print(ad_example_data.shape)\n","print(orig_label_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrHjDgblT_c1"},"outputs":[],"source":["def DefenseGAN(img_at,L,R):\n","    z_list = []\n","    img = img_at.reshape(32,32,3)\n","    img_st = (img - np.mean(img)) / np.std(img) \n","    img_var = tf.Variable(img_st,dtype = float)\n","    opt = tf.keras.optimizers.SGD(learning_rate=0.1,momentum = 0.7)\n","\n","    def compute():\n","        z_hats_recs = generator_base(z_var)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","\n","    for r in range(R):\n","        z = np.random.normal(0, 1, (1, 100))\n","        z_var = tf.Variable(z,dtype = float)\n","    \n","        for l in range(L):\n","            opt.minimize(compute,[z_var])\n","        z_list.append(z_var)\n","\n","    def compute_10(z):\n","        #generator_base.trainable = False #아직 더해야하는지 뺴야하는지 판단 x\n","        z_hats_recs = generator_base(z)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","    \n","\n","    loss_list = []\n","    \n","    for i in range(len(z_list)):\n","        loss = compute_10(z_list[i])\n","        loss_list.append(loss)\n","    \n","    index_min = np.argmin(loss_list)\n","\n","    z_min = np.array(z_list[index_min])\n","\n","    generated_images = 0.5 * generator_base.predict(z_min)+ 0.5\n","\n","    generated_images = generated_images.reshape(32,32,3)\n","\n","    return generated_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ks2QRZbUKO_"},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = DefenseGAN(data.reshape(32,32,3),200,10).reshape(-1,32,32,3)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmgqhBaWaqXz"},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_example_data[:100], orig_label_data[:100])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"]},{"cell_type":"markdown","source":["### DefenseGAN 구현 - BIM\n","\n"],"metadata":{"id":"2TtBNBJxn2Db"}},{"cell_type":"code","source":["ad_examples_BIM = np.array(ad_examples) # 이미 정규화되어서 나온 값으로 추가적인 /255 정규화 필요 없음\n","orig_labels_BIM = to_categorical(orig_labels)\n","\n","print(ad_examples_BIM.shape)\n","print(orig_labels_BIM.shape)"],"metadata":{"id":"fhZ-PyFGn3lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GAN 모델 저장\n","from keras.models import load_model\n","\n","generator_base = load_model('baseGAN_Generator_attacked0.02.h5')"],"metadata":{"id":"lZngn6VduEUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(generator_base.predict(np.random.normal(0, 1, (1, 100))).reshape(32,32,3))"],"metadata":{"id":"P4m8Jituudng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def DefenseGAN(img_at,L,R):\n","    z_list = []\n","    img = img_at.reshape(32,32,3)\n","    img_st = (img - np.mean(img)) / np.std(img) \n","    img_var = tf.Variable(img_st,dtype = float)\n","    opt = tf.keras.optimizers.SGD(learning_rate=0.1,momentum = 0.7)\n","\n","    def compute():\n","        z_hats_recs = generator_base(z_var)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","\n","    for r in range(R):\n","        z = np.random.normal(0, 1, (1, 100))\n","        z_var = tf.Variable(z,dtype = float)\n","    \n","        for l in range(L):\n","            opt.minimize(compute,[z_var])\n","        z_list.append(z_var)\n","\n","    def compute_10(z):\n","        #generator_base.trainable = False #아직 더해야하는지 뺴야하는지 판단 x\n","        z_hats_recs = generator_base(z)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","    \n","\n","    loss_list = []\n","    \n","    for i in range(len(z_list)):\n","        loss = compute_10(z_list[i])\n","        loss_list.append(loss)\n","    \n","    index_min = np.argmin(loss_list)\n","\n","    z_min = np.array(z_list[index_min])\n","\n","    generated_images = 0.5 * generator_base.predict(z_min)+ 0.5\n","\n","    generated_images = generated_images.reshape(32,32,3)\n","\n","    return generated_images"],"metadata":{"id":"2Zppe2W6n3n9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_example_data, orig_label_data)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = DefenseGAN(data.reshape(32,32,3),200,10).reshape(-1,32,32,3)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"],"metadata":{"id":"HsGI-NY4n3qe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_examples_BIM[:100], orig_labels_BIM[:100])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"],"metadata":{"id":"qnYmHP5Bn3tC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PCA"],"metadata":{"id":"63SBpYd-IL2u"}},{"cell_type":"markdown","metadata":{"id":"1ywsHS3sfPxD"},"source":["### Defense 2 : PCA (Components = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuoOeVKxsyvD"},"outputs":[],"source":["ad_example_data = ad_examples1 #/255로 이미 정규화가 된 이미지이다\n","orig_label_data = orig_labels1\n","\n","print(ad_example_data.shape)\n","print(orig_label_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAB3avuSbB3q"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2uWTxjIf-a-"},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = defense_PCA(data,5)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGm6ObXDhR2V"},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_example_data, orig_label_data)\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"]},{"cell_type":"markdown","metadata":{"id":"9Y_-9xiJiAeC"},"source":["### Defense 2 : PCA (Components = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOiE-bIgsz6w"},"outputs":[],"source":["ad_example_data = ad_examples1 #/255로 이미 정규화가 된 이미지이다\n","orig_label_data = orig_labels1\n","\n","print(ad_example_data.shape)\n","print(orig_label_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyLQ-kGQwqEZ"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWXuP7V4iPO0"},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = defense_PCA(data,10)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP7_KoXqiWhf"},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_example_data, orig_label_data)\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"]},{"cell_type":"markdown","source":["### Defense 2 : PCA (Components = 5)"],"metadata":{"id":"hGoEmuWF4S1T"}},{"cell_type":"code","source":["ad_examples_BIM = np.array(ad_examples) # 이미 정규화되어서 나온 값으로 추가적인 /255 정규화 필요 없음\n","orig_labels_BIM = to_categorical(orig_labels)\n","\n","print(ad_examples_BIM.shape)\n","print(orig_labels_BIM.shape)\n","\n","from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))\n","\n","## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples_BIM, orig_labels_BIM)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = defense_PCA(data,5)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"],"metadata":{"id":"sP6AyMGH4bkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_examples_BIM[:1000], orig_labels_BIM[:1000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"],"metadata":{"id":"pJv8QiS94bm8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defense 2 : PCA (Components = 10)"],"metadata":{"id":"7rWZ6_Kf48hG"}},{"cell_type":"code","source":["ad_examples_BIM = np.array(ad_examples) # 이미 정규화되어서 나온 값으로 추가적인 /255 정규화 필요 없음\n","orig_labels_BIM = to_categorical(orig_labels)\n","\n","print(ad_examples_BIM.shape)\n","print(orig_labels_BIM.shape)\n","\n","from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))\n","\n","## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples_BIM, orig_labels_BIM)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        generated_img = defense_PCA(data,10)\n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples"],"metadata":{"id":"fpPzHByO4bpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df = test(model, ad_examples_BIM[:1000], orig_labels_BIM[:1000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"],"metadata":{"id":"aCH5IdKe4br4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["lDK9o_--lA_g","bt4nIJ5xg73T","EhncucofhEvv","4e-GBUowhNUQ","ukxjkHWahV8B","vIK9wb4vIIlv","P9v5DhrcQTi_","2TtBNBJxn2Db","63SBpYd-IL2u","1ywsHS3sfPxD","9Y_-9xiJiAeC","hGoEmuWF4S1T","7rWZ6_Kf48hG"],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}