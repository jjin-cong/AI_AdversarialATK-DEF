{"cells":[{"cell_type":"markdown","source":["데이터셋 설정 관련\n","* for문으로 돌리려다가, 방어기법에서까지 하나하나 돌리는게 너무 오래 걸려서 코드로 나누고 계정별로 돌릴 수 있게 코드 변경했습니다\n","* 공격 종류 (FGSM, PGD), 공격 입실론 (0.02 등)을 목차 [Attack 수행 : 공격 데이터셋 만드는 코드] 부분에서 수정하고 코드 돌리면 됩니다\n","\n","1. 모든 train valid 데이터는 0~1사이로 정규화 되어있음.\n","2. 모든 test 데이터는 0~255로 정규화 안되어있음\n","3. target 값은 정규화하면 안됨."],"metadata":{"id":"05NggkxjGxlN"}},{"cell_type":"markdown","source":["# 기본 import"],"metadata":{"id":"H5r08xIXerXB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgKss5hJBb5B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import os\n","import pathlib\n","\n","import cv2 #영상처리에 사용하는 오픈소스 라이브러리, 컴퓨터가 사람 눈처럼 인식할 수 있게 처리\n","from PIL import Image # 파이썬 이미지 처리 pillow 라이브러리\n","from tensorflow.keras.preprocessing import image\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator #imagedatagenerater는 이미지를 학습시킬 때 학습 데이터의 양이 적을 경우 학습데이터를 조금씩 변형 시켜서 학습데이터의 양을 늘리는 방식중 하나\n","from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from tqdm.auto import tqdm\n","\n","#난수 랜덤성 고정\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19490,"status":"ok","timestamp":1664887473425,"user":{"displayName":"김채현","userId":"06024775478798789360"},"user_tz":-540},"id":"Qn7HC1qiBnNE","outputId":"8a80aee3-4a42-4737-8687-17d5cf80162b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1664887473848,"user":{"displayName":"김채현","userId":"06024775478798789360"},"user_tz":-540},"id":"VwTVXhm7BoaE","outputId":"de6e156b-566a-448b-c859-2aff78b94d4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB\n"]}],"source":["cd drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/GTSRB"]},{"cell_type":"markdown","source":["# Train & Test 데이터 불러오기"],"metadata":{"id":"FhnfS-0MH1sa"}},{"cell_type":"markdown","metadata":{"id":"JjBcm524Fams"},"source":["Train Data 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZnmTWHIBo_E"},"outputs":[],"source":["import numpy as np\n","import os\n","import gzip\n","import urllib.request\n","\n","from keras.models import load_model\n","\n","def ordered_onehotencoding(labels):\n","    labels_ordered = []\n","    for i in range(len(labels)):\n","        if labels[i] == 3:\n","            labels_ordered.append(0)\n","        elif labels[i] == 7:\n","            labels_ordered.append(1)\n","        elif labels[i] == 9:\n","            labels_ordered.append(2)\n","        elif labels[i] == 10:\n","            labels_ordered.append(3)\n","        elif labels[i] == 11:\n","            labels_ordered.append(4)\n","        elif labels[i] == 12:\n","            labels_ordered.append(5)\n","        elif labels[i] == 13:\n","            labels_ordered.append(6)\n","        elif labels[i] == 17:\n","            labels_ordered.append(7)\n","        elif labels[i] == 18:\n","            labels_ordered.append(8)\n","        elif labels[i] == 25:\n","            labels_ordered.append(9)\n","        elif labels[i] == 35:\n","            labels_ordered.append(10)\n","        elif labels[i] == 38:\n","            labels_ordered.append(11)\n","    \n","    return np.array(labels_ordered)\n","\n","class GTSRB:\n","    def __init__(self):\n","        imgs_path = \"Train\"\n","        data_list = []\n","        labels_list = []\n","\n","        result_class = [3,7, 9, 10, 11, 12, 13, 17, 18, 25, 35, 38]\n","\n","        for i in result_class:\n","            i_path = os.path.join(imgs_path, str(i)) # 3, 7, 9, 10, 11, 12,13, 17, 18, 25, 35, 38\n","            num = 0\n","            for img in os.listdir(i_path):\n","          \n","                im = Image.open(i_path +'/'+ img)\n","                im = im.resize((32,32))\n","                im = np.array(im)\n","\n","                data_list.append(im)\n","                labels_list.append(i)\n","                num = num + 1\n","                if num == 1000:\n","                    break;\n","\n","        data = np.array(data_list)\n","        labels = ordered_onehotencoding(labels_list)\n","\n","        labels = to_categorical(labels)\n","\n","        VALIDATION_SIZE = 5000\n","        \n","        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(np.array(data), labels, test_size=0.4)    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWo9BzndEm67"},"outputs":[],"source":["data = GTSRB()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zc5Q_EXiEppb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664887717602,"user_tz":-540,"elapsed":42,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"da89478d-1d2d-417f-8abe-1b2cb1598164"},"outputs":[{"output_type":"stream","name":"stdout","text":["(7200, 32, 32, 3)\n","(4800, 32, 32, 3)\n","(7200, 12)\n","(4800, 12)\n"]}],"source":["print(data.x_train.shape) # 0~255\n","print(data.x_test.shape) # 0~255\n","print(data.y_train.shape) # 0~11 원핫 인코딩\n","print(data.y_test.shape) # 0~11 원핫 인코딩"]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtM8GsuYfDFc","executionInfo":{"status":"ok","timestamp":1664887717603,"user_tz":-540,"elapsed":34,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"bb99af68-d8e3-45c3-fae3-69af1457a69c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","255\n","0\n","255\n"]}]},{"cell_type":"code","source":["data.x_train, data.y_train, data.x_test, data.y_test =data.x_train/255, data.y_train, data.x_test/255, data.y_test #0~1로 찐 train만 정규화"],"metadata":{"id":"L88pCixpf4wC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoKSRN2ef-n0","executionInfo":{"status":"ok","timestamp":1664887717603,"user_tz":-540,"elapsed":15,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"0615b5e7-affb-4880-f74f-926538a27166"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","0.0\n","1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"P2uSEM66FdVa"},"source":["Test Data 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_shX58HHFisj"},"outputs":[],"source":["metainfo = pd.read_csv(\"Meta.csv\")\n","traininfo = pd.read_csv(\"Train.csv\")\n","testinfo = pd.read_csv(\"Test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9HPAPg2FZ9L"},"outputs":[],"source":["import natsort\n","\n","class GTSRB_test:\n","    def __init__(self):\n","        imgs_path = \"Test\"\n","        data_list = []\n","        labels_list = []\n","        \n","        for img in natsort.natsorted(os.listdir(imgs_path)):\n","            im = Image.open(imgs_path +'/'+ img)\n","            im = im.resize((32,32))\n","            im = np.array(im)\n","            data_list.append(im)\n","        data_test = np.array(data_list)\n","        \n","        for i in range(len(testinfo.ClassId)):\n","            labels_list.append(testinfo.ClassId[i])\n","        \n","        labels_test = np.array(labels_list)\n","\n","        labels_test_index = []\n","        for i in range(len(labels_test)):\n","            if (labels_test[i] == 3) | (labels_test[i] == 7) | (labels_test[i] == 9) | (labels_test[i] == 10) | (labels_test[i] == 11) | (labels_test[i] == 12) | (labels_test[i] == 13) | (labels_test[i] == 17) | (labels_test[i] == 18) | (labels_test[i] == 25) | (labels_test[i] == 35) | (labels_test[i] == 38):\n","                labels_test_index.append(i)\n","\n","        test_data = []\n","        test_label = []\n","        for i in labels_test_index:\n","            test_data.append(data_test[i])\n","            test_label.append(labels_test[i])\n","\n","        data_test = np.array(test_data)\n","\n","        labels_test =ordered_onehotencoding(test_label)\n","\n","        labels_test = to_categorical(labels_test)\n","        \n","        self.x_test = data_test\n","        self.y_test = labels_test    \n","\n","    @staticmethod\n","    def print():\n","        return \"GTSRB_test\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcWfJaa6Ft7z"},"outputs":[],"source":["data_test = GTSRB_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VE9NTZrAFv7e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664888008251,"user_tz":-540,"elapsed":23,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"a78ebf5b-8e87-469a-cec1-6ab1aa653254"},"outputs":[{"output_type":"stream","name":"stdout","text":["(6180, 32, 32, 3)\n","(6180, 12)\n"]}],"source":["print(data_test.x_test.shape) # 0~255\n","print(data_test.y_test.shape) # 12개 원핫 인코딩"]},{"cell_type":"code","source":["print(np.min(data_test.x_test))\n","print(np.max(data_test.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phupmxLufTsN","executionInfo":{"status":"ok","timestamp":1664888008251,"user_tz":-540,"elapsed":15,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"3218a87c-aa22-47f2-bdbd-ea0809e795e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","255\n"]}]},{"cell_type":"markdown","metadata":{"id":"XWfrLWt3EtSN"},"source":["# 분류기 : CNN"]},{"cell_type":"code","source":["print(np.min(data.x_train))\n","print(np.max(data.x_train))\n","print(np.min(data.x_test))\n","print(np.max(data.x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ll78wYPewyM","executionInfo":{"status":"ok","timestamp":1664888008252,"user_tz":-540,"elapsed":11,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"1d94c111-fd6a-4d9f-e5e0-bb93e8373e2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","0.0\n","1.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI5nIfEQE0Dt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664888060246,"user_tz":-540,"elapsed":52000,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"c0e215d9-1011-4d3d-a34a-06f79a8c5af6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 30, 30, 96)        2688      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 15, 15, 96)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 15, 15, 96)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 13, 13, 192)       166080    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 6, 6, 192)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 6, 6, 192)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 192)         331968    \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                196672    \n","                                                                 \n"," dense_1 (Dense)             (None, 12)                780       \n","                                                                 \n","=================================================================\n","Total params: 698,188\n","Trainable params: 698,188\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/30\n","60/60 [==============================] - 13s 27ms/step - loss: 0.3198 - accuracy: 0.1254 - val_loss: 0.2709 - val_accuracy: 0.2104\n","Epoch 2/30\n","60/60 [==============================] - 1s 19ms/step - loss: 0.1916 - accuracy: 0.5378 - val_loss: 0.1025 - val_accuracy: 0.8696\n","Epoch 3/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0619 - accuracy: 0.9167 - val_loss: 0.0358 - val_accuracy: 0.9617\n","Epoch 4/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0297 - accuracy: 0.9712 - val_loss: 0.0241 - val_accuracy: 0.9744\n","Epoch 5/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0183 - accuracy: 0.9817 - val_loss: 0.0155 - val_accuracy: 0.9852\n","Epoch 6/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0128 - accuracy: 0.9885 - val_loss: 0.0131 - val_accuracy: 0.9890\n","Epoch 7/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0092 - accuracy: 0.9924 - val_loss: 0.0109 - val_accuracy: 0.9879\n","Epoch 8/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0079 - accuracy: 0.9928 - val_loss: 0.0108 - val_accuracy: 0.9881\n","Epoch 9/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0061 - accuracy: 0.9957 - val_loss: 0.0072 - val_accuracy: 0.9935\n","Epoch 10/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0047 - accuracy: 0.9962 - val_loss: 0.0073 - val_accuracy: 0.9937\n","Epoch 11/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0042 - accuracy: 0.9971 - val_loss: 0.0072 - val_accuracy: 0.9942\n","Epoch 12/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0038 - accuracy: 0.9969 - val_loss: 0.0057 - val_accuracy: 0.9958\n","Epoch 13/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0028 - accuracy: 0.9983 - val_loss: 0.0057 - val_accuracy: 0.9954\n","Epoch 14/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0025 - accuracy: 0.9981 - val_loss: 0.0055 - val_accuracy: 0.9956\n","Epoch 15/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0021 - accuracy: 0.9986 - val_loss: 0.0052 - val_accuracy: 0.9948\n","Epoch 16/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0021 - accuracy: 0.9989 - val_loss: 0.0058 - val_accuracy: 0.9948\n","Epoch 17/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0018 - accuracy: 0.9982 - val_loss: 0.0053 - val_accuracy: 0.9946\n","Epoch 18/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0057 - val_accuracy: 0.9948\n","Epoch 19/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0047 - val_accuracy: 0.9969\n","Epoch 20/30\n","60/60 [==============================] - 1s 24ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0045 - val_accuracy: 0.9969\n","Epoch 21/30\n","60/60 [==============================] - 1s 24ms/step - loss: 9.5333e-04 - accuracy: 0.9997 - val_loss: 0.0043 - val_accuracy: 0.9967\n","Epoch 22/30\n","60/60 [==============================] - 1s 18ms/step - loss: 6.0016e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9971\n","Epoch 23/30\n","60/60 [==============================] - 1s 24ms/step - loss: 6.0315e-04 - accuracy: 0.9999 - val_loss: 0.0055 - val_accuracy: 0.9962\n","Epoch 24/30\n","60/60 [==============================] - 1s 18ms/step - loss: 6.8637e-04 - accuracy: 0.9997 - val_loss: 0.0054 - val_accuracy: 0.9962\n","Epoch 25/30\n","60/60 [==============================] - 1s 18ms/step - loss: 8.8281e-04 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9962\n","Epoch 26/30\n","60/60 [==============================] - 1s 19ms/step - loss: 9.1439e-04 - accuracy: 0.9996 - val_loss: 0.0058 - val_accuracy: 0.9960\n","Epoch 27/30\n","60/60 [==============================] - 1s 18ms/step - loss: 7.1275e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9971\n","Epoch 28/30\n","60/60 [==============================] - 1s 18ms/step - loss: 3.6259e-04 - accuracy: 0.9999 - val_loss: 0.0044 - val_accuracy: 0.9969\n","Epoch 29/30\n","60/60 [==============================] - 1s 18ms/step - loss: 4.3057e-04 - accuracy: 0.9999 - val_loss: 0.0058 - val_accuracy: 0.9962\n","Epoch 30/30\n","60/60 [==============================] - 1s 18ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0046 - val_accuracy: 0.9973\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","import tensorflow as tf\n","import os\n","\n","\n","def train(data, file_name, num_epochs=50, batch_size=128):\n","    \"\"\"\n","    Standard neural network training procedure.\n","    \"\"\"\n","    model = Sequential()\n","\n","    IMG_HEIGHT = 32\n","    IMG_WIDTH = 32\n","\n","    # 첫번째 Convolutional Layer : 입력 데이터로부터 특징을 추출\n","    model.add(Conv2D(filters=96, kernel_size=3, activation='relu', input_shape=data.x_train.shape[1:]))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25))\n","\n","    # 두번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Dropout(rate=0.25)) # 인풋데이터의 25%를 무작위로 0으로 만듦\n","\n","    # 세번째 Convolutional Layer\n","    model.add(Conv2D(filters=192, kernel_size=3, activation='relu')) # 특징을 추출하는 기능을 하는 필터, 비선형 값으로 바꿔주는 activation 함수->relu\n","    # model.add(GlobalAveragePooling2D())\n","    model.add(Flatten())\n","\n","    model.add(Dense(units=64, activation='relu'))\n","    model.add(Dense(12, activation='softmax'))\n","\n","\n","    # 모델 컴파일 하기\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","\n","    # 모델 핏하기\n","    EPOCHS = num_epochs\n","    model.fit(data.x_train, data.y_train,\n","              validation_data = (data.x_test, data.y_test), \n","              epochs=EPOCHS, steps_per_epoch=60\n","              )\n","\n","    if file_name != None:\n","        model.save(file_name)\n","\n","    return model\n","\n","\n","if not os.path.isdir('models'):\n","    os.makedirs('models')\n","\n","model = train(data,None, num_epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ez-W2phKFLws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664888062410,"user_tz":-540,"elapsed":2180,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"9e752911-e382-4aba-8735-0e2427f206a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["225/225 [==============================] - 1s 3ms/step - loss: 1.5197e-04 - accuracy: 1.0000\n","train set accuracy:  100.0\n","150/150 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 0.9973\n","valid set accuracy:  99.72916841506958\n"]}],"source":["loss, accuracy = model.evaluate(data.x_train, data.y_train)\n","\n","print('train set accuracy: ', accuracy * 100) #train 성능\n","\n","loss, accuracy = model.evaluate(data.x_test, data.y_test)\n","\n","print('valid set accuracy: ', accuracy * 100) #val 성능"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbYmfSHYHOIa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664888063948,"user_tz":-540,"elapsed":1542,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"bb7c77d7-9210-4205-ea26-d1379e71d187"},"outputs":[{"output_type":"stream","name":"stdout","text":["194/194 [==============================] - 1s 4ms/step - loss: 0.0356 - accuracy: 0.9693\n","test set accuracy with nomalization:  96.92556858062744\n"]}],"source":["loss, accuracy = model.evaluate(data_test.x_test/255, data_test.y_test)\n","\n","print('test set accuracy with nomalization: ', accuracy * 100) #찐 test"]},{"cell_type":"markdown","source":["# 공격 데이터셋 : FGSM & PGD"],"metadata":{"id":"QNRCIvO3-8lf"}},{"cell_type":"code","source":["def tf_preprocess(image):\n","  image = tf.cast(image, tf.float32)\n","  image = image/255\n","  image = tf.image.resize(image, (32, 32))\n","  image = image[None, ...]\n","  return image\n","\n","# 확률 벡터에서 레이블을 추출해주는 헬퍼 메서드\n","def get_tf_label(labels):\n","    label = tf.cast(labels, tf.int32)\n","    label = tf.reshape(label,[1,12])\n","    return label\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","\n","def create_adversarial_pattern(input_image, input_label):\n","  with tf.GradientTape() as tape:\n","    tape.watch(input_image)\n","    input_img = tf.reshape(input_image,[1,32,32,3])\n","    prediction = model(input_img)\n","    loss = loss_object(input_label, prediction)\n","\n","  # 입력 이미지에 대한 손실 함수의 기울기를 구합니다.\n","  gradient = tape.gradient(loss, input_image)\n","  # 왜곡을 생성하기 위해 그래디언트의 부호를 구합니다.\n","  signed_grad = tf.sign(gradient)\n","  return signed_grad"],"metadata":{"id":"Dn9x64obA9UB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SUb6-MoJtay"},"source":["### FGSM & PGD define"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGBdi7tWJT1K"},"outputs":[],"source":["def fgsm_attack(model,test_x,test_y,eps):\n","    \n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # FGSM 공격 수행\n","        perturbations = create_adversarial_pattern(img, label)\n","        adv_x = img + eps * perturbations\n","        adv_x = tf.clip_by_value(adv_x, 0, 1) # 공격받은 이미지\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(target_label)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"]},{"cell_type":"code","source":["def pgd_attack(model,test_x,test_y,eps,step_size=2,num_steps=7): \n","    \"\"\"\n","    FGSM 코드와 차이점\n","    - step_size, num_steps 파라미터 추가됨\n","    - unifrom distribution 코드 추가\n","    - FGSM 공격 수행 -> PGD 공격 수행\n","    ** 모든 return 형식은 동일함\n","    \n","    default 값\n","    - step_size = 2 (alpha 값)\n","    - num_steps = 7 (iterations 값)\n","\n","    \"\"\"\n","\n","    prog = 0 # 진행상황 확인용 변수\n","\n","    correct = 0\n","    adv_examples = []\n","    save_adv_examples = [] # 공격받은 이미지들이 저장될 리스트\n","    save_original_output = [] # 공격받은 이미지들의 정답 라벨 값이 저장될 리스트\n","    \n","    for i in range(len(test_x)):\n","        # 1장의 이미지와 그 label\n","        data = test_x[i]\n","        target_onehot = test_y[i] # one-hot 형태\n","        target_label = int(np.argmax(target_onehot)) # label 형태\n","\n","        # model이 정상 데이터를 분류한 결과 (각각 one-hot 형태, int label 형태)\n","        result_onehot = model.predict(data.reshape(1,32,32,3) / 255) # one-hot 형태\n","        result_label = int(np.argmax(result_onehot))\n","\n","        # 모델이 정상 데이터인데도 잘못 분류했다면 사용하지 않는다 (아래 코드 실행하지 않고 다음 이미지로 넘어감)\n","        if target_label != result_label:\n","            continue\n","\n","        # PGD uniform distribution 코드\n","        data = data + np.random.uniform(-eps,eps,data.shape)\n","        data = np.clip(data,0,255)\n","\n","        # 이미지 전처리\n","        img =  tf_preprocess(data) # 텐서플로 전처리 -> 0~1사이로 정규화 함.\n","        label = get_tf_label(target_onehot) # 확률벡터에서 레이블 추출\n","        \n","        # PGD 공격 수행\n","        adv_x = img # 공격받은 이미지 (for문으로 업데이트)\n","        for num_step in range(num_steps):\n","          perturbations = create_adversarial_pattern(adv_x,label) # signed_grad를 리턴한 값\n","          adv_x += step_size * perturbations\n","          adv_x = tf.clip_by_value(adv_x,img-eps,img+eps)\n","          adv_x = tf.clip_by_value(adv_x,0,1)\n","\n","        # 공격 이미지를 분류기에 넣은 결과; 잘못 분류되어야 할 것 \n","        atkresult_onehot = model.predict(adv_x) # one-hot 형태\n","        atkresult_label = int(np.argmax(atkresult_onehot)) # label 형태\n","\n","        # 만약 공격 받아도 제대로 분류된다면 correct로 count\n","        if atkresult_label == target_label:\n","            correct += 1\n","        # ####################################################################################\n","        # ################ 여기 코드는 필요 없지 않나?\n","        #     if (eps == 0) and (len(adv_examples) < 5):\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # else:\n","        #     if len(adv_examples) < 5:\n","        #         adv_ex = adv_x\n","        #         adv_examples.append((init_output,final_pred,adv_x))\n","        # ####################################################################################\n","        \n","        # 공격 적용된 이미지, 그 공격 받은 이미지의 원래 정답 label을 각각 리스트에 저장합니다\n","        save_adv_examples.append(tf.reshape(adv_x,[32,32,3]))\n","        save_original_output.append(target_label) #공격 받은 이미지에 원래 타겟 값을 저장합니다 ! \n","\n","        prog += 1\n","\n","        if prog%500 == 0:\n","          print(\"prog :\", prog)\n","\n","    # 해당 엡실론에서의 최종 정확도를 계산합니다\n","    final_acc = correct/float(len(test_x))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(eps, correct, len(test_x), final_acc))\n","\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return final_acc, adv_examples, save_adv_examples, save_original_output"],"metadata":{"id":"GmiLgFduVhR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Attack 수행 : 공격 데이터셋 만드는 코드\n","\n","**여기에서 [attack_type]과 [eps] 설정하면 됩니다!!!**\n","\n","* attack_type : FGSM, PGD\n","* eps = 0.02, 0.03, 8/255, 0.05, 0.08, 0.10\n","* 정상 이미지에 대한 분류기 정확도 -> 위에 있음 (분류기:CNN; 약 97%)"],"metadata":{"id":"AVs5AnagBdTK"}},{"cell_type":"code","source":["################################################################################\n","# 공격 데이터 설정 #################################################################\n","attack_type = \"PGD\" # FGSM, PGD\n","eps = 0.05 # 0.02, 0.03, 0.04, 0.05, 0.08, 0.10\n","################################################################################\n","################################################################################\n","\n","if attack_type == \"FGSM\":\n","  acc, ex, ad_examples, orig_labels = fgsm_attack(model, data_test.x_test[:50], data_test.y_test[:50], eps) #x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\n","elif attack_type == \"PGD\":\n","  acc, ex, ad_examples, orig_labels = pgd_attack(model, data_test.x_test, data_test.y_test, eps) #x 공격데이터 : 0~1 정규화 완료, y 데이터 : 12개 라벨\n","\n","print(\"Attack Data 생성 완료\")"],"metadata":{"id":"ZE2wZtfAEOBh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664888987856,"user_tz":-540,"elapsed":923911,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"d8eaa728-ea0a-473b-fdda-2564e91ed618"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prog : 500\n","prog : 1000\n","prog : 1500\n","prog : 2000\n","prog : 2500\n","prog : 3000\n","prog : 3500\n","prog : 4000\n","prog : 4500\n","prog : 5000\n","prog : 5500\n","Epsilon: 0.05\tTest Accuracy = 1692 / 6180 = 0.2737864077669903\n","Attack Data 생성 완료\n"]}]},{"cell_type":"code","source":["print(orig_labels[0])\n","plt.figure()\n","plt.imshow(ad_examples[0])\n","plt.show();"],"metadata":{"id":"Fjghmlv4kRKa","executionInfo":{"status":"ok","timestamp":1664888987857,"user_tz":-540,"elapsed":39,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"8bbddab1-d56f-48bf-a843-dc92bfd39e20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["plt.imshow(data_test.x_test[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"z4Zq6MPEWwio","executionInfo":{"status":"ok","timestamp":1664888987857,"user_tz":-540,"elapsed":25,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"f2298b4d-c746-4667-91e4-bcdd115aff96"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7eff147545d0>"]},"metadata":{},"execution_count":24},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["# 방어모델 : MagNet, Defense-GAN, PCA"],"metadata":{"id":"6DfDZcE5_GGB"}},{"cell_type":"markdown","source":["## MagNet"],"metadata":{"id":"Lb-doZmmIFqj"}},{"cell_type":"markdown","source":["### MagNet - def (utils, worker, Defensive Model)"],"metadata":{"id":"-Yc-yrW7ktMW"}},{"cell_type":"markdown","source":["#### MagNet - utils"],"metadata":{"id":"bt4nIJ5xg73T"}},{"cell_type":"code","source":["## utils.py -- utility functions\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","def prepare_data(dataset, idx):\n","    \"\"\"\n","    Extract data from index.\n","\n","    dataset: Full, working dataset. Such as MNIST().\n","    idx: Index of test examples that we care about.\n","    return: X, targets, Y\n","    \"\"\"\n","    return dataset.x_test[idx], dataset.y_test[idx], np.argmax(dataset.y_test[idx], axis=1)\n","\n","\n","def save_obj(obj, name, directory='./attack_data/'):\n","    with open(os.path.join(directory, name + '.pkl'), 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","\n","def load_obj(name, directory='./attack_data/'):\n","    if name.endswith(\".pkl\"): name = name[:-4]\n","    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n","        return pickle.load(f)"],"metadata":{"id":"hAvlahKmMFdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - worker"],"metadata":{"id":"EhncucofhEvv"}},{"cell_type":"code","source":["## setup_mnist.py -- mnist data and model loading code\n","##\n","## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","## Modified for MagNet's use.\n","\n","## worker.py -- evaluation code\n","##\n","## Copyright (C) 2017, Dongyu Meng <zbshfmmm@gmail.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","from scipy.stats import entropy\n","from numpy.linalg import norm\n","from matplotlib.ticker import FuncFormatter\n","from keras.models import Sequential, load_model\n","from keras.activations import softmax\n","from keras.layers import Lambda\n","import numpy as np\n","import pylab\n","import os\n","import matplotlib.pyplot as plt\n","\n","\n","class AEDetector:\n","    def __init__(self, path, p=1):\n","        \"\"\"\n","        Error based detector.\n","        Marks examples for filtering decisions.\n","\n","        path: Path to the autoencoder used.\n","        p: Distance measure to use.\n","        \"\"\"\n","\n","        self.model = load_model(path)\n","        self.path = path\n","        self.p = p\n","\n","    def mark(self, X):\n","        diff = np.abs(X - self.model.predict(X)) # input X와 예측값 X'(autoencoder를 통해 노이즈가 더해진 값) 의 오차 값\n","        marks = np.mean(np.power(diff, self.p), axis=(1,2,3)) # 오차값의 분산\n","        return marks\n","\n","    def print(self):\n","        return \"AEDetector:\" + self.path.split(\"/\")[-1]\n","\n","\n","class IdReformer:\n","    def __init__(self, path=\"IdentityFunction\"):\n","        \"\"\"\n","        Identity reformer.\n","        Reforms an example to itself.\n","        \"\"\"\n","        self.path = path\n","        self.heal = lambda X: X\n","\n","    def print(self):\n","        return \"IdReformer:\" + self.path\n","\n","\n","class SimpleReformer:\n","    def __init__(self, path):\n","        \"\"\"\n","        Reformer.\n","        Reforms examples with autoencoder. Action of reforming is called heal.\n","\n","        path: Path to the autoencoder used.\n","        \"\"\"\n","        self.model = load_model(path)\n","        self.path = path\n","\n","    def heal(self, X):\n","        X = self.model.predict(X) # autoencoder로 X값 재구성\n","        return np.clip(X, 0.0, 1.0)\n","\n","    def print(self):\n","        return \"SimpleReformer:\" + self.path.split(\"/\")[-1]\n","\n","\n","def JSD(P, Q):\n","    _P = P / norm(P, ord=1)\n","    _Q = Q / norm(Q, ord=1)\n","    _M = 0.5 * (_P + _Q)\n","    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M)) # Xp와 Xr의 분포의 entropy \n","    # KL divergence: Q(one autoencoder)를 기반으로 했을 때의 cross entropy와 P(magnet)를 기반으로 했을 때의 entropy의 차이\n","\n","\n","\n","class DBDetector:\n","    def __init__(self, reconstructor, prober, classifier, option=\"jsd\", T=1):\n","        \"\"\"\n","        Divergence-Based Detector.\n","\n","        reconstructor: One autoencoder.\n","        prober: Another autoencoder.\n","        classifier: Classifier object.\n","        option: Measure of distance, jsd as default.\n","        T: Temperature to soften the classification decision.\n","        \"\"\"\n","        self.prober = prober\n","        self.reconstructor = reconstructor\n","        self.classifier = classifier\n","        self.option = option\n","        self.T = T\n","\n","    def mark(self, X):\n","        return self.mark_jsd(X)\n","\n","    def mark_jsd(self, X):\n","        Xp = self.prober.heal(X) # 1번 autoencoder로 생성한 이미지 \n","        Xr = self.reconstructor.heal(X) #2번 autoencoder로 생성한 이미지 \n","        Pp = self.classifier.classify(Xp, option=\"prob\", T=self.T) # Xp의 확률\n","        Pr = self.classifier.classify(Xr, option=\"prob\", T=self.T) # Xr의 확률\n","\n","        marks = [(JSD(Pp[i], Pr[i])) for i in range(len(Pr))]\n","        return np.array(marks)\n","\n","    def print(self):\n","        return \"Divergence-Based Detector\"\n","\n","\n","class Classifier:\n","    def __init__(self, classifier_path):\n","        \"\"\"\n","        Keras classifier wrapper.\n","        Note that the wrapped classifier should spit logits as output.\n","\n","        classifier_path: Path to Keras classifier file.\n","        \"\"\"\n","        self.path = classifier_path\n","        self.model = load_model(classifier_path)\n","        self.softmax = Sequential()\n","        self.softmax.add(Lambda(lambda X: softmax(X, axis=1)))\n","\n","    def classify(self, X, option=\"logit\", T=1):\n","        if option == \"logit\":\n","            return self.model.predict(X)\n","        if option == \"prob\":\n","            logits = self.model.predict(X)/T\n","            return self.softmax.predict(logits)\n","\n","    def print(self):\n","        return \"Classifier:\"+self.path.split(\"/\")[-1]\n","\n","\n","class Operator:\n","    def __init__(self, data, classifier, det_dict, reformer):\n","        \"\"\"\n","        Operator.\n","        Describes the classification problem and defense.\n","\n","        data: Standard problem dataset. Including train, test, and validation.\n","        classifier: Target classifier.\n","        reformer: Reformer of defense.\n","        det_dict: Detector(s) of defense.\n","        \"\"\"\n","\n","        self.data = data\n","        self.classifier = classifier\n","        self.det_dict = det_dict \n","        self.reformer = reformer\n","        self.normal = self.operate(AttackData(data.x_train, np.argmax(data.y_train, axis=1), \"Normal\"))\n","        \n","\n","    def get_thrs(self, drop_rate):\n","        \"\"\"\n","        Get filtering threshold by marking validation set.\n","        \"\"\"\n","        thrs = dict()\n","        for name, detector in self.det_dict.items():\n","            num = int(len(data.x_test) * drop_rate[name])\n","            marks = detector.mark(data.x_test)\n","            marks = np.sort(marks)\n","            thrs[name] = marks[-num]\n","        return thrs\n","\n","    def operate(self, untrusted_obj):\n","        \"\"\"\n","        For untrusted input(normal or adversarial), classify original input and\n","        reformed input. Classifier is unaware of the source of input.\n","\n","        untrusted_obj: Input data.\n","        \"\"\"\n","\n","        X = untrusted_obj.data\n","        Y_true = untrusted_obj.labels\n","\n","\n","        X_prime = self.reformer.heal(X) # autoencoder 값으로 재구성\n","        Y = np.argmax(self.classifier.classify(X), axis=1) # 원본 input X 분류\n","        Y_judgement = (Y == Y_true[:len(X_prime)]) # 실제 label과 X 분류 label 비교\n","        Y_prime = np.argmax(self.classifier.classify(X_prime), axis=1)  # autoencoder로 재구성한 X' 분류\n","        Y_prime_judgement = (Y_prime == Y_true[:len(X_prime)])  # 실제 label과 X' 분류 label 비교\n","        return np.array(list(zip(Y_judgement, Y_prime_judgement)))\n","\n","    def filter(self, X, thrs):\n","        \"\"\"\n","        untrusted_obj: Untrusted input to test against.\n","        thrs: Thresholds.\n","\n","        return:\n","        all_pass: Index of examples that passed all detectors.\n","        collector: Number of examples that escaped each detector.\n","        \"\"\"\n","        collector = dict()\n","        all_pass = np.array(range(10000)) #Index\n","        for name, detector in self.det_dict.items():\n","            marks = detector.mark(X) #  KL divergnece: Xp와 Xr의 분포의 entropy \n","            idx_pass = np.argwhere(marks < thrs[name]) # KL divergnece가 thershold보다 작을 경우 pass, 클 경우 reject\n","            collector[name] = len(idx_pass) # pass가 된 수\n","            all_pass = np.intersect1d(all_pass, idx_pass) # 전체 index array와 pass된 array의 교집합\n","        return all_pass, collector\n","\n","    def print(self):\n","        components = [self.reformer, self.classifier]\n","        return \" \".join(map(lambda obj: getattr(obj, \"print\")(), components))\n","\n","\n","class AttackData:\n","    def __init__(self, examples, labels, name=\"\"):\n","        \"\"\"\n","        Input data wrapper. May be normal or adversarial.\n","\n","        examples: Path or object of input examples.\n","        labels: Ground truth labels.\n","        \"\"\"\n","        # if isinstance(examples, str): \n","        #   self.data = load_obj(examples)\n","        # else: \n","\n","        self.data = examples\n","        self.labels = labels\n","        self.name = name\n","\n","    def print(self):\n","        return \"Attack:\"+self.name\n","\n","\n","class Evaluator:\n","    def __init__(self, operator, untrusted_data, graph_dir=\"./graph\"):\n","        \"\"\"\n","        Evaluator.\n","        For strategy described by operator, conducts tests on untrusted input.\n","        Mainly stats and plotting code. Most methods omitted for clarity.\n","\n","        operator: Operator object.\n","        untrusted_data: Input data to test against.\n","        graph_dir: Where to spit the graphs.\n","        \"\"\"\n","        self.operator = operator\n","        self.untrusted_data = untrusted_data # attacked data\n","        self.graph_dir = graph_dir\n","        self.data_package = operator.operate(untrusted_data)\n","\n","    def bind_operator(self, operator):\n","        self.operator = operator\n","        self.data_package = operator.operate(self.untrusted_data)\n","\n","    def load_data(self, data):\n","        self.untrusted_data = data\n","        self.data_package = self.operator.operate(self.untrusted_data)\n","\n","    def get_normal_acc(self, normal_all_pass):\n","        \"\"\"\n","        traning data에 대한 정확도\n","\n","        Break down of who does what in defense. Accuracy of defense on normal\n","        input.\n","\n","        both: Both detectors and reformer take effect\n","        det_only: detector(s) take effect\n","        ref_only: Only reformer takes effect\n","        none: Attack effect with no defense\n","        \"\"\"\n","        normal_tups = self.operator.normal\n","        num_normal = len(normal_tups)\n","        filtered_normal_tups = normal_tups[normal_all_pass]\n","\n","        both_acc = sum(1 for _, XpC in filtered_normal_tups if XpC)/num_normal # detector and refomer\n","        det_only_acc = sum(1 for XC, XpC in filtered_normal_tups if XC)/num_normal # only detector\n","        ref_only_acc = sum([1 for _, XpC in normal_tups if XpC])/num_normal # only reformer\n","        none_acc = sum([1 for XC, _ in normal_tups if XC])/num_normal # no defense\n","\n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def get_attack_acc(self, attack_pass):\n","        \"\"\"\n","        attacked data에 대한 정확도 \n","        \"\"\"\n","        attack_tups = self.data_package\n","        num_untrusted = len(attack_tups)\n","        filtered_attack_tups = attack_tups[attack_pass]\n","\n","\n","        both_acc = 1 - sum(1 for _, XpC in filtered_attack_tups if not XpC)/num_untrusted # detector and refomer\n","        det_only_acc = 1 - sum(1 for XC, XpC in filtered_attack_tups if not XC)/num_untrusted # only detector\n","        ref_only_acc = sum([1 for _, XpC in attack_tups if XpC])/num_untrusted # only reformer\n","        none_acc = sum([1 for XC, _ in attack_tups if XC])/num_untrusted # no defense\n","        \n","        return both_acc, det_only_acc, ref_only_acc, none_acc\n","\n","    def plot_various_confidences(self, graph_name, drop_rate,\n","                                 idx_file=\"example_idx\",\n","                                 confs=(0.0, 10.0),\n","                                 get_attack_data_name=lambda c: \"example_carlini_\"+str(c)):\n","        \"\"\"\n","        Test defense performance against Carlini L2 attack of various confidences.\n","\n","        graph_name: Name of graph file.\n","        drop_rate: How many normal examples should each detector drops?\n","        idx_file: Index of adversarial examples in standard test set.\n","        confs: A series of confidence to test against.\n","        get_attack_data_name: Function mapping confidence to corresponding file.\n","        \"\"\"\n","        pylab.rcParams['figure.figsize'] = 6, 4\n","        fig = plt.figure(1, (6, 4))\n","        ax = fig.add_subplot(1, 1, 1)\n","\n","        idx = orig_labels\n","        # idx = original_labels_list\n","        X, _, Y = prepare_data(self.operator.data, idx)\n","\n","\n","        det_only = []\n","        ref_only = []\n","        both = []\n","        none = []\n","\n","        print(\"\\n==========================================================\")\n","        print(\"Drop Rate:\", drop_rate)\n","        thrs = self.operator.get_thrs(drop_rate)\n","\n","        all_pass, _ = self.operator.filter(self.operator.data.x_train, thrs)\n","        all_on_acc, _, _, _ = self.get_normal_acc(all_pass)\n","\n","        print(\"Classification accuracy with all defense on:\", all_on_acc)\n","\n","        for confidence in confs:\n","            # f = get_attack_data_name(confidence)\n","            self.load_data(AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\"))\n","\n","            print(\"----------------------------------------------------------\")\n","            print(\"Confidence:\", confidence)\n","            all_pass, detector_breakdown = self.operator.filter(self.untrusted_data.data, thrs)\n","            both_acc, det_only_acc, ref_only_acc, none_acc = self.get_attack_acc(all_pass)\n","            print(detector_breakdown)\n","            both.append(both_acc)\n","            det_only.append(det_only_acc)\n","            ref_only.append(ref_only_acc)\n","            none.append(none_acc)\n","\n","        size = 2.5\n","\n","        print(\"With detector & reformer: \", both_acc)\n","        print(\"With detector: \",det_only_acc)\n","        print(\"With reformer: \", ref_only_acc)\n","        print(\"No Defense: \",none_acc)\n","\n","        # print(\"With detector & reformer: \", both)\n","        # print(\"With detector: \",det_only)\n","        # print(\"With reformer: \", ref_only)\n","        # print(\"No Defense: \",none)\n","\n","        plt.plot(confs, none, c=\"green\", label=\"No Defense\", marker=\"x\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, det_only, c=\"orange\", label=\"With detector\", marker=\"o\", markersize=size,alpha=0.5)\n","        # plt.plot(confs, ref_only, c=\"blue\", label=\"With reformer\", marker=\"^\", markersize=size,alpha=0.5)\n","        plt.plot(confs, both, c=\"red\", label=\"With detector & reformer\", marker=\"s\", markersize=size,alpha=0.5)\n","\n","        pylab.legend(loc='lower left', bbox_to_anchor=(0.02, 0.1), prop={'size':8})\n","        plt.grid(linestyle='dotted')\n","        plt.xlabel(r\"Confidence in Carlini $L^2$ attack\")\n","        plt.ylabel(\"Classification accuracy\")\n","        plt.xlim(min(confs)-1.0, max(confs)+1.0)\n","        plt.ylim(-0.05, 1.05)\n","        ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n","\n","        save_path = os.path.join(self.graph_dir, graph_name+\".pdf\")\n","        plt.savefig(save_path)\n","        plt.clf()\n","\n","    def print(self):\n","        return \" \".join([self.operator.print(), self.untrusted_data.print()])"],"metadata":{"id":"ZRv7EXuShHMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MagNet - Defensive Model"],"metadata":{"id":"4e-GBUowhNUQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from keras.layers.core import Lambda\n","from keras.layers.merge import Average, add\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D\n","from keras.models import Model\n","import keras.regularizers as regs\n","\n","\n","class DenoisingAutoEncoder:\n","    def __init__(self, image_shape,\n","                 structure,\n","                 v_noise=0.0,\n","                 activation=\"relu\",\n","                 model_dir=\"./defensive_models/\",\n","                 reg_strength=0.0):\n","        \"\"\"\n","        Denoising Autoencoder(DAE)\n","        training data에 nosie를 추가하여 인코더에 넣어서 학습된 결과가 \n","        noise를 붙이기 전 데이터와의 error을 최소화하는 목적을 가진 Autoencoder\n","\n","        image_shape: Shape of input image. e.g. 28, 28, 1.\n","        structure: Structure of autoencoder.\n","        v_noise: Volume of noise while training.\n","        activation: What activation function to use.\n","        model_dir: Where to save / load model from.\n","        reg_strength: Strength of L2 regularization.\n","        \"\"\"\n","        h, w, c = image_shape\n","        self.image_shape = image_shape # shape of input image (32,32,3)\n","        self.model_dir = model_dir \n","        self.v_noise = v_noise\n","\n","        input_img = Input(shape=self.image_shape)\n","        x = input_img\n","\n","        # encoder 정의 \n","        for layer in structure: \n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\":\n","                x = MaxPooling2D((2, 2), padding=\"same\")(x)\n","            elif layer == \"average\":\n","                x = AveragePooling2D((2, 2), padding=\"same\")(x)\n","            else:\n","                print(layer, \"is not recognized!\")\n","                exit(0)\n","        \n","        for layer in reversed(structure):\n","            if isinstance(layer, int):\n","                x = Conv2D(layer, (3, 3), activation=activation, padding=\"same\",\n","                           activity_regularizer=regs.l2(reg_strength))(x)\n","            elif layer == \"max\" or layer == \"average\":\n","                x = UpSampling2D((2, 2))(x)\n","\n","        # decoder 정의 \n","        decoded = Conv2D(c, (3, 3), activation='sigmoid', padding='same',\n","                         activity_regularizer=regs.l2(reg_strength))(x)\n","\n","        self.model = Model(input_img, decoded) # autoencoder 모델\n","\n","    def train(self, data, archive_name, num_epochs=100, batch_size=32):\n","        self.model.compile(loss='mean_squared_error',\n","                           metrics=['mean_squared_error'],\n","                           optimizer='adam')\n","        \n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train)) # 랜덤 노이즈 \n","        noisy_train_data = data.x_train + noise # Input Data에 랜덤 노이즈 추가 \n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0) # [0,1] 범위로 재구성\n","\n","        self.model.fit(noisy_train_data, data.x_train,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test, data.x_test),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        print(os.path.join(self.model_dir, archive_name))        \n","        self.model.save(os.path.join(self.model_dir, archive_name))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))\n","\n","\n","class PackedAutoEncoder:\n","    def __init__(self, image_shape, structure, data,\n","                 v_noise=0.1, n_pack=2, pre_epochs=3, activation=\"relu\",\n","                 model_dir=\"./defensive_models/\"):\n","        \"\"\"\n","        Train different autoencoders.\n","        Demo code for graybox scenario.\n","\n","        pre_epochs: How many epochs do we train before fine-tuning.\n","        n_pack: Number of autoencoders we want to train at once.\n","        \"\"\"\n","        self.v_noise = v_noise\n","        self.n_pack = n_pack\n","        self.model_dir = model_dir\n","        pack = []\n","\n","\n","\n","        for i in range(n_pack):\n","            dae = DenoisingAutoEncoder(image_shape, structure, v_noise=v_noise,\n","                                       activation=activation, model_dir=model_dir)\n","            dae.train(data, \"\", num_epochs=pre_epochs)\n","            pack.append(dae.model)\n","\n","\n","        shared_input = Input(shape=image_shape, name=\"shared_input\")\n","        outputs = [dae(shared_input) for dae in pack]\n","        avg_output = Average()(outputs)\n","        delta_outputs = [add([avg_output, Lambda(lambda x: -x)(output)])\n","                         for output in outputs]\n","\n","        self.model = Model(inputs=shared_input, outputs=outputs+delta_outputs)\n","\n","    def train(self, data, archive_name, alpha, num_epochs=10, batch_size=32):\n","        noise = self.v_noise * np.random.normal(size=np.shape(data.x_train))\n","        noisy_train_data = data.x_train + noise\n","        noisy_train_data = np.clip(noisy_train_data, 0.0, 1.0)\n","\n","        train_zeros = [np.zeros_like(data.x_train)] * self.n_pack\n","        val_zeros = [np.zeros_like(data.x_test)] * self.n_pack\n","\n","        self.model.compile(loss=\"mean_squared_error\", optimizer=\"adam\",\n","                           loss_weights=[1.0]*self.n_pack + [-alpha]*self.n_pack)\n","\n","        self.model.fit(noisy_train_data,\n","                       [data.x_train]*self.n_pack + train_zeros,\n","                       batch_size=batch_size,\n","                       validation_data=(data.x_test,\n","                            [data.x_test]*self.n_pack+val_zeros),\n","                       epochs=num_epochs,\n","                       shuffle=True)\n","\n","        for i in range(self.n_pack):\n","            model = Model(self.model.input, self.model.outputs[i])\n","            self.model.save(\"\")\n","            print(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","            self.model.save(os.path.join(self.model_dir, archive_name+\"_\"+str(i)))\n","\n","    def load(self, archive_name, model_dir=None):\n","        if model_dir is None: model_dir = self.model_dir\n","        self.model.load_weights(os.path.join(model_dir, archive_name))"],"metadata":{"id":"HyIrYmBIhPRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - Train Defensive Model"],"metadata":{"id":"ukxjkHWahV8B"}},{"cell_type":"code","source":["DAE = DenoisingAutoEncoder\n","PAE = PackedAutoEncoder\n","\n","shape = [32, 32, 3]\n","combination_I = [3, \"average\", 3]\n","combination_II = [3]\n","activation = \"sigmoid\"\n","reg_strength = 1e-9\n","epochs = 350\n","\n","# data = GTSRB()\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=8)\n","# AE_II.train(data, \"_8_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)\n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation)\n","# AE_II.train(data, \"_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)    \n","\n","# AE_II = PAE(shape, combination_II, data, v_noise=0.025, activation=activation, n_pack=32)\n","# AE_II.train(data, \"O32_PAE_GTSRB_II\", alpha=.2, num_epochs=epochs)  \n","\n","AE_I = DAE(shape, combination_I, v_noise=0.1, activation=activation, reg_strength=reg_strength)\n","#AE_I.train(data, \"350_0918_DAE_GTSRB_I\", num_epochs=epochs)\n","\n","AE_II = DAE(shape, combination_II, v_noise=0.1, activation=activation,  reg_strength=reg_strength)\n","#AE_II.train(data, \"350_0918_DAE_GTSRB_II\", num_epochs=epochs)"],"metadata":{"id":"C5S8s9wxhanJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MagNet - 1"],"metadata":{"id":"mzltzwx8he_N"}},{"cell_type":"code","source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"],"metadata":{"id":"BZXZc5yXGbHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ad_examples1.shape"],"metadata":{"id":"jNeNPwhcwXgR","executionInfo":{"status":"ok","timestamp":1664888990246,"user_tz":-540,"elapsed":34,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9119ae5-059b-491b-8a25-e42d2e8da060"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5990, 32, 32, 3)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["print(np.min(ad_examples1[0]))\n","print(np.max(ad_examples1[0]))"],"metadata":{"id":"n88gOua2r0cF","executionInfo":{"status":"ok","timestamp":1664888990247,"user_tz":-540,"elapsed":32,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2195d56e-9d73-4b61-f3a1-100c9955099b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.058741488\n","0.9175941\n"]}]},{"cell_type":"code","source":["orig_labels1[0]"],"metadata":{"id":"S6TVgUZer8GA","executionInfo":{"status":"ok","timestamp":1664888990247,"user_tz":-540,"elapsed":21,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ca57af4-725e-459d-83ea-1b2d728834f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":[" data_test.y_test[0]"],"metadata":{"id":"W0UMav8hwSJ0","executionInfo":{"status":"ok","timestamp":1664888990247,"user_tz":-540,"elapsed":12,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bbbdda98-da4c-4544-d36a-883d9940e7e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# 원본 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(data_test.x_test/255, data_test.y_test)\n","\n","print('test set accuracy (original) : ', accuracy * 100)"],"metadata":{"id":"0vMxKPzjhhAy","executionInfo":{"status":"error","timestamp":1664888996055,"user_tz":-540,"elapsed":5812,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"colab":{"base_uri":"https://localhost:8080/","height":766},"outputId":"33fb7944-b922-49e9-a4ba-3a8ebf736fc2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-95d01cb2189a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 원본 이미지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/gtsrb_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test set accuracy (original) : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1473, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 14) vs (None, 12)).\n"]}]},{"cell_type":"code","source":["# 공격받은 이미지\n","classifier = Classifier(\"./models/gtsrb_classifier\")\n","loss, accuracy = classifier.model.evaluate(ad_examples1, orig_labels1)\n","\n","print('test set accuracy (attacked) : ', accuracy * 100)"],"metadata":{"id":"ca5TN0fGhrgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_detector_I = AEDetector(\"./defensive_models/350_0918_DAE_GTSRB_I\", p=2)\n","DAE_detector_II = AEDetector(\"./defensive_models/350_0918_DAE_GTSRB_II\", p=1)\n","DAE_reformer = SimpleReformer(\"./defensive_models/350_0918_DAE_GTSRB_I\")\n","\n","\n","DAE_id_reformer = IdReformer()\n","DAE_classifier = Classifier(\"./models/gtsrb_classifier\")"],"metadata":{"id":"L6gv49XOhtCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detector_JSD1 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=10)\n","detector_JSD2 = DBDetector(DAE_id_reformer, DAE_reformer, DAE_classifier, T=40)\n","\n","\n","DAE_detector_dict = dict()\n","DAE_detector_dict[\"I\"] = DAE_detector_I\n","DAE_detector_dict[\"II\"] = DAE_detector_II\n","DAE_detector_dict[\"JSD1\"] = detector_JSD1\n","DAE_detector_dict[\"JSD2\"] = detector_JSD2"],"metadata":{"id":"qRcUtasdhuRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_operator = Operator(data, DAE_classifier, DAE_detector_dict, DAE_reformer)\n","DAE_testAttack = AttackData(ad_examples1, orig_labels, \"GTSRB FSGM\")"],"metadata":{"id":"36Yx1eqahvso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DAE_evaluator = Evaluator(DAE_operator, DAE_testAttack)\n","\n","DAE_evaluator.plot_various_confidences(\"defense_performance\", drop_rate={\"I\": 0.001, \"II\": 0.001,\"JSD1\": 0.001,\"JSD2\": 0.001})"],"metadata":{"id":"C3MbM1HDhwv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('with reformer')\n","start = time.time()\n","predict = np.array((DAE_reformer.model.predict(ad_examples1))).reshape(-1,32,32,3)\n","activ_time = time.time() - start\n","meantime = activ_time / len(ad_examples1)\n","predicted1 = np.argmax(classifier.model.predict(predict),axis=1)\n","\n","print(np.mean(predicted1[:len(orig_labels)] == orig_labels[:len(orig_labels)]))\n","# print(.sum(predicted==orig_labels))#,predicted)\n","print(meantime)"],"metadata":{"id":"gBi_uO4Dhx6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(data.x_train))\n","print(type(ad_examples))"],"metadata":{"id":"AmEhY0bFhzJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defense-GAN"],"metadata":{"id":"vIK9wb4vIIlv"}},{"cell_type":"markdown","metadata":{"id":"P9v5DhrcQTi_"},"source":["###  Defense Model 1 : DefenseGAN"]},{"cell_type":"markdown","source":["### DefenseGAN 구현\n"],"metadata":{"id":"2TtBNBJxn2Db"}},{"cell_type":"code","source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"],"metadata":{"id":"oDv8n7tn6Hep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ad_examples1.shape)\n","print(orig_labels1.shape)"],"metadata":{"id":"fhZ-PyFGn3lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GAN 모델 저장\n","from keras.models import load_model\n","\n","generator_base = load_model('baseGAN_Generator_attacked0.02.h5')"],"metadata":{"id":"lZngn6VduEUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(generator_base.predict(np.random.normal(0, 1, (1, 100))).reshape(32,32,3))"],"metadata":{"id":"P4m8Jituudng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt"],"metadata":{"id":"g5GA13HB7okG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def DefenseGAN(img_at,L,R):\n","    z_list = []\n","    img = img_at.reshape(32,32,3)\n","    img_st = (img - np.mean(img)) / np.std(img) \n","    img_var = tf.Variable(img_st,dtype = float)\n","    opt = tf.keras.optimizers.SGD(learning_rate=0.1,momentum = 0.7)\n","\n","    def compute():\n","        z_hats_recs = generator_base(z_var)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","\n","    for r in range(R):\n","        z = np.random.normal(0, 1, (1, 100))\n","        z_var = tf.Variable(z,dtype = float)\n","    \n","        for l in range(L):\n","            opt.minimize(compute,[z_var])\n","        z_list.append(z_var)\n","\n","    def compute_10(z):\n","        #generator_base.trainable = False #아직 더해야하는지 뺴야하는지 판단 x\n","        z_hats_recs = generator_base(z)\n","        z_hats_recs = tf.reshape(z_hats_recs, [32,32,3])\n","        num_dim = len(z_hats_recs.get_shape())\n","        axes = range(1, num_dim)\n","        image_rec_loss = tf.reduce_mean(tf.square(z_hats_recs - img_var),axis=axes)\n","        rec_loss = tf.reduce_sum(image_rec_loss)\n","        return rec_loss\n","    \n","\n","    loss_list = []\n","    \n","    for i in range(len(z_list)):\n","        loss = compute_10(z_list[i])\n","        loss_list.append(loss)\n","    \n","    index_min = np.argmin(loss_list)\n","\n","    z_min = np.array(z_list[index_min])\n","\n","    generated_images = 0.5 * generator_base.predict(z_min)+ 0.5\n","\n","    generated_images = generated_images.reshape(32,32,3)\n","\n","    return generated_images"],"metadata":{"id":"2Zppe2W6n3n9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_example_data, orig_label_data)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","\n","        generated_img = DefenseGAN(data.reshape(32,32,3),200,10).reshape(-1,32,32,3)\n","\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"],"metadata":{"id":"HsGI-NY4n3qe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df, meantime = test(model, ad_examples1[:100], orig_labels1[:100])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)"],"metadata":{"id":"qnYmHP5Bn3tC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meantime"],"metadata":{"id":"PBdm3k8iA2kP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PCA"],"metadata":{"id":"63SBpYd-IL2u"}},{"cell_type":"markdown","metadata":{"id":"1ywsHS3sfPxD"},"source":["### Defense 2 : PCA (Components = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuoOeVKxsyvD"},"outputs":[],"source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAB3avuSbB3q"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2uWTxjIf-a-"},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","        generated_img = defense_PCA(data,5)\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGm6ObXDhR2V"},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df,meantime = test(model, ad_examples1[:2000], orig_labels1[:2000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)\n","meantime"]},{"cell_type":"markdown","metadata":{"id":"9Y_-9xiJiAeC"},"source":["### Defense 2 : PCA (Components = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOiE-bIgsz6w"},"outputs":[],"source":["ad_examples1 = np.array(ad_examples)\n","orig_labels1 = to_categorical(orig_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyLQ-kGQwqEZ"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# data shape이 32,32,3이어야한다.\n","def defense_PCA(data,component):\n","    #r,g,b를 각각 나눠준다\n","    data = data.reshape(32,32,3)\n","    r = data[:,:,0]\n","    g = data[:,:,1]\n","    b = data[:,:,2]\n","\n","    pca_r = PCA(n_components=component)\n","    pca_r_trans = pca_r.fit_transform(r)\n","\n","    pca_g = PCA(n_components=component)\n","    pca_g_trans = pca_g.fit_transform(g)\n","\n","    pca_b = PCA(n_components=component)\n","    pca_b_trans = pca_b.fit_transform(b)\n","\n","    pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","    pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","    pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","    img_compressed = np.stack((pca_r_org, pca_g_org, pca_b_org),axis = 2)\n","\n","    return img_compressed.reshape((-1,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWXuP7V4iPO0"},"outputs":[],"source":["## predict_output에 한개씩 넣는 것부터 구현해야한다. (전체가 들어가는 것만 정상 작동함)\n","\n","def test(classifier, ad_example_data, orig_label_data):\n","\n","    # 정확도 카운터\n","    correct = 0\n","    defense_correct = 0\n","    df_examples = []\n","    alltime = []\n","    # 테스트 셋의 모든 예제에 대해 루프를 돕니다\n","\n","    loss, accuracy = classifier.evaluate(ad_examples1, orig_labels1)\n","\n","    print('방어 전 모델 정확도 : ',accuracy * 100)\n","\n","    for i in range(len(ad_example_data)):\n","        data = ad_example_data[i].reshape(-1,32,32,3)\n","        target = orig_label_data[i]\n","        \n","        data_plot = data.reshape(32,32,3)\n","        print('원본 label : ',np.argmax(target))\n","        plt.figure()\n","        plt.imshow(data_plot)\n","        plt.show();\n","\n","        start = time.time()\n","        generated_img = defense_PCA(data,10)\n","        activ_time = time.time() - start \n","\n","        generated_img_plot = generated_img.reshape(32,32,3)\n","\n","        defense_output = classifier.predict(generated_img.reshape(1,32,32,3))\n","\n","        defense_pred= int(np.argmax(defense_output))\n","\n","        print('방어 라벨 : ',defense_pred)\n","        plt.figure()\n","        plt.imshow(generated_img_plot)\n","        plt.show();\n","\n","        if defense_pred == int(np.argmax(target)):\n","            defense_correct += 1\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","        else:\n","            df_ex = generated_img\n","            df_examples.append((np.argmax(target),defense_pred,generated_img))\n","\n","        print('Working...!')\n","\n","        alltime.append(activ_time)\n","\n","    meantime = sum(alltime) / len(alltime)\n","    defense_acc = defense_correct / float(len(ad_example_data))\n","    print(\"Defense Accuracy = {} / {} = {}\".format(defense_correct, len(ad_example_data), defense_acc))\n","    # 정확도와 적대적 예제를 리턴합니다\n","    return defense_acc, df_examples, meantime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP7_KoXqiWhf"},"outputs":[],"source":["accuracies_df = []\n","examples_df = []\n","\n","acc_df, ex_df,meantime = test(model, ad_examples1[:2000], orig_labels1[:2000])\n","\n","accuracies_df.append(acc_df)\n","examples_df.append(ex_df)\n","meantime"]},{"cell_type":"code","source":["  "],"metadata":{"id":"M-46grPbaoGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G0j5-A4iyADA"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}