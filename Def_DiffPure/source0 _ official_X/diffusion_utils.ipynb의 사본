{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"diffusion_utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZkZwsG//Z0CvZMV/jMzEy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X4P0ASR77W_u"},"outputs":[],"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","\n","from . import nn\n","\n","\n","def normal_kl(mean1, logvar1, mean2, logvar2):\n","  \"\"\"\n","  KL divergence between normal distributions parameterized by mean and log-variance.\n","  \"\"\"\n","  return 0.5 * (-1.0 + logvar2 - logvar1 + tf.exp(logvar1 - logvar2)\n","                + tf.squared_difference(mean1, mean2) * tf.exp(-logvar2))\n","\n","\n","def _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):\n","  betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","  warmup_time = int(num_diffusion_timesteps * warmup_frac)\n","  betas[:warmup_time] = np.linspace(beta_start, beta_end, warmup_time, dtype=np.float64)\n","  return betas\n","\n","\n","def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","  if beta_schedule == 'quad':\n","    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n","  elif beta_schedule == 'linear':\n","    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n","  elif beta_schedule == 'warmup10':\n","    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n","  elif beta_schedule == 'warmup50':\n","    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n","  elif beta_schedule == 'const':\n","    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","  elif beta_schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","    betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n","  else:\n","    raise NotImplementedError(beta_schedule)\n","  assert betas.shape == (num_diffusion_timesteps,)\n","  return betas\n","\n","\n","def noise_like(shape, noise_fn=tf.random_normal, repeat=False, dtype=tf.float32):\n","  repeat_noise = lambda: tf.repeat(noise_fn(shape=(1, *shape[1:]), dtype=dtype), repeats=shape[0], axis=0)\n","  noise = lambda: noise_fn(shape=shape, dtype=dtype)\n","  return repeat_noise() if repeat else noise()\n","\n","\n","class GaussianDiffusion:\n","  \"\"\"\n","  Contains utilities for the diffusion model.\n","  \"\"\"\n","\n","  def __init__(self, *, betas, loss_type, tf_dtype=tf.float32):\n","    self.loss_type = loss_type\n","\n","    assert isinstance(betas, np.ndarray)\n","    self.np_betas = betas = betas.astype(np.float64)  # computations here in float64 for accuracy\n","    assert (betas > 0).all() and (betas <= 1).all()\n","    timesteps, = betas.shape\n","    self.num_timesteps = int(timesteps)\n","\n","    alphas = 1. - betas\n","    alphas_cumprod = np.cumprod(alphas, axis=0)\n","    alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n","    assert alphas_cumprod_prev.shape == (timesteps,)\n","\n","    self.betas = tf.constant(betas, dtype=tf_dtype)\n","    self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf_dtype)\n","    self.alphas_cumprod_prev = tf.constant(alphas_cumprod_prev, dtype=tf_dtype)\n","\n","    # calculations for diffusion q(x_t | x_{t-1}) and others\n","    self.sqrt_alphas_cumprod = tf.constant(np.sqrt(alphas_cumprod), dtype=tf_dtype)\n","    self.sqrt_one_minus_alphas_cumprod = tf.constant(np.sqrt(1. - alphas_cumprod), dtype=tf_dtype)\n","    self.log_one_minus_alphas_cumprod = tf.constant(np.log(1. - alphas_cumprod), dtype=tf_dtype)\n","    self.sqrt_recip_alphas_cumprod = tf.constant(np.sqrt(1. / alphas_cumprod), dtype=tf_dtype)\n","    self.sqrt_recipm1_alphas_cumprod = tf.constant(np.sqrt(1. / alphas_cumprod - 1), dtype=tf_dtype)\n","\n","    # calculations for posterior q(x_{t-1} | x_t, x_0)\n","    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n","    # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n","    self.posterior_variance = tf.constant(posterior_variance, dtype=tf_dtype)\n","    # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n","    self.posterior_log_variance_clipped = tf.constant(np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf_dtype)\n","    self.posterior_mean_coef1 = tf.constant(\n","      betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod), dtype=tf_dtype)\n","    self.posterior_mean_coef2 = tf.constant(\n","      (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod), dtype=tf_dtype)\n","\n","  @staticmethod\n","  def _extract(a, t, x_shape):\n","    \"\"\"\n","    Extract some coefficients at specified timesteps,\n","    then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n","    \"\"\"\n","    bs, = t.shape\n","    assert x_shape[0] == bs\n","    out = tf.gather(a, t)\n","    assert out.shape == [bs]\n","    return tf.reshape(out, [bs] + ((len(x_shape) - 1) * [1]))\n","\n","  def q_mean_variance(self, x_start, t):\n","    mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n","    variance = self._extract(1. - self.alphas_cumprod, t, x_start.shape)\n","    log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n","    return mean, variance, log_variance\n","\n","  def q_sample(self, x_start, t, noise=None):\n","    \"\"\"\n","    Diffuse the data (t == 0 means diffused for 1 step)\n","    \"\"\"\n","    if noise is None:\n","      noise = tf.random_normal(shape=x_start.shape)\n","    assert noise.shape == x_start.shape\n","    return (\n","        self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n","        self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n","    )\n","\n","  def predict_start_from_noise(self, x_t, t, noise):\n","    assert x_t.shape == noise.shape\n","    return (\n","        self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n","        self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n","    )\n","\n","  def q_posterior(self, x_start, x_t, t):\n","    \"\"\"\n","    Compute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0)\n","    \"\"\"\n","    assert x_start.shape == x_t.shape\n","    posterior_mean = (\n","        self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n","        self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n","    )\n","    posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n","    posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n","    assert (posterior_mean.shape[0] == posterior_variance.shape[0] == posterior_log_variance_clipped.shape[0] ==\n","            x_start.shape[0])\n","    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n","\n","  def p_losses(self, denoise_fn, x_start, t, noise=None):\n","    \"\"\"\n","    Training loss calculation\n","    \"\"\"\n","    B, H, W, C = x_start.shape.as_list()\n","    assert t.shape == [B]\n","\n","    if noise is None:\n","      noise = tf.random_normal(shape=x_start.shape, dtype=x_start.dtype)\n","    assert noise.shape == x_start.shape and noise.dtype == x_start.dtype\n","    x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n","    x_recon = denoise_fn(x_noisy, t)\n","    assert x_noisy.shape == x_start.shape\n","    assert x_recon.shape[:3] == [B, H, W] and len(x_recon.shape) == 4\n","\n","    if self.loss_type == 'noisepred':\n","      # predict the noise instead of x_start. seems to be weighted naturally like SNR\n","      assert x_recon.shape == x_start.shape\n","      losses = nn.meanflat(tf.squared_difference(noise, x_recon))\n","    else:\n","      raise NotImplementedError(self.loss_type)\n","\n","    assert losses.shape == [B]\n","    return losses\n","\n","  def p_mean_variance(self, denoise_fn, *, x, t, clip_denoised: bool):\n","    if self.loss_type == 'noisepred':\n","      x_recon = self.predict_start_from_noise(x, t=t, noise=denoise_fn(x, t))\n","    else:\n","      raise NotImplementedError(self.loss_type)\n","\n","    if clip_denoised:\n","      x_recon = tf.clip_by_value(x_recon, -1., 1.)\n","\n","    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n","    assert model_mean.shape == x_recon.shape == x.shape\n","    assert posterior_variance.shape == posterior_log_variance.shape == [x.shape[0], 1, 1, 1]\n","    return model_mean, posterior_variance, posterior_log_variance\n","\n","  def p_sample(self, denoise_fn, *, x, t, noise_fn, clip_denoised=True, repeat_noise=False):\n","    \"\"\"\n","    Sample from the model\n","    \"\"\"\n","    model_mean, _, model_log_variance = self.p_mean_variance(denoise_fn, x=x, t=t, clip_denoised=clip_denoised)\n","    noise = noise_like(x.shape, noise_fn, repeat_noise)\n","    assert noise.shape == x.shape\n","    # no noise when t == 0\n","    nonzero_mask = tf.reshape(1 - tf.cast(tf.equal(t, 0), tf.float32), [x.shape[0]] + [1] * (len(x.shape) - 1))\n","    return model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise\n","\n","  def p_sample_loop(self, denoise_fn, *, shape, noise_fn=tf.random_normal):\n","    \"\"\"\n","    Generate samples\n","    \"\"\"\n","    i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\n","    assert isinstance(shape, (tuple, list))\n","    img_0 = noise_fn(shape=shape, dtype=tf.float32)\n","    _, img_final = tf.while_loop(\n","      cond=lambda i_, _: tf.greater_equal(i_, 0),\n","      body=lambda i_, img_: [\n","        i_ - 1,\n","        self.p_sample(denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn)\n","      ],\n","      loop_vars=[i_0, img_0],\n","      shape_invariants=[i_0.shape, img_0.shape],\n","      back_prop=False\n","    )\n","    assert img_final.shape == shape\n","    return img_final\n","\n","  def p_sample_loop_trajectory(self, denoise_fn, *, shape, noise_fn=tf.random_normal, repeat_noise_steps=-1):\n","    \"\"\"\n","    Generate samples, returning intermediate images\n","    Useful for visualizing how denoised images evolve over time\n","    Args:\n","      repeat_noise_steps (int): Number of denoising timesteps in which the same noise\n","        is used across the batch. If >= 0, the initial noise is the same for all batch elemements.\n","    \"\"\"\n","    i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\n","    assert isinstance(shape, (tuple, list))\n","    img_0 = noise_like(shape, noise_fn, repeat_noise_steps >= 0)\n","    times = tf.Variable([i_0])\n","    imgs = tf.Variable([img_0])\n","    # Steps with repeated noise\n","    times, imgs = tf.while_loop(\n","      cond=lambda times_, _: tf.less_equal(self.num_timesteps - times_[-1], repeat_noise_steps),\n","      body=lambda times_, imgs_: [\n","        tf.concat([times_, [times_[-1] - 1]], 0),\n","        tf.concat([imgs_, [self.p_sample(denoise_fn=denoise_fn,\n","                                         x=imgs_[-1],\n","                                         t=tf.fill([shape[0]], times_[-1]),\n","                                         noise_fn=noise_fn,\n","                                         repeat_noise=True)]], 0)\n","      ],\n","      loop_vars=[times, imgs],\n","      shape_invariants=[tf.TensorShape([None, *i_0.shape]),\n","                        tf.TensorShape([None, *img_0.shape])],\n","      back_prop=False\n","    )\n","    # Steps with different noise for each batch element\n","    times, imgs = tf.while_loop(\n","      cond=lambda times_, _: tf.greater_equal(times_[-1], 0),\n","      body=lambda times_, imgs_: [\n","        tf.concat([times_, [times_[-1] - 1]], 0),\n","        tf.concat([imgs_, [self.p_sample(denoise_fn=denoise_fn,\n","                                         x=imgs_[-1],\n","                                         t=tf.fill([shape[0]], times_[-1]),\n","                                         noise_fn=noise_fn,\n","                                         repeat_noise=False)]], 0)\n","      ],\n","      loop_vars=[times, imgs],\n","      shape_invariants=[tf.TensorShape([None, *i_0.shape]),\n","                        tf.TensorShape([None, *img_0.shape])],\n","      back_prop=False\n","    )\n","    assert imgs[-1].shape == shape\n","    return times, imgs\n","\n","  def interpolate(self, denoise_fn, *, shape, noise_fn=tf.random_normal):\n","    \"\"\"\n","    Interpolate between images.\n","    t == 0 means diffuse images for 1 timestep before mixing.\n","    \"\"\"\n","    assert isinstance(shape, (tuple, list))\n","\n","    # Placeholders for real samples to interpolate\n","    x1 = tf.placeholder(tf.float32, shape)\n","    x2 = tf.placeholder(tf.float32, shape)\n","    # lam == 0.5 averages diffused images.\n","    lam = tf.placeholder(tf.float32, shape=())\n","    t = tf.placeholder(tf.int32, shape=())\n","\n","    # Add noise via forward diffusion\n","    # TODO: use the same noise for both endpoints?\n","    # t_batched = tf.constant([t] * x1.shape[0], dtype=tf.int32)\n","    t_batched = tf.stack([t] * x1.shape[0])\n","    xt1 = self.q_sample(x1, t=t_batched)\n","    xt2 = self.q_sample(x2, t=t_batched)\n","\n","    # Mix latents\n","    # Linear interpolation\n","    xt_interp = (1 - lam) * xt1 + lam * xt2\n","    # Constant variance interpolation\n","    # xt_interp = tf.sqrt(1 - lam * lam) * xt1 + lam * xt2\n","\n","    # Reverse diffusion (similar to self.p_sample_loop)\n","    # t = tf.constant(t, dtype=tf.int32)\n","    _, x_interp = tf.while_loop(\n","      cond=lambda i_, _: tf.greater_equal(i_, 0),\n","      body=lambda i_, img_: [\n","        i_ - 1,\n","        self.p_sample(denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn)\n","      ],\n","      loop_vars=[t, xt_interp],\n","      shape_invariants=[t.shape, xt_interp.shape],\n","      back_prop=False\n","    )\n","    assert x_interp.shape == shape\n","\n","    return x1, x2, lam, x_interp, t"]}]}