{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simple_eval_worker.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyc5vuYyEQqPx86ebxn1ti"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JFVINoCv67j9"},"outputs":[],"source":["\"\"\"\n","\"One-shot\" evaluation worker (i.e. run something once, not in a loop over the course of training)\n","\n","- Computes log prob\n","- Generates samples progressively\n","\"\"\"\n","\n","import os\n","import pickle\n","import time\n","\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","from tqdm import trange\n","\n","from .tpu_utils import Model, make_ema, distributed, normalize_data\n","from .. import utils\n","\n","\n","def _make_ds_iterator(strategy, ds):\n","  return strategy.experimental_distribute_dataset(ds).make_initializable_iterator()\n","\n","\n","class SimpleEvalWorker:\n","  def __init__(self, tpu_name, model_constructor, total_bs, dataset):\n","    tf.logging.set_verbosity(tf.logging.INFO)\n","\n","    self.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n","    tf.tpu.experimental.initialize_tpu_system(self.resolver)\n","    self.strategy = tf.distribute.experimental.TPUStrategy(self.resolver)\n","\n","    self.num_cores = self.strategy.num_replicas_in_sync\n","    assert total_bs % self.num_cores == 0\n","    self.total_bs = total_bs\n","    self.local_bs = total_bs // self.num_cores\n","    print('num cores: {}'.format(self.num_cores))\n","    print('total batch size: {}'.format(self.total_bs))\n","    print('local batch size: {}'.format(self.local_bs))\n","    self.dataset = dataset\n","\n","    # TPU context\n","    with self.strategy.scope():\n","      # Dataset iterators\n","      self.train_ds_iterator = _make_ds_iterator(\n","        self.strategy, dataset.train_one_pass_input_fn(params={'batch_size': total_bs}))\n","      self.eval_ds_iterator = _make_ds_iterator(\n","        self.strategy, dataset.eval_input_fn(params={'batch_size': total_bs}))\n","\n","      img_batch_shape = self.train_ds_iterator.output_shapes['image'].as_list()\n","      assert img_batch_shape[0] == self.local_bs\n","\n","      # Model\n","      self.model = model_constructor()\n","      assert isinstance(self.model, Model)\n","\n","      # Eval/samples graphs\n","      print('===== SAMPLES =====')\n","      self.samples_outputs = self._make_progressive_sampling_graph(img_shape=img_batch_shape[1:])\n","\n","      # Model with EMA parameters\n","      print('===== EMA =====')\n","      self.global_step = tf.train.get_or_create_global_step()\n","      ema, _ = make_ema(global_step=self.global_step, ema_decay=1e-10, trainable_variables=tf.trainable_variables())\n","\n","      # EMA versions of the above\n","      with utils.ema_scope(ema):\n","        print('===== EMA SAMPLES =====')\n","        self.ema_samples_outputs = self._make_progressive_sampling_graph(img_shape=img_batch_shape[1:])\n","        print('===== EMA BPD =====')\n","        self.bpd_train = self._make_bpd_graph(self.train_ds_iterator)\n","        self.bpd_eval = self._make_bpd_graph(self.eval_ds_iterator)\n","\n","  def _make_progressive_sampling_graph(self, img_shape):\n","    return distributed(\n","      lambda x_: self.model.progressive_samples_fn(\n","        x_, tf.random_uniform([self.local_bs], 0, self.dataset.num_classes, dtype=tf.int32)),\n","      args=(tf.fill([self.local_bs, *img_shape], value=np.nan),),\n","      reduction='concat', strategy=self.strategy)\n","\n","  def _make_bpd_graph(self, ds_iterator):\n","    return distributed(\n","      lambda x_: self.model.bpd_fn(normalize_data(tf.cast(x_['image'], tf.float32)), x_['label']),\n","      args=(next(ds_iterator),), reduction='concat', strategy=self.strategy)\n","\n","  def init_all_iterators(self, sess):\n","    sess.run([self.train_ds_iterator.initializer, self.eval_ds_iterator.initializer])\n","\n","  def dump_progressive_samples(self, sess, curr_step, samples_dir, ema: bool, num_samples=50000, batches_per_flush=20):\n","    if not tf.gfile.IsDirectory(samples_dir):\n","      tf.gfile.MakeDirs(samples_dir)\n","\n","    batch_cache, num_flushes_so_far = [], 0\n","\n","    def _write_batch_cache():\n","      nonlocal batch_cache, num_flushes_so_far\n","      # concat all the batches\n","      assert all(set(b.keys()) == set(self.samples_outputs.keys()) for b in batch_cache)\n","      concatenated = {\n","        k: np.concatenate([b[k].astype(np.float32) for b in batch_cache], axis=0)\n","        for k in self.samples_outputs.keys()\n","      }\n","      assert len(set(len(v) for v in concatenated.values())) == 1\n","      # write the file\n","      filename = os.path.join(\n","        samples_dir, 'samples_xstartpred_ema{}_step{:09d}_part{:06d}.pkl'.format(\n","          int(ema), curr_step, num_flushes_so_far))\n","      assert not tf.io.gfile.exists(filename), 'samples file already exists: {}'.format(filename)\n","      print('writing samples batch to:', filename)\n","      with tf.io.gfile.GFile(filename, 'wb') as f:\n","        f.write(pickle.dumps(concatenated, protocol=pickle.HIGHEST_PROTOCOL))\n","      print('done writing samples batch')\n","      num_flushes_so_far += 1\n","      batch_cache = []\n","\n","    num_gen_batches = int(np.ceil(num_samples / self.total_bs))\n","    print('generating {} samples ({} batches)...'.format(num_samples, num_gen_batches))\n","    self.init_all_iterators(sess)\n","    for i_batch in trange(num_gen_batches, desc='sampling'):\n","      batch_cache.append(sess.run(self.ema_samples_outputs if ema else self.samples_outputs))\n","      if i_batch != 0 and i_batch % batches_per_flush == 0:\n","        _write_batch_cache()\n","    if batch_cache:\n","      _write_batch_cache()\n","\n","  def dump_bpd(self, sess, curr_step, output_dir, train: bool, ema: bool):\n","    assert ema\n","    if not tf.gfile.IsDirectory(output_dir):\n","      tf.gfile.MakeDirs(output_dir)\n","    filename = os.path.join(\n","      output_dir, 'bpd_{}_ema{}_step{:09d}.pkl'.format('train' if train else 'eval', int(ema), curr_step))\n","    assert not tf.io.gfile.exists(filename), 'bpd file already exists: {}'.format(filename)\n","    print('will write bpd data to:', filename)\n","\n","    batches = []\n","    tf_op = self.bpd_train if train else self.bpd_eval\n","    self.init_all_iterators(sess)\n","    last_print_time = time.time()\n","    while True:\n","      try:\n","        batches.append(sess.run(tf_op))\n","        if time.time() - last_print_time > 30:\n","          print('num batches so far: {} ({:.2f} sec)'.format(len(batches), time.time() - last_print_time))\n","          last_print_time = time.time()\n","      except tf.errors.OutOfRangeError:\n","        break\n","\n","    assert all(set(b.keys()) == set(tf_op.keys()) for b in batches)\n","    concatenated = {\n","      k: np.concatenate([b[k].astype(np.float32) for b in batches], axis=0)\n","      for k in tf_op.keys()\n","    }\n","    num_samples = len(list(concatenated.values())[0])\n","    assert all(len(v) == num_samples for v in concatenated.values())\n","    print('evaluated on {} examples'.format(num_samples))\n","\n","    print('writing results to:', filename)\n","    with tf.io.gfile.GFile(filename, 'wb') as f:\n","      f.write(pickle.dumps(concatenated, protocol=pickle.HIGHEST_PROTOCOL))\n","    print('done writing results')\n","\n","  def run(self, mode: str, logdir: str, load_ckpt: str):\n","    \"\"\"\n","    Main entry point.\n","\n","    :param mode: what to do\n","    :param logdir: model directory for the checkpoint to load\n","    :param load_ckpt: the name of the checkpoint, e.g. \"model.ckpt-1000000\"\n","    \"\"\"\n","\n","    # Input checkpoint: load_ckpt should be of the form: model.ckpt-1000000\n","    ckpt = os.path.join(logdir, load_ckpt)\n","    assert tf.io.gfile.exists(ckpt + '.index')\n","\n","    # Output dir\n","    output_dir = os.path.join(logdir, 'simple_eval')\n","    print('Writing output to: {}'.format(output_dir))\n","\n","    # Make the session\n","    config = tf.ConfigProto()\n","    config.allow_soft_placement = True\n","    cluster_spec = self.resolver.cluster_spec()\n","    if cluster_spec:\n","      config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())\n","    print('making session...')\n","    with tf.Session(target=self.resolver.master(), config=config) as sess:\n","\n","      print('initializing global variables')\n","      sess.run(tf.global_variables_initializer())\n","\n","      # Checkpoint loading\n","      print('making saver')\n","      saver = tf.train.Saver()\n","      saver.restore(sess, ckpt)\n","      global_step_val = sess.run(self.global_step)\n","      print('restored global step: {}'.format(global_step_val))\n","\n","      if mode in ['bpd_train', 'bpd_eval']:\n","        self.dump_bpd(\n","          sess, curr_step=global_step_val, output_dir=os.path.join(output_dir, 'bpd'), ema=True,\n","          train=mode == 'bpd_train')\n","      elif mode == 'progressive_samples':\n","        return self.dump_progressive_samples(\n","          sess, curr_step=global_step_val, samples_dir=os.path.join(output_dir, 'progressive_samples'), ema=True)\n","      else:\n","        raise NotImplementedError(mode)"]}]}