{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tpu_summaries.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPXV4fyqAml1ZBUUKHPDZj3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg5ZU-wH7Gdx"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import collections\n","\n","from absl import logging\n","import tensorflow as tf\n","\n","\n","summary = tf.contrib.summary  # TensorFlow Summary API v2.\n","\n","\n","TpuSummaryEntry = collections.namedtuple(\n","    \"TpuSummaryEntry\", \"summary_fn name tensor reduce_fn\")\n","\n","\n","class TpuSummaries(object):\n","  \"\"\"Class to simplify TF summaries on TPU.\n","\n","  An instance of the class provides simple methods for writing summaries in the\n","  similar way to tf.summary. The difference is that each summary entry must\n","  provide a reduction function that is used to reduce the summary values from\n","  all the TPU cores.\n","  \"\"\"\n","\n","  def __init__(self, log_dir, save_summary_steps=250):\n","    self._log_dir = log_dir\n","    self._entries = []\n","    # While False no summary entries will be added. On TPU we unroll the graph\n","    # and don't want to add multiple summaries per step.\n","    self.record = True\n","    self._save_summary_steps = save_summary_steps\n","\n","  def image(self, name, tensor, reduce_fn):\n","    \"\"\"Add a summary for images. Tensor must be of 4-D tensor.\"\"\"\n","    if not self.record:\n","      return\n","    self._entries.append(\n","        TpuSummaryEntry(summary.image, name, tensor, reduce_fn))\n","\n","  def scalar(self, name, tensor, reduce_fn=tf.math.reduce_mean):\n","    \"\"\"Add a summary for a scalar tensor.\"\"\"\n","    if not self.record:\n","      return\n","    tensor = tf.convert_to_tensor(tensor)\n","    if tensor.shape.ndims == 0:\n","      tensor = tf.expand_dims(tensor, 0)\n","    self._entries.append(\n","        TpuSummaryEntry(summary.scalar, name, tensor, reduce_fn))\n","\n","  def get_host_call(self):\n","    \"\"\"Returns the tuple (host_call_fn, host_call_args) for TPUEstimatorSpec.\"\"\"\n","    # All host_call_args must be tensors with batch dimension.\n","    # All tensors are streamed to the host machine (mind the band width).\n","    global_step = tf.train.get_or_create_global_step()\n","    host_call_args = [tf.expand_dims(global_step, 0)]\n","    host_call_args.extend([e.tensor for e in self._entries])\n","    logging.info(\"host_call_args: %s\", host_call_args)\n","    return (self._host_call_fn, host_call_args)\n","\n","  def _host_call_fn(self, step, *args):\n","    \"\"\"Function that will run on the host machine.\"\"\"\n","    # Host call receives values from all tensor cores (concatenate on the\n","    # batch dimension). Step is the same for all cores.\n","    step = step[0]\n","    logging.info(\"host_call_fn: args=%s\", args)\n","    with summary.create_file_writer(self._log_dir).as_default():\n","      with summary.record_summaries_every_n_global_steps(\n","          self._save_summary_steps, step):\n","        for i, e in enumerate(self._entries):\n","          value = e.reduce_fn(args[i])\n","          e.summary_fn(e.name, value, step=step)\n","        return summary.all_summary_ops()"]}]}