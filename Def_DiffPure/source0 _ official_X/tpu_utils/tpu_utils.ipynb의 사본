{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tpu_utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkhXs81HvfYGuBmlvP+6x0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PbWle0rL7N9A"},"outputs":[],"source":["import json\n","import os\n","import pickle\n","import time\n","from datetime import datetime\n","\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","import tensorflow_gan as tfgan\n","from tensorflow.contrib.tpu.python.ops import tpu_ops\n","from tensorflow.python.tpu import tpu_function\n","from tqdm import trange\n","\n","from . import classifier_metrics_numpy\n","from .tpu_summaries import TpuSummaries\n","from .. import utils\n","\n","\n","# ========== TPU utilities ==========\n","\n","def num_tpu_replicas():\n","  return tpu_function.get_tpu_context().number_of_shards\n","\n","\n","def get_tpu_replica_id():\n","  with tf.control_dependencies(None):\n","    return tpu_ops.tpu_replicated_input(list(range(num_tpu_replicas())))\n","\n","\n","def distributed(fn, *, args, reduction, strategy):\n","  \"\"\"\n","  Sharded computation followed by concat/mean for TPUStrategy.\n","  \"\"\"\n","  out = strategy.experimental_run_v2(fn, args=args)\n","  if reduction == 'mean':\n","    return tf.nest.map_structure(lambda x: tf.reduce_mean(strategy.reduce('mean', x)), out)\n","  elif reduction == 'concat':\n","    return tf.nest.map_structure(lambda x: tf.concat(strategy.experimental_local_results(x), axis=0), out)\n","  else:\n","    raise NotImplementedError(reduction)\n","\n","\n","# ========== Inception utilities ==========\n","\n","INCEPTION_URL = 'http://download.tensorflow.org/models/frozen_inception_v1_2015_12_05_v4.tar.gz'\n","INCEPTION_FROZEN_GRAPH = 'inceptionv1_for_inception_score_tpu.pb'\n","INCEPTION_GRAPH_DEF = tfgan.eval.get_graph_def_from_url_tarball(\n","  INCEPTION_URL, INCEPTION_FROZEN_GRAPH, os.path.basename(INCEPTION_URL))\n","\n","\n","def run_inception(images):\n","  assert images.dtype == tf.float32  # images should be in [-1, 1]\n","  out = tfgan.eval.run_inception(\n","    images,\n","    graph_def=INCEPTION_GRAPH_DEF,\n","    default_graph_def_fn=None,\n","    output_tensor=['pool_3:0', 'logits:0']\n","  )\n","  return {'pool_3': out[0], 'logits': out[1]}\n","\n","\n","# ========== Training ==========\n","\n","normalize_data = lambda x_: x_ / 127.5 - 1.\n","unnormalize_data = lambda x_: (x_ + 1.) * 127.5\n","\n","\n","class Model:\n","  # All images (inputs and outputs) should be normalized to [-1, 1]\n","  def train_fn(self, x, y) -> dict:\n","    raise NotImplementedError\n","\n","  def samples_fn(self, dummy_x, y) -> dict:\n","    raise NotImplementedError\n","\n","  def sample_and_run_inception(self, dummy_x, y, clip_samples=True):\n","    samples_dict = self.samples_fn(dummy_x, y)\n","    assert isinstance(samples_dict, dict)\n","    return {\n","      k: run_inception(tfgan.eval.preprocess_image(unnormalize_data(\n","        tf.clip_by_value(v, -1., 1.) if clip_samples else v)))\n","      for (k, v) in samples_dict.items()\n","    }\n","\n","  def bpd_fn(self, x, y) -> dict:\n","    return None\n","\n","\n","def make_ema(global_step, ema_decay, trainable_variables):\n","  ema = tf.train.ExponentialMovingAverage(decay=tf.where(tf.less(global_step, 1), 1e-10, ema_decay))\n","  ema_op = ema.apply(trainable_variables)\n","  return ema, ema_op\n","\n","\n","def load_train_kwargs(model_dir):\n","  with tf.io.gfile.GFile(os.path.join(model_dir, 'kwargs.json'), 'r') as f:\n","    kwargs = json.loads(f.read())\n","  return kwargs\n","\n","\n","def run_training(\n","    *, model_constructor, train_input_fn, total_bs,\n","    optimizer, lr, warmup, grad_clip, ema_decay=0.9999,\n","    tpu=None, zone=None, project=None,\n","    log_dir, exp_name, dump_kwargs=None,\n","    date_str=None, iterations_per_loop=1000, keep_checkpoint_max=2, max_steps=int(1e10),\n","    warm_start_from=None\n","):\n","  tf.logging.set_verbosity(tf.logging.INFO)\n","\n","  # Create checkpoint directory\n","  model_dir = os.path.join(\n","    log_dir,\n","    datetime.now().strftime('%Y-%m-%d') if date_str is None else date_str,\n","    exp_name\n","  )\n","  print('model dir:', model_dir)\n","  if tf.io.gfile.exists(model_dir):\n","    print('model dir already exists: {}'.format(model_dir))\n","    if input('continue training? [y/n] ') != 'y':\n","      print('aborting')\n","      return\n","\n","  # Save kwargs in json format\n","  if dump_kwargs is not None:\n","    with tf.io.gfile.GFile(os.path.join(model_dir, 'kwargs.json'), 'w') as f:\n","      f.write(json.dumps(dump_kwargs, indent=2, sort_keys=True) + '\\n')\n","\n","  # model_fn for TPUEstimator\n","  def model_fn(features, params, mode):\n","    local_bs = params['batch_size']\n","    print('Global batch size: {}, local batch size: {}'.format(total_bs, local_bs))\n","    assert total_bs == num_tpu_replicas() * local_bs\n","\n","    assert mode == tf.estimator.ModeKeys.TRAIN, 'only TRAIN mode supported'\n","    assert features['image'].shape[0] == local_bs\n","    assert features['label'].shape == [local_bs] and features['label'].dtype == tf.int32\n","    # assert labels.dtype == features['label'].dtype and labels.shape == features['label'].shape\n","\n","    del params\n","\n","    ##########\n","\n","    # create model\n","    model = model_constructor()\n","    assert isinstance(model, Model)\n","\n","    # training loss\n","    train_info_dict = model.train_fn(normalize_data(tf.cast(features['image'], tf.float32)), features['label'])\n","    loss = train_info_dict['loss']\n","    assert loss.shape == []\n","\n","    # train op\n","    trainable_variables = tf.trainable_variables()\n","    print('num params: {:,}'.format(sum(int(np.prod(p.shape.as_list())) for p in trainable_variables)))\n","    global_step = tf.train.get_or_create_global_step()\n","    warmed_up_lr = utils.get_warmed_up_lr(max_lr=lr, warmup=warmup, global_step=global_step)\n","    train_op, gnorm = utils.make_optimizer(\n","      loss=loss,\n","      trainable_variables=trainable_variables,\n","      global_step=global_step,\n","      lr=warmed_up_lr,\n","      optimizer=optimizer,\n","      grad_clip=grad_clip / float(num_tpu_replicas()),\n","      tpu=True\n","    )\n","\n","    # ema\n","    ema, ema_op = make_ema(global_step=global_step, ema_decay=ema_decay, trainable_variables=trainable_variables)\n","    with tf.control_dependencies([train_op]):\n","      train_op = tf.group(ema_op)\n","\n","    # summary\n","    tpu_summary = TpuSummaries(model_dir, save_summary_steps=100)\n","    tpu_summary.scalar('train/loss', loss)\n","    tpu_summary.scalar('train/gnorm', gnorm)\n","    tpu_summary.scalar('train/pnorm', utils.rms(trainable_variables))\n","    tpu_summary.scalar('train/lr', warmed_up_lr)\n","    return tf.estimator.tpu.TPUEstimatorSpec(\n","      mode=mode, host_call=tpu_summary.get_host_call(), loss=loss, train_op=train_op)\n","\n","  # Set up Estimator and train\n","  print(\"warm_start_from:\", warm_start_from)\n","  estimator = tf.estimator.tpu.TPUEstimator(\n","    model_fn=model_fn,\n","    use_tpu=True,\n","    train_batch_size=total_bs,\n","    eval_batch_size=total_bs,\n","    config=tf.estimator.tpu.RunConfig(\n","      cluster=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu, zone=zone, project=project),\n","      model_dir=model_dir,\n","      session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),\n","      tpu_config=tf.estimator.tpu.TPUConfig(\n","        iterations_per_loop=iterations_per_loop,\n","        num_shards=None,\n","        per_host_input_for_training=tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n","      ),\n","      save_checkpoints_secs=1600,  # 30 minutes\n","      keep_checkpoint_max=keep_checkpoint_max\n","    ),\n","    warm_start_from=warm_start_from\n","  )\n","  estimator.train(input_fn=train_input_fn, max_steps=max_steps)\n","\n","\n","# ========== Evaluation / sampling ==========\n","\n","\n","class InceptionFeatures:\n","  \"\"\"\n","  Compute and store Inception features for a dataset\n","  \"\"\"\n","\n","  def __init__(self, dataset, strategy, limit_dataset_size=0):\n","    # distributed dataset iterator\n","    if limit_dataset_size > 0:\n","      dataset = dataset.take(limit_dataset_size)\n","    self.ds_iterator = strategy.experimental_distribute_dataset(dataset).make_initializable_iterator()\n","\n","    # inception network on the dataset\n","    self.inception_real = distributed(\n","      lambda x_: run_inception(tfgan.eval.preprocess_image(x_['image'])),\n","      args=(next(self.ds_iterator),), reduction='concat', strategy=strategy)\n","\n","    self.cached_inception_real = None  # cached inception features\n","    self.real_inception_score = None  # saved inception scores for the dataset\n","\n","  def get(self, sess):\n","    # On the first invocation, compute Inception activations for the eval dataset\n","    if self.cached_inception_real is None:\n","      print('computing inception features on the eval set...')\n","      sess.run(self.ds_iterator.initializer)  # reset the eval dataset iterator\n","      inception_real_batches, tstart = [], time.time()\n","      while True:\n","        try:\n","          inception_real_batches.append(sess.run(self.inception_real))\n","        except tf.errors.OutOfRangeError:\n","          break\n","      self.cached_inception_real = {\n","        feat_key: np.concatenate([batch[feat_key] for batch in inception_real_batches], axis=0).astype(np.float64)\n","        for feat_key in ['pool_3', 'logits']\n","      }\n","      print('cached eval inception tensors: logits: {}, pool_3: {} (time: {})'.format(\n","        self.cached_inception_real['logits'].shape, self.cached_inception_real['pool_3'].shape,\n","        time.time() - tstart))\n","\n","      self.real_inception_score = float(\n","        classifier_metrics_numpy.classifier_score_from_logits(self.cached_inception_real['logits']))\n","      del self.cached_inception_real['logits']  # save memory\n","    print('real inception score', self.real_inception_score)\n","\n","    return self.cached_inception_real, self.real_inception_score\n","\n","\n","class EvalWorker:\n","  def __init__(self, tpu_name, model_constructor, total_bs, dataset, inception_bs=8, num_inception_samples=1024, limit_dataset_size=0):\n","\n","    self.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n","    tf.tpu.experimental.initialize_tpu_system(self.resolver)\n","    self.strategy = tf.distribute.experimental.TPUStrategy(self.resolver)\n","\n","    self.num_cores = self.strategy.num_replicas_in_sync\n","    assert total_bs % self.num_cores == 0\n","    self.total_bs = total_bs\n","    self.local_bs = total_bs // self.num_cores\n","    print('num cores: {}'.format(self.num_cores))\n","    print('total batch size: {}'.format(self.total_bs))\n","    print('local batch size: {}'.format(self.local_bs))\n","    self.num_inception_samples = num_inception_samples\n","    assert inception_bs % self.num_cores == 0\n","    self.inception_bs = inception_bs\n","    self.inception_local_bs = inception_bs // self.num_cores\n","    self.dataset = dataset\n","    assert dataset.num_classes == 1, 'not supported'\n","\n","    # TPU context\n","    with self.strategy.scope():\n","      # Inception network on real data\n","      print('===== INCEPTION =====')\n","      # Eval dataset iterator (this is the training set without repeat & shuffling)\n","      self.inception_real_train = InceptionFeatures(\n","        dataset=dataset.train_one_pass_input_fn(params={'batch_size': total_bs}), strategy=self.strategy, limit_dataset_size=limit_dataset_size // total_bs)\n","      # Val dataset, if it exists\n","      val_ds = dataset.eval_input_fn(params={'batch_size': total_bs})\n","      self.inception_real_val = None if val_ds is None else InceptionFeatures(dataset=val_ds, strategy=self.strategy, limit_dataset_size=limit_dataset_size // total_bs)\n","\n","      img_batch_shape = self.inception_real_train.ds_iterator.output_shapes['image'].as_list()\n","      assert img_batch_shape[0] == self.local_bs\n","\n","      # Model\n","      self.model = model_constructor()\n","      assert isinstance(self.model, Model)\n","\n","      # Eval/samples graphs\n","      print('===== SAMPLES =====')\n","      self.samples_outputs, self.samples_inception = self._make_sampling_graph(\n","        img_shape=img_batch_shape[1:], with_inception=True)\n","\n","      # Model with EMA parameters\n","      self.global_step = tf.train.get_or_create_global_step()\n","      print('===== EMA =====')\n","      ema, _ = make_ema(global_step=self.global_step, ema_decay=1e-10, trainable_variables=tf.trainable_variables())\n","\n","      # EMA versions of the above\n","      with utils.ema_scope(ema):\n","        print('===== EMA SAMPLES =====')\n","        self.ema_samples_outputs, self.ema_samples_inception = self._make_sampling_graph(\n","          img_shape=img_batch_shape[1:], with_inception=True)\n","\n","  def _make_sampling_graph(self, img_shape, with_inception):\n","\n","    def _make_inputs(total_bs, local_bs):\n","      # Dummy inputs to feed to samplers\n","      input_x = tf.fill([local_bs, *img_shape], value=np.nan)\n","      input_y = tf.random_uniform([local_bs], 0, self.dataset.num_classes, dtype=tf.int32)\n","      return input_x, input_y\n","\n","    # Samples\n","    samples_outputs = distributed(\n","      self.model.samples_fn,\n","      args=_make_inputs(self.total_bs, self.local_bs),\n","      reduction='concat', strategy=self.strategy)\n","    if not with_inception:\n","      return samples_outputs\n","\n","    # Inception activations of samples\n","    samples_inception = distributed(\n","      self.model.sample_and_run_inception,\n","      args=_make_inputs(self.inception_bs, self.inception_local_bs),\n","      reduction='concat', strategy=self.strategy)\n","    return samples_outputs, samples_inception\n","\n","  def _run_sampling(self, sess, ema: bool):\n","    out = {}\n","    print('sampling...')\n","    tstart = time.time()\n","    samples = sess.run(self.ema_samples_outputs if ema else self.samples_outputs)\n","    print('sampling done in {} sec'.format(time.time() - tstart))\n","    for k, v in samples.items():\n","      out['samples/{}'.format(k)] = v\n","    return out\n","\n","  def _run_metrics(self, sess, ema: bool):\n","    print('computing sample quality metrics...')\n","    metrics = {}\n","\n","    # Get Inception activations on the real dataset\n","    cached_inception_real_train, metrics['real_inception_score_train'] = self.inception_real_train.get(sess)\n","    if self.inception_real_val is not None:\n","      cached_inception_real_val, metrics['real_inception_score'] = self.inception_real_val.get(sess)\n","    else:\n","      cached_inception_real_val = None\n","\n","    # Generate lots of samples\n","    num_inception_gen_batches = int(np.ceil(self.num_inception_samples / self.inception_bs))\n","    print('generating {} samples and inception features ({} batches)...'.format(\n","      self.num_inception_samples, num_inception_gen_batches))\n","    inception_gen_batches = [\n","      sess.run(self.ema_samples_inception if ema else self.samples_inception)\n","      for _ in trange(num_inception_gen_batches, desc='sampling inception batch')\n","    ]\n","\n","    # Compute FID and Inception score\n","    assert set(self.samples_outputs.keys()) == set(inception_gen_batches[0].keys())\n","    for samples_key in self.samples_outputs.keys():\n","      # concat features from all batches into a single array\n","      inception_gen = {\n","        feat_key: np.concatenate(\n","          [batch[samples_key][feat_key] for batch in inception_gen_batches], axis=0\n","        )[:self.num_inception_samples].astype(np.float64)\n","        for feat_key in ['pool_3', 'logits']\n","      }\n","      assert all(v.shape[0] == self.num_inception_samples for v in inception_gen.values())\n","\n","      # Inception score\n","      metrics['{}/inception{}'.format(samples_key, self.num_inception_samples)] = float(\n","        classifier_metrics_numpy.classifier_score_from_logits(inception_gen['logits']))\n","\n","      # FID vs training set\n","      metrics['{}/trainfid{}'.format(samples_key, self.num_inception_samples)] = float(\n","        classifier_metrics_numpy.frechet_classifier_distance_from_activations(\n","          cached_inception_real_train['pool_3'], inception_gen['pool_3']))\n","\n","      # FID vs val set\n","      if cached_inception_real_val is not None:\n","        metrics['{}/fid{}'.format(samples_key, self.num_inception_samples)] = float(\n","          classifier_metrics_numpy.frechet_classifier_distance_from_activations(\n","            cached_inception_real_val['pool_3'], inception_gen['pool_3']))\n","\n","    return metrics\n","\n","  def _write_eval_and_samples(self, sess, log: utils.SummaryWriter, curr_step, prefix, ema: bool):\n","    # Samples\n","    for k, v in self._run_sampling(sess, ema=ema).items():\n","      assert len(v.shape) == 4 and v.shape[0] == self.total_bs\n","      log.images('{}/{}'.format(prefix, k), np.clip(unnormalize_data(v), 0, 255).astype('uint8'), step=curr_step)\n","    log.flush()\n","\n","    # Metrics\n","    metrics = self._run_metrics(sess, ema=ema)\n","    print('metrics:', json.dumps(metrics, indent=2, sort_keys=True))\n","    for k, v in metrics.items():\n","      log.scalar('{}/{}'.format(prefix, k), v, step=curr_step)\n","    log.flush()\n","\n","  def _dump_samples(self, sess, curr_step, samples_dir, ema: bool, num_samples=50000):\n","    print('will dump samples to', samples_dir)\n","    if not tf.gfile.IsDirectory(samples_dir):\n","      tf.gfile.MakeDirs(samples_dir)\n","    filename = os.path.join(\n","      samples_dir, 'samples_ema{}_step{:09d}.pkl'.format(int(ema), curr_step))\n","    assert not tf.io.gfile.exists(filename), 'samples file already exists: {}'.format(filename)\n","\n","    num_gen_batches = int(np.ceil(num_samples / self.total_bs))\n","    print('generating {} samples ({} batches)...'.format(num_samples, num_gen_batches))\n","\n","    # gen_batches = [\n","    #   sess.run(self.ema_samples_outputs if ema else self.samples_outputs)\n","    #   for _ in trange(num_gen_batches, desc='sampling')\n","    # ]\n","    # assert all(set(b.keys()) == set(self.samples_outputs.keys()) for b in gen_batches)\n","    # concatenated = {\n","    #   k: np.concatenate([b[k].astype(np.float32) for b in gen_batches], axis=0)[:num_samples]\n","    #   for k in self.samples_outputs.keys()\n","    # }\n","    # assert all(len(v) == num_samples for v in concatenated.values())\n","    #\n","    # print('writing samples to:', filename)\n","    # with tf.io.gfile.GFile(filename, 'wb') as f:\n","    #   f.write(pickle.dumps(concatenated, protocol=pickle.HIGHEST_PROTOCOL))\n","\n","    for i in trange(num_gen_batches, desc='sampling'):\n","        b = sess.run(self.ema_samples_outputs if ema else self.samples_outputs)\n","        assert set(b.keys()) == set(self.samples_outputs.keys())\n","        b = {\n","          k: b[k].astype(np.float32) for k in self.samples_outputs.keys()\n","        }\n","        #assert all(len(v) == num_samples for v in concatenated.values())\n","\n","        filename_i = \"{}.batch{:05d}\".format(filename, i)\n","        print('writing samples for batch', i, 'to:', filename_i)\n","        with tf.io.gfile.GFile(filename_i, 'wb') as f:\n","            f.write(pickle.dumps(b, protocol=pickle.HIGHEST_PROTOCOL))\n","    print('done writing samples')\n","\n","  def run(self, logdir, once: bool, skip_non_ema_pass=True, dump_samples_only=False, load_ckpt=None, samples_dir=None, seed=0):\n","    \"\"\"Runs the eval/sampling worker loop.\n","    Args:\n","      logdir: directory to read checkpoints from\n","      once: if True, writes results to a temporary directory (not to logdir),\n","        and exits after evaluating one checkpoint.\n","    \"\"\"\n","    tf.logging.set_verbosity(tf.logging.INFO)\n","\n","    # Are we evaluating a single checkpoint or looping on the latest?\n","    if load_ckpt is not None:\n","      # load_ckpt should be of the form: model.ckpt-1000000\n","      assert tf.io.gfile.exists(os.path.join(logdir, load_ckpt) + '.index')\n","      ckpt_iterator = [os.path.join(logdir, load_ckpt)]  # load this one checkpoint only\n","    else:\n","      ckpt_iterator = tf.train.checkpoints_iterator(logdir)  # wait for checkpoints to come in\n","    assert tf.io.gfile.isdir(logdir), 'expected {} to be a directory'.format(logdir)\n","\n","    # Set up eval SummaryWriter\n","    if once:\n","      eval_logdir = os.path.join(logdir, 'eval_once_{}'.format(time.time()))\n","    else:\n","      eval_logdir = os.path.join(logdir, 'eval')\n","    print('Writing eval data to: {}'.format(eval_logdir))\n","    eval_log = utils.SummaryWriter(eval_logdir, write_graph=False)\n","\n","    # Make the session\n","    config = tf.ConfigProto()\n","    config.allow_soft_placement = True\n","    cluster_spec = self.resolver.cluster_spec()\n","    if cluster_spec:\n","      config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())\n","    print('making session...')\n","    with tf.Session(target=self.resolver.master(), config=config) as sess:\n","\n","      print('initializing global variables')\n","      sess.run(tf.global_variables_initializer())\n","\n","      # Checkpoint loading\n","      print('making saver')\n","      saver = tf.train.Saver()\n","\n","      for ckpt in ckpt_iterator:\n","        # Restore params\n","        saver.restore(sess, ckpt)\n","        global_step_val = sess.run(self.global_step)\n","        print('restored global step: {}'.format(global_step_val))\n","\n","        print('seeding')\n","        utils.seed_all(seed)\n","\n","        print('ema pass')\n","        if dump_samples_only:\n","          if not samples_dir:\n","            samples_dir = os.path.join(eval_logdir, '{}_samples{}'.format(type(self.dataset).__name__, global_step_val))\n","          self._dump_samples(\n","            sess, curr_step=global_step_val, samples_dir=samples_dir, ema=True)\n","        else:\n","          self._write_eval_and_samples(sess, log=eval_log, curr_step=global_step_val, prefix='eval_ema', ema=True)\n","\n","        if not skip_non_ema_pass:\n","          print('non-ema pass')\n","          if dump_samples_only:\n","            self._dump_samples(\n","              sess, curr_step=global_step_val, samples_dir=os.path.join(eval_logdir, 'samples'), ema=False)\n","          else:\n","            self._write_eval_and_samples(sess, log=eval_log, curr_step=global_step_val, prefix='eval', ema=False)\n","\n","        if once:\n","          break"]}]}