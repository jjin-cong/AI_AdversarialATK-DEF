{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"datasets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0Kt8Y1nNosHuD5EROZP4W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mdm0auqC6x1G"},"outputs":[],"source":["\"\"\"Dataset loading utilities.\n","\n","All images are scaled to [0, 255] instead of [0, 1]\n","\"\"\"\n","\n","import functools\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","\n","def pack(image, label):\n","  label = tf.cast(label, tf.int32)\n","  return {'image': image, 'label': label}\n","\n","\n","class SimpleDataset:\n","  DATASET_NAMES = ('cifar10', 'celebahq256')\n","\n","  def __init__(self, name, tfds_data_dir):\n","    self._name = name\n","    self._data_dir = tfds_data_dir\n","    self._img_size = {'cifar10': 32, 'celebahq256': 256}[name]\n","    self._img_shape = [self._img_size, self._img_size, 3]\n","    self._tfds_name = {\n","      'cifar10': 'cifar10:3.0.0',\n","      'celebahq256': 'celeb_a_hq/256:2.0.0',\n","    }[name]\n","    self.num_train_examples, self.num_eval_examples = {\n","      'cifar10': (50000, 10000),\n","      'celebahq256': (30000, 0),\n","    }[name]\n","    self.num_classes = 1  # unconditional\n","    self.eval_split_name = {\n","      'cifar10': 'test',\n","      'celebahq256': None,\n","    }[name]\n","\n","  @property\n","  def image_shape(self):\n","    \"\"\"Returns a tuple with the image shape.\"\"\"\n","    return tuple(self._img_shape)\n","\n","  def _proc_and_batch(self, ds, batch_size):\n","    def _process_data(x_):\n","      img_ = tf.cast(x_['image'], tf.int32)\n","      img_.set_shape(self._img_shape)\n","      return pack(image=img_, label=tf.constant(0, dtype=tf.int32))\n","\n","    ds = ds.map(_process_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    ds = ds.batch(batch_size, drop_remainder=True)\n","    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n","    return ds\n","\n","  def train_input_fn(self, params):\n","    ds = tfds.load(self._tfds_name, split='train', shuffle_files=True, data_dir=self._data_dir)\n","    ds = ds.repeat()\n","    ds = ds.shuffle(50000)\n","    return self._proc_and_batch(ds, params['batch_size'])\n","\n","  def train_one_pass_input_fn(self, params):\n","    ds = tfds.load(self._tfds_name, split='train', shuffle_files=False, data_dir=self._data_dir)\n","    return self._proc_and_batch(ds, params['batch_size'])\n","\n","  def eval_input_fn(self, params):\n","    if self.eval_split_name is None:\n","      return None\n","    ds = tfds.load(self._tfds_name, split=self.eval_split_name, shuffle_files=False, data_dir=self._data_dir)\n","    return self._proc_and_batch(ds, params['batch_size'])\n","\n","\n","class LsunDataset:\n","  def __init__(self,\n","    tfr_file,            # Path to tfrecord file.\n","    resolution=256,      # Dataset resolution.\n","    max_images=None,     # Maximum number of images to use, None = use all images.\n","    shuffle_mb=4096,     # Shuffle data within specified window (megabytes), 0 = disable shuffling.\n","    buffer_mb=256,       # Read buffer size (megabytes).\n","  ):\n","    \"\"\"Adapted from https://github.com/NVlabs/stylegan2/blob/master/training/dataset.py.\n","    Use StyleGAN2 dataset_tool.py to generate tf record files.\n","    \"\"\"\n","    self.tfr_file           = tfr_file\n","    self.dtype              = 'int32'\n","    self.max_images         = max_images\n","    self.buffer_mb          = buffer_mb\n","    self.num_classes        = 1         # unconditional\n","\n","    # Determine shape and resolution.\n","    self.resolution = resolution\n","    self.resolution_log2 = int(np.log2(self.resolution))\n","    self.image_shape = [self.resolution, self.resolution, 3]\n","\n","  def _train_input_fn(self, params, one_pass: bool):\n","    # Build TF expressions.\n","    dset = tf.data.TFRecordDataset(self.tfr_file, compression_type='', buffer_size=self.buffer_mb<<20)\n","    if self.max_images is not None:\n","      dset = dset.take(self.max_images)\n","    if not one_pass:\n","      dset = dset.repeat()\n","    dset = dset.map(self._parse_tfrecord_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    # Shuffle and prefetch\n","    dset = dset.shuffle(50000)\n","    dset = dset.batch(params['batch_size'], drop_remainder=True)\n","    dset = dset.prefetch(tf.data.experimental.AUTOTUNE)\n","    return dset\n","\n","  def train_input_fn(self, params):\n","    return self._train_input_fn(params, one_pass=False)\n","\n","  def train_one_pass_input_fn(self, params):\n","    return self._train_input_fn(params, one_pass=True)\n","\n","  def eval_input_fn(self, params):\n","    return None\n","\n","  # Parse individual image from a tfrecords file into TensorFlow expression.\n","  def _parse_tfrecord_tf(self, record):\n","    features = tf.parse_single_example(record, features={\n","      'shape': tf.FixedLenFeature([3], tf.int64),\n","      'data': tf.FixedLenFeature([], tf.string)})\n","    data = tf.decode_raw(features['data'], tf.uint8)\n","    data = tf.cast(data, tf.int32)\n","    data = tf.reshape(data, features['shape'])\n","    data = tf.transpose(data, [1, 2, 0])  # CHW -> HWC\n","    data.set_shape(self.image_shape)\n","    return pack(image=data, label=tf.constant(0, dtype=tf.int32))\n","\n","\n","DATASETS = {\n","  \"cifar10\": functools.partial(SimpleDataset, name=\"cifar10\"),\n","  \"celebahq256\": functools.partial(SimpleDataset, name=\"celebahq256\"),\n","  \"lsun\": LsunDataset,\n","}\n","\n","\n","def get_dataset(name, *, tfds_data_dir=None, tfr_file=None, seed=547):\n","  \"\"\"Instantiates a data set and sets the random seed.\"\"\"\n","  if name not in DATASETS:\n","    raise ValueError(\"Dataset %s is not available.\" % name)\n","  kwargs = {}\n","\n","  if name == 'lsun':\n","    # LsunDataset takes the path to the tf record, not a directory\n","    assert tfr_file is not None\n","    kwargs['tfr_file'] = tfr_file\n","  else:\n","    kwargs['tfds_data_dir'] = tfds_data_dir\n","\n","  if name not in ['lsun', *SimpleDataset.DATASET_NAMES]:\n","    kwargs['seed'] = seed\n","\n","  return DATASETS[name](**kwargs)"]}]}