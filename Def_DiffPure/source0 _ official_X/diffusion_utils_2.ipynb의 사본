{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"diffusion_utils_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjLFc74hAwqa2TZna1jZP+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UWV4L5yQ7gHL"},"outputs":[],"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","\n","from . import nn\n","from . import utils\n","\n","\n","def normal_kl(mean1, logvar1, mean2, logvar2):\n","  \"\"\"\n","  KL divergence between normal distributions parameterized by mean and log-variance.\n","  \"\"\"\n","  return 0.5 * (-1.0 + logvar2 - logvar1 + tf.exp(logvar1 - logvar2)\n","                + tf.squared_difference(mean1, mean2) * tf.exp(-logvar2))\n","\n","\n","def _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):\n","  betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","  warmup_time = int(num_diffusion_timesteps * warmup_frac)\n","  betas[:warmup_time] = np.linspace(beta_start, beta_end, warmup_time, dtype=np.float64)\n","  return betas\n","\n","\n","def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","  if beta_schedule == 'quad':\n","    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n","  elif beta_schedule == 'linear':\n","    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n","  elif beta_schedule == 'warmup10':\n","    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n","  elif beta_schedule == 'warmup50':\n","    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n","  elif beta_schedule == 'const':\n","    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","  elif beta_schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","    betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n","  else:\n","    raise NotImplementedError(beta_schedule)\n","  assert betas.shape == (num_diffusion_timesteps,)\n","  return betas\n","\n","\n","class GaussianDiffusion2:\n","  \"\"\"\n","  Contains utilities for the diffusion model.\n","\n","  Arguments:\n","  - what the network predicts (x_{t-1}, x_0, or epsilon)\n","  - which loss function (kl or unweighted MSE)\n","  - what is the variance of p(x_{t-1}|x_t) (learned, fixed to beta, or fixed to weighted beta)\n","  - what type of decoder, and how to weight its loss? is its variance learned too?\n","  \"\"\"\n","\n","  def __init__(self, *, betas, model_mean_type, model_var_type, loss_type):\n","    self.model_mean_type = model_mean_type  # xprev, xstart, eps\n","    self.model_var_type = model_var_type  # learned, fixedsmall, fixedlarge\n","    self.loss_type = loss_type  # kl, mse\n","\n","    assert isinstance(betas, np.ndarray)\n","    self.betas = betas = betas.astype(np.float64)  # computations here in float64 for accuracy\n","    assert (betas > 0).all() and (betas <= 1).all()\n","    timesteps, = betas.shape\n","    self.num_timesteps = int(timesteps)\n","\n","    alphas = 1. - betas\n","    self.alphas_cumprod = np.cumprod(alphas, axis=0)\n","    self.alphas_cumprod_prev = np.append(1., self.alphas_cumprod[:-1])\n","    assert self.alphas_cumprod_prev.shape == (timesteps,)\n","\n","    # calculations for diffusion q(x_t | x_{t-1}) and others\n","    self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n","    self.sqrt_one_minus_alphas_cumprod = np.sqrt(1. - self.alphas_cumprod)\n","    self.log_one_minus_alphas_cumprod = np.log(1. - self.alphas_cumprod)\n","    self.sqrt_recip_alphas_cumprod = np.sqrt(1. / self.alphas_cumprod)\n","    self.sqrt_recipm1_alphas_cumprod = np.sqrt(1. / self.alphas_cumprod - 1)\n","\n","    # calculations for posterior q(x_{t-1} | x_t, x_0)\n","    self.posterior_variance = betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n","    # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n","    self.posterior_log_variance_clipped = np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:]))\n","    self.posterior_mean_coef1 = betas * np.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n","    self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * np.sqrt(alphas) / (1. - self.alphas_cumprod)\n","\n","  @staticmethod\n","  def _extract(a, t, x_shape):\n","    \"\"\"\n","    Extract some coefficients at specified timesteps,\n","    then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n","    \"\"\"\n","    bs, = t.shape\n","    assert x_shape[0] == bs\n","    out = tf.gather(tf.convert_to_tensor(a, dtype=tf.float32), t)\n","    assert out.shape == [bs]\n","    return tf.reshape(out, [bs] + ((len(x_shape) - 1) * [1]))\n","\n","  def q_mean_variance(self, x_start, t):\n","    mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n","    variance = self._extract(1. - self.alphas_cumprod, t, x_start.shape)\n","    log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n","    return mean, variance, log_variance\n","\n","  def q_sample(self, x_start, t, noise=None):\n","    \"\"\"\n","    Diffuse the data (t == 0 means diffused for 1 step)\n","    \"\"\"\n","    if noise is None:\n","      noise = tf.random_normal(shape=x_start.shape)\n","    assert noise.shape == x_start.shape\n","    return (\n","        self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n","        self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n","    )\n","\n","  def q_posterior_mean_variance(self, x_start, x_t, t):\n","    \"\"\"\n","    Compute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0)\n","    \"\"\"\n","    assert x_start.shape == x_t.shape\n","    posterior_mean = (\n","        self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n","        self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n","    )\n","    posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n","    posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n","    assert (posterior_mean.shape[0] == posterior_variance.shape[0] == posterior_log_variance_clipped.shape[0] ==\n","            x_start.shape[0])\n","    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n","\n","  def p_mean_variance(self, denoise_fn, *, x, t, clip_denoised: bool, return_pred_xstart: bool):\n","    B, H, W, C = x.shape\n","    assert t.shape == [B]\n","    model_output = denoise_fn(x, t)\n","\n","    # Learned or fixed variance?\n","    if self.model_var_type == 'learned':\n","      assert model_output.shape == [B, H, W, C * 2]\n","      model_output, model_log_variance = tf.split(model_output, 2, axis=-1)\n","      model_variance = tf.exp(model_log_variance)\n","    elif self.model_var_type in ['fixedsmall', 'fixedlarge']:\n","      # below: only log_variance is used in the KL computations\n","      model_variance, model_log_variance = {\n","        # for fixedlarge, we set the initial (log-)variance like so to get a better decoder log likelihood\n","        'fixedlarge': (self.betas, np.log(np.append(self.posterior_variance[1], self.betas[1:]))),\n","        'fixedsmall': (self.posterior_variance, self.posterior_log_variance_clipped),\n","      }[self.model_var_type]\n","      model_variance = self._extract(model_variance, t, x.shape) * tf.ones(x.shape.as_list())\n","      model_log_variance = self._extract(model_log_variance, t, x.shape) * tf.ones(x.shape.as_list())\n","    else:\n","      raise NotImplementedError(self.model_var_type)\n","\n","    # Mean parameterization\n","    _maybe_clip = lambda x_: (tf.clip_by_value(x_, -1., 1.) if clip_denoised else x_)\n","    if self.model_mean_type == 'xprev':  # the model predicts x_{t-1}\n","      pred_xstart = _maybe_clip(self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output))\n","      model_mean = model_output\n","    elif self.model_mean_type == 'xstart':  # the model predicts x_0\n","      pred_xstart = _maybe_clip(model_output)\n","      model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n","    elif self.model_mean_type == 'eps':  # the model predicts epsilon\n","      pred_xstart = _maybe_clip(self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output))\n","      model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n","    else:\n","      raise NotImplementedError(self.model_mean_type)\n","\n","    assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n","    if return_pred_xstart:\n","      return model_mean, model_variance, model_log_variance, pred_xstart\n","    else:\n","      return model_mean, model_variance, model_log_variance\n","\n","  def _predict_xstart_from_eps(self, x_t, t, eps):\n","    assert x_t.shape == eps.shape\n","    return (\n","        self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n","        self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n","    )\n","\n","  def _predict_xstart_from_xprev(self, x_t, t, xprev):\n","    assert x_t.shape == xprev.shape\n","    return (  # (xprev - coef2*x_t) / coef1\n","        self._extract(1. / self.posterior_mean_coef1, t, x_t.shape) * xprev -\n","        self._extract(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t\n","    )\n","\n","  # === Sampling ===\n","\n","  def p_sample(self, denoise_fn, *, x, t, noise_fn, clip_denoised=True, return_pred_xstart: bool):\n","    \"\"\"\n","    Sample from the model\n","    \"\"\"\n","    model_mean, _, model_log_variance, pred_xstart = self.p_mean_variance(\n","      denoise_fn, x=x, t=t, clip_denoised=clip_denoised, return_pred_xstart=True)\n","    noise = noise_fn(shape=x.shape, dtype=x.dtype)\n","    assert noise.shape == x.shape\n","    # no noise when t == 0\n","    nonzero_mask = tf.reshape(1 - tf.cast(tf.equal(t, 0), tf.float32), [x.shape[0]] + [1] * (len(x.shape) - 1))\n","    sample = model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise\n","    assert sample.shape == pred_xstart.shape\n","    return (sample, pred_xstart) if return_pred_xstart else sample\n","\n","  def p_sample_loop(self, denoise_fn, *, shape, noise_fn=tf.random_normal):\n","    \"\"\"\n","    Generate samples\n","    \"\"\"\n","    assert isinstance(shape, (tuple, list))\n","    i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\n","    img_0 = noise_fn(shape=shape, dtype=tf.float32)\n","    _, img_final = tf.while_loop(\n","      cond=lambda i_, _: tf.greater_equal(i_, 0),\n","      body=lambda i_, img_: [\n","        i_ - 1,\n","        self.p_sample(\n","          denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn, return_pred_xstart=False)\n","      ],\n","      loop_vars=[i_0, img_0],\n","      shape_invariants=[i_0.shape, img_0.shape],\n","      back_prop=False\n","    )\n","    assert img_final.shape == shape\n","    return img_final\n","\n","  def p_sample_loop_progressive(self, denoise_fn, *, shape, noise_fn=tf.random_normal, include_xstartpred_freq=50):\n","    \"\"\"\n","    Generate samples and keep track of prediction of x0\n","    \"\"\"\n","    assert isinstance(shape, (tuple, list))\n","    i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\n","    img_0 = noise_fn(shape=shape, dtype=tf.float32)  # [B, H, W, C]\n","\n","    num_recorded_xstartpred = self.num_timesteps // include_xstartpred_freq\n","    xstartpreds_0 = tf.zeros([shape[0], num_recorded_xstartpred, *shape[1:]], dtype=tf.float32)  # [B, N, H, W, C]\n","\n","    def _loop_body(i_, img_, xstartpreds_):\n","      # Sample p(x_{t-1} | x_t) as usual\n","      sample, pred_xstart = self.p_sample(\n","        denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn, return_pred_xstart=True)\n","      assert sample.shape == pred_xstart.shape == shape\n","      # Keep track of prediction of x0\n","      insert_mask = tf.equal(tf.floordiv(i_, include_xstartpred_freq),\n","                             tf.range(num_recorded_xstartpred, dtype=tf.int32))\n","      insert_mask = tf.reshape(tf.cast(insert_mask, dtype=tf.float32),\n","                               [1, num_recorded_xstartpred, *([1] * len(shape[1:]))])  # [1, N, 1, 1, 1]\n","      new_xstartpreds = insert_mask * pred_xstart[:, None, ...] + (1. - insert_mask) * xstartpreds_\n","      return [i_ - 1, sample, new_xstartpreds]\n","\n","    _, img_final, xstartpreds_final = tf.while_loop(\n","      cond=lambda i_, img_, xstartpreds_: tf.greater_equal(i_, 0),\n","      body=_loop_body,\n","      loop_vars=[i_0, img_0, xstartpreds_0],\n","      shape_invariants=[i_0.shape, img_0.shape, xstartpreds_0.shape],\n","      back_prop=False\n","    )\n","    assert img_final.shape == shape and xstartpreds_final.shape == xstartpreds_0.shape\n","    return img_final, xstartpreds_final  # xstart predictions should agree with img_final at step 0\n","\n","  # === Log likelihood calculation ===\n","\n","  def _vb_terms_bpd(self, denoise_fn, x_start, x_t, t, *, clip_denoised: bool, return_pred_xstart: bool):\n","    true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n","    model_mean, _, model_log_variance, pred_xstart = self.p_mean_variance(\n","      denoise_fn, x=x_t, t=t, clip_denoised=clip_denoised, return_pred_xstart=True)\n","    kl = normal_kl(true_mean, true_log_variance_clipped, model_mean, model_log_variance)\n","    kl = nn.meanflat(kl) / np.log(2.)\n","\n","    decoder_nll = -utils.discretized_gaussian_log_likelihood(\n","      x_start, means=model_mean, log_scales=0.5 * model_log_variance)\n","    assert decoder_nll.shape == x_start.shape\n","    decoder_nll = nn.meanflat(decoder_nll) / np.log(2.)\n","\n","    # At the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n","    assert kl.shape == decoder_nll.shape == t.shape == [x_start.shape[0]]\n","    output = tf.where(tf.equal(t, 0), decoder_nll, kl)\n","    return (output, pred_xstart) if return_pred_xstart else output\n","\n","  def training_losses(self, denoise_fn, x_start, t, noise=None):\n","    \"\"\"\n","    Training loss calculation\n","    \"\"\"\n","\n","    # Add noise to data\n","    assert t.shape == [x_start.shape[0]]\n","    if noise is None:\n","      noise = tf.random_normal(shape=x_start.shape, dtype=x_start.dtype)\n","    assert noise.shape == x_start.shape and noise.dtype == x_start.dtype\n","    x_t = self.q_sample(x_start=x_start, t=t, noise=noise)\n","\n","    # Calculate the loss\n","    if self.loss_type == 'kl':  # the variational bound\n","      losses = self._vb_terms_bpd(\n","        denoise_fn=denoise_fn, x_start=x_start, x_t=x_t, t=t, clip_denoised=False, return_pred_xstart=False)\n","    elif self.loss_type == 'mse':  # unweighted MSE\n","      assert self.model_var_type != 'learned'\n","      target = {\n","        'xprev': self.q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)[0],\n","        'xstart': x_start,\n","        'eps': noise\n","      }[self.model_mean_type]\n","      model_output = denoise_fn(x_t, t)\n","      assert model_output.shape == target.shape == x_start.shape\n","      losses = nn.meanflat(tf.squared_difference(target, model_output))\n","    else:\n","      raise NotImplementedError(self.loss_type)\n","\n","    assert losses.shape == t.shape\n","    return losses\n","\n","  def _prior_bpd(self, x_start):\n","    B, T = x_start.shape[0], self.num_timesteps\n","    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t=tf.fill([B], tf.constant(T - 1, dtype=tf.int32)))\n","    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0., logvar2=0.)\n","    assert kl_prior.shape == x_start.shape\n","    return nn.meanflat(kl_prior) / np.log(2.)\n","\n","  def calc_bpd_loop(self, denoise_fn, x_start, *, clip_denoised=True):\n","    (B, H, W, C), T = x_start.shape, self.num_timesteps\n","\n","    def _loop_body(t_, cur_vals_bt_, cur_mse_bt_):\n","      assert t_.shape == []\n","      t_b = tf.fill([B], t_)\n","      # Calculate VLB term at the current timestep\n","      new_vals_b, pred_xstart = self._vb_terms_bpd(\n","        denoise_fn, x_start=x_start, x_t=self.q_sample(x_start=x_start, t=t_b), t=t_b,\n","        clip_denoised=clip_denoised, return_pred_xstart=True)\n","      # MSE for progressive prediction loss\n","      assert pred_xstart.shape == x_start.shape\n","      new_mse_b = nn.meanflat(tf.squared_difference(pred_xstart, x_start))\n","      assert new_vals_b.shape == new_mse_b.shape == [B]\n","      # Insert the calculated term into the tensor of all terms\n","      mask_bt = tf.cast(tf.equal(t_b[:, None], tf.range(T)[None, :]), dtype=tf.float32)\n","      new_vals_bt = cur_vals_bt_ * (1. - mask_bt) + new_vals_b[:, None] * mask_bt\n","      new_mse_bt = cur_mse_bt_ * (1. - mask_bt) + new_mse_b[:, None] * mask_bt\n","      assert mask_bt.shape == cur_vals_bt_.shape == new_vals_bt.shape == [B, T]\n","      return t_ - 1, new_vals_bt, new_mse_bt\n","\n","    t_0 = tf.constant(T - 1, dtype=tf.int32)\n","    terms_0 = tf.zeros([B, T])\n","    mse_0 = tf.zeros([B, T])\n","    _, terms_bpd_bt, mse_bt = tf.while_loop(  # Note that this can be implemented with tf.map_fn instead\n","      cond=lambda t_, cur_vals_bt_, cur_mse_bt_: tf.greater_equal(t_, 0),\n","      body=_loop_body,\n","      loop_vars=[t_0, terms_0, mse_0],\n","      shape_invariants=[t_0.shape, terms_0.shape, mse_0.shape],\n","      back_prop=False\n","    )\n","    prior_bpd_b = self._prior_bpd(x_start)\n","    total_bpd_b = tf.reduce_sum(terms_bpd_bt, axis=1) + prior_bpd_b\n","    assert terms_bpd_bt.shape == mse_bt.shape == [B, T] and total_bpd_b.shape == prior_bpd_b.shape == [B]\n","    return total_bpd_b, terms_bpd_bt, prior_bpd_b, mse_bt"]}]}