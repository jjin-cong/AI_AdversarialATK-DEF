{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_cifar.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9Oaa4ZlUP6VPcnhfjHv5F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix0aycOeliW7"},"outputs":[],"source":["\"\"\"\n","Unconditional CIFAR10\n","\n","python3 scripts/run_cifar.py train --bucket_name_prefix $BUCKET_PREFIX --exp_name $EXPERIMENT_NAME --tpu_name $TPU_NAME\n","python3 scripts/run_cifar.py evaluation --bucket_name_prefix $BUCKET_PREFIX --tpu_name $EVAL_TPU_NAME --model_dir $MODEL_DIR\n","\"\"\"\n","\n","import functools\n","\n","!pip install fire # import fire\n","import numpy as np\n","import tensorflow.compat.v1 as tf"]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Pr2p3p_mTr4","executionInfo":{"status":"ok","timestamp":1658639203605,"user_tz":-540,"elapsed":30877,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"eeeddff1-3da1-4e16-c792-fa5847b23e84"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# cd drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/DDPM/diffusion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDLYnLXEmYqC","executionInfo":{"status":"ok","timestamp":1658639219061,"user_tz":-540,"elapsed":1161,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"8576caf2-8983-479f-dcbc-5d99c0ff8229"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/DDPM/diffusion\n"]}]},{"cell_type":"code","source":["from diffusion_tf import utils\n","from diffusion_tf.diffusion_utils_2 import get_beta_schedule, GaussianDiffusion2\n","from diffusion_tf.models import unet\n","from diffusion_tf.tpu_utils import tpu_utils, datasets, simple_eval_worker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"V0P0gs4Ol_ou","executionInfo":{"status":"error","timestamp":1658639470957,"user_tz":-540,"elapsed":10,"user":{"displayName":"김채현","userId":"06024775478798789360"}},"outputId":"01f61da0-205a-4847-d50a-b26c198383b9"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6cc7b1bb6439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusion_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusion_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_utils_2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_beta_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGaussianDiffusion2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusion_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusion_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpu_utils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_eval_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/[한이음] 적대적 AI 공격에 대한 인공지능 보안기술 연구/3. 소스코드/DDPM/diffusion/diffusion_tf/models/unet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_contrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["class Model(tpu_utils.Model):\n","  def __init__(self, *, model_name, betas: np.ndarray, model_mean_type: str, model_var_type: str, loss_type: str,\n","               num_classes: int, dropout: float, randflip):\n","    self.model_name = model_name\n","    self.diffusion = GaussianDiffusion2(\n","      betas=betas, model_mean_type=model_mean_type, model_var_type=model_var_type, loss_type=loss_type)\n","    self.num_classes = num_classes\n","    self.dropout = dropout\n","    self.randflip = randflip\n","\n","  def _denoise(self, x, t, y, dropout):\n","    B, H, W, C = x.shape.as_list()\n","    assert x.dtype == tf.float32\n","    assert t.shape == [B] and t.dtype in [tf.int32, tf.int64]\n","    assert y.shape == [B] and y.dtype in [tf.int32, tf.int64]\n","    out_ch = (C * 2) if self.diffusion.model_var_type == 'learned' else C\n","    y = None\n","    if self.model_name == 'unet2d16b2':  # 35.7M\n","      return unet.model(\n","        x, t=t, y=y, name='model', ch=128, ch_mult=(1, 2, 2, 2), num_res_blocks=2, attn_resolutions=(16,),\n","        out_ch=out_ch, num_classes=self.num_classes, dropout=dropout\n","      )\n","    raise NotImplementedError(self.model_name)\n","\n","  def train_fn(self, x, y):\n","    B, H, W, C = x.shape\n","    if self.randflip:\n","      x = tf.image.random_flip_left_right(x)\n","      assert x.shape == [B, H, W, C]\n","    t = tf.random_uniform([B], 0, self.diffusion.num_timesteps, dtype=tf.int32)\n","    losses = self.diffusion.training_losses(\n","      denoise_fn=functools.partial(self._denoise, y=y, dropout=self.dropout), x_start=x, t=t)\n","    assert losses.shape == t.shape == [B]\n","    return {'loss': tf.reduce_mean(losses)}\n","\n","  def samples_fn(self, dummy_noise, y):\n","    return {\n","      'samples': self.diffusion.p_sample_loop(\n","        denoise_fn=functools.partial(self._denoise, y=y, dropout=0),\n","        shape=dummy_noise.shape.as_list(),\n","        noise_fn=tf.random_normal\n","      )\n","    }\n","\n","  def progressive_samples_fn(self, dummy_noise, y):\n","    samples, progressive_samples = self.diffusion.p_sample_loop_progressive(\n","      denoise_fn=functools.partial(self._denoise, y=y, dropout=0),\n","      shape=dummy_noise.shape.as_list(),\n","      noise_fn=tf.random_normal\n","    )\n","    return {'samples': samples, 'progressive_samples': progressive_samples}\n","\n","  def bpd_fn(self, x, y):\n","    total_bpd_b, terms_bpd_bt, prior_bpd_b, mse_bt = self.diffusion.calc_bpd_loop(\n","      denoise_fn=functools.partial(self._denoise, y=y, dropout=0),\n","      x_start=x\n","    )\n","    return {\n","      'total_bpd': total_bpd_b,\n","      'terms_bpd': terms_bpd_bt,\n","      'prior_bpd': prior_bpd_b,\n","      'mse': mse_bt\n","    }\n","\n","\n","def _load_model(kwargs, ds):\n","  return Model(\n","    model_name=kwargs['model_name'],\n","    betas=get_beta_schedule(\n","      kwargs['beta_schedule'], beta_start=kwargs['beta_start'], beta_end=kwargs['beta_end'],\n","      num_diffusion_timesteps=kwargs['num_diffusion_timesteps']\n","    ),\n","    model_mean_type=kwargs['model_mean_type'],\n","    model_var_type=kwargs['model_var_type'],\n","    loss_type=kwargs['loss_type'],\n","    num_classes=ds.num_classes,\n","    dropout=kwargs['dropout'],\n","    randflip=kwargs['randflip']\n","  )\n","\n","\n","def simple_eval(model_dir, tpu_name, bucket_name_prefix, mode, load_ckpt, total_bs=256, tfds_data_dir='tensorflow_datasets'):\n","  region = utils.get_gcp_region()\n","  tfds_data_dir = 'gs://{}-{}/{}'.format(bucket_name_prefix, region, tfds_data_dir)\n","  kwargs = tpu_utils.load_train_kwargs(model_dir)\n","  print('loaded kwargs:', kwargs)\n","  ds = datasets.get_dataset(kwargs['dataset'], tfds_data_dir=tfds_data_dir)\n","  worker = simple_eval_worker.SimpleEvalWorker(\n","    tpu_name=tpu_name, model_constructor=functools.partial(_load_model, kwargs=kwargs, ds=ds),\n","    total_bs=total_bs, dataset=ds)\n","  worker.run(mode=mode, logdir=model_dir, load_ckpt=load_ckpt)\n","\n","\n","def evaluation(  # evaluation loop for use during training\n","    model_dir, tpu_name, bucket_name_prefix, once=False, dump_samples_only=False, total_bs=256,\n","    tfds_data_dir='tensorflow_datasets', load_ckpt=None\n","):\n","  region = utils.get_gcp_region()\n","  tfds_data_dir = 'gs://{}-{}/{}'.format(bucket_name_prefix, region, tfds_data_dir)\n","  kwargs = tpu_utils.load_train_kwargs(model_dir)\n","  print('loaded kwargs:', kwargs)\n","  ds = datasets.get_dataset(kwargs['dataset'], tfds_data_dir=tfds_data_dir)\n","  worker = tpu_utils.EvalWorker(\n","    tpu_name=tpu_name,\n","    model_constructor=functools.partial(_load_model, kwargs=kwargs, ds=ds),\n","    total_bs=total_bs, inception_bs=total_bs, num_inception_samples=50000,\n","    dataset=ds,\n","  )\n","  worker.run(\n","    logdir=model_dir, once=once, skip_non_ema_pass=True, dump_samples_only=dump_samples_only, load_ckpt=load_ckpt)\n","\n","\n","def train(\n","    exp_name, tpu_name, bucket_name_prefix, model_name='unet2d16b2', dataset='cifar10',\n","    optimizer='adam', total_bs=128, grad_clip=1., lr=2e-4, warmup=5000,\n","    num_diffusion_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule='linear',\n","    model_mean_type='eps', model_var_type='fixedlarge', loss_type='mse',\n","    dropout=0.1, randflip=1,\n","    tfds_data_dir='tensorflow_datasets', log_dir='logs', keep_checkpoint_max=2\n","):\n","  region = utils.get_gcp_region()\n","  tfds_data_dir = 'gs://{}-{}/{}'.format(bucket_name_prefix, region, tfds_data_dir)\n","  log_dir = 'gs://{}-{}/{}'.format(bucket_name_prefix, region, log_dir)\n","  kwargs = dict(locals())\n","  ds = datasets.get_dataset(dataset, tfds_data_dir=tfds_data_dir)\n","  tpu_utils.run_training(\n","    date_str='9999-99-99',\n","    exp_name='{exp_name}_{dataset}_{model_name}_{optimizer}_bs{total_bs}_lr{lr}w{warmup}_beta{beta_start}-{beta_end}-{beta_schedule}_t{num_diffusion_timesteps}_{model_mean_type}-{model_var_type}-{loss_type}_dropout{dropout}_randflip{randflip}'.format(\n","      **kwargs),\n","    model_constructor=lambda: Model(\n","      model_name=model_name,\n","      betas=get_beta_schedule(\n","        beta_schedule, beta_start=beta_start, beta_end=beta_end, num_diffusion_timesteps=num_diffusion_timesteps\n","      ),\n","      model_mean_type=model_mean_type,\n","      model_var_type=model_var_type,\n","      loss_type=loss_type,\n","      num_classes=ds.num_classes,\n","      dropout=dropout,\n","      randflip=randflip\n","    ),\n","    optimizer=optimizer, total_bs=total_bs, lr=lr, warmup=warmup, grad_clip=grad_clip,\n","    train_input_fn=ds.train_input_fn,\n","    tpu=tpu_name, log_dir=log_dir, dump_kwargs=kwargs, iterations_per_loop=2000, keep_checkpoint_max=keep_checkpoint_max\n","  )\n","\n","\n","if __name__ == '__main__':\n","  fire.Fire()"],"metadata":{"id":"Tjpb38Hpltmg"},"execution_count":null,"outputs":[]}]}